{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ“— Part 3 v3 ì™„ì „íŒ\\n\\n**Priority 1:** Ensemble+Statistical Test, n_iter=200  \\n**Priority 2:** Cumulative Gains, ì‹œê°í™”, CV ì„ê³„ê°’  \\n**Priority 3:** Winsorizer, Calibration, Learning Curve"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. í™˜ê²½ ì„¤ì •"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport platform\nif platform.system() == 'Darwin': plt.rc('font', family='AppleGothic')\nelif platform.system() == 'Windows': plt.rc('font', family='Malgun Gothic')\nelse: plt.rc('font', family='NanumGothic')\nplt.rc('axes', unicode_minus=False)\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict, RandomizedSearchCV, learning_curve\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, precision_recall_curve, fbeta_score, make_scorer\nfrom sklearn.calibration import calibration_curve\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import wilcoxon\nimport joblib, os\nfrom datetime import datetime\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nprint('âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DATA_DIR = '../data'\nPROCESSED_DIR = os.path.join(DATA_DIR, 'processed')\nos.makedirs(PROCESSED_DIR, exist_ok=True)\nINPUT_FILE = os.path.join(DATA_DIR, 'features', 'domain_based_features_ì™„ì „íŒ.csv')\nOUTPUT_PREFIX = 'ë°œí‘œ_Part3_v3'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. 3-Way Split (Train 60% / Val 20% / Test 20%)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv(INPUT_FILE, encoding='utf-8')\nTARGET_COL = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'\nX = df.drop(columns=[TARGET_COL])\ny = df[TARGET_COL]\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE)\nprint(f'Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}')\nprint('âš ï¸ TestëŠ” ìµœì¢… í‰ê°€ê¹Œì§€ ì‚¬ìš© ì•ˆ í•¨!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. ì „ì²˜ë¦¬ + â­ Winsorizer ì‹¤í—˜ (Priority 3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\nclass InfiniteHandler(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None): return self\n    def transform(self, X): return X.replace([np.inf, -np.inf], np.nan)\n\nclass LogTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, eps=1e-10): self.eps = eps\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        X_c = X.copy()\n        for c in X_c.columns:\n            if (X_c[c] >= 0).all(): X_c[c] = np.log1p(X_c[c] + self.eps)\n        return X_c\n\nclass Winsorizer(BaseEstimator, TransformerMixin):\n    def __init__(self, l=0.005, u=0.995): self.l, self.u, self.b = l, u, {}\n    def fit(self, X, y=None):\n        for c in X.columns: self.b[c] = (X[c].quantile(self.l), X[c].quantile(self.u))\n        return self\n    def transform(self, X):\n        X_c = X.copy()\n        for c in X_c.columns: X_c[c] = X_c[c].clip(*self.b[c])\n        return X_c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n\ndef create_pipeline(clf, wins=False, resamp=None):\n    s = [('inf', InfiniteHandler()), ('imp', SimpleImputer(strategy='median')), ('log', LogTransformer())]\n    if wins: s.append(('wins', Winsorizer()))\n    s.append(('scaler', RobustScaler()))\n    s.append(('resamp', SMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE) if resamp else 'passthrough'))\n    s.append(('clf', clf))\n    return ImbPipeline(s)\n\nlgbm_t = lgb.LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE, verbose=-1)\nprint('Winsorizer ì‹¤í—˜...')\nwins_r = {}\nfor w in [False, True]:\n    p = create_pipeline(lgbm_t, wins=w)\n    p.fit(X_train, y_train)\n    pr = average_precision_score(y_val, p.predict_proba(X_val)[:, 1])\n    wins_r[w] = pr\n    print(f'Wins={w}: PR-AUC={pr:.4f}')\nUSE_WINSORIZER = wins_r[True] > wins_r[False]\nprint(f'âœ… Winsorizer={USE_WINSORIZER}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. ë¦¬ìƒ˜í”Œë§ ì „ëµ (SMOTE vs Class Weight)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lgbm_b = lgb.LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE, verbose=-1)\nstrat_a = {}\nfor v in ['smote', 'borderline', 'smote_tomek']:\n    p = create_pipeline(lgbm_b, wins=USE_WINSORIZER, resamp=v)\n    p.fit(X_train, y_train)\n    pr = average_precision_score(y_val, p.predict_proba(X_val)[:, 1])\n    strat_a[v] = pr\n    print(f'{v}: {pr:.4f}')\n\nlgbm_w = lgb.LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.05, scale_pos_weight=scale_pos_weight, random_state=RANDOM_STATE, verbose=-1)\np_b = create_pipeline(lgbm_w, wins=USE_WINSORIZER)\np_b.fit(X_train, y_train)\nstrat_b_pr = average_precision_score(y_val, p_b.predict_proba(X_val)[:, 1])\nprint(f'Class Weight: {strat_b_pr:.4f}')\n\nbest_smote = max(strat_a, key=strat_a.get)\nif strat_b_pr > strat_a[best_smote]:\n    selected_strategy = 'Class Weight'\n    selected_resampler = None\nelse:\n    selected_strategy = f'SMOTE ({best_smote})'\n    selected_resampler = best_smote\nprint(f'âœ… ì„ íƒ: {selected_strategy}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. â­ AutoML (Priority 1 - n_iter=200)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lgbm_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.05, 0.1], 'num_leaves': [31, 63, 127], 'min_child_samples': [20, 50, 100]}\nxgb_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.05, 0.1], 'min_child_weight': [1, 3, 5], 'subsample': [0.8, 0.9, 1.0]}\ncb_grid = {'iterations': [100, 200, 300], 'depth': [4, 6, 8], 'learning_rate': [0.01, 0.05, 0.1], 'l2_leaf_reg': [1, 3, 5]}\nlr_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n\nif selected_resampler is None:\n    for g in [lgbm_grid, xgb_grid, cb_grid]: g['scale_pos_weight'] = [scale_pos_weight]\n    lr_grid['class_weight'] = ['balanced']\nelse:\n    for g in [lgbm_grid, xgb_grid, cb_grid]: g['scale_pos_weight'] = [1]\n    lr_grid['class_weight'] = [None]\n\nmodels = {\n    'LightGBM': (lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1), lgbm_grid),\n    'XGBoost': (xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'), xgb_grid),\n    'CatBoost': (CatBoostClassifier(random_state=RANDOM_STATE, verbose=0), cb_grid),\n    'LogisticRegression': (LogisticRegression(random_state=RANDOM_STATE, max_iter=1000), lr_grid)\n}\n\ntuning_results = {}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nscorer = make_scorer(average_precision_score, needs_proba=True)\n\nprint('AutoML ì‹œì‘ (n_iter=200)...')\nfor name, (model, grid) in models.items():\n    pipe = create_pipeline(model, wins=USE_WINSORIZER, resamp=selected_resampler)\n    pipe_grid = {f'clf__{k}': v for k, v in grid.items()}\n    search = RandomizedSearchCV(pipe, pipe_grid, n_iter=200, scoring=scorer, cv=cv, n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n    search.fit(X_train, y_train)\n    tuning_results[name] = {'best_estimator': search.best_estimator_, 'best_cv_score': search.best_score_, 'best_params': search.best_params_}\n    print(f'{name}: CV PR-AUC={search.best_score_:.4f}')\nprint('âœ… AutoML ì™„ë£Œ')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. â­ ëª¨ë¸ ì„ íƒ (Priority 1 - Ensemble + Statistical Test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validation í‰ê°€\nval_results = {}\nfor name, res in tuning_results.items():\n    pr = average_precision_score(y_val, res['best_estimator'].predict_proba(X_val)[:, 1])\n    val_results[name] = pr\n    print(f'{name}: Val PR-AUC={pr:.4f}')\n\nbest_single_name = max(val_results, key=val_results.get)\nbest_single = tuning_results[best_single_name]['best_estimator']\nbest_single_pr = val_results[best_single_name]\nprint(f'\\nBest Single: {best_single_name} ({best_single_pr:.4f})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Voting Ensemble (ìƒìœ„ 3ê°œ)\nsorted_models = sorted(val_results.items(), key=lambda x: x[1], reverse=True)[:3]\nestimators = [(name, tuning_results[name]['best_estimator']) for name, _ in sorted_models]\nvoting = VotingClassifier(estimators=estimators, voting='soft')\nvoting.fit(X_train, y_train)\nvoting_pr = average_precision_score(y_val, voting.predict_proba(X_val)[:, 1])\nprint(f'Voting Ensemble: Val PR-AUC={voting_pr:.4f}')\nprint(f'\\nì„±ëŠ¥ ì°¨ì´: {voting_pr - best_single_pr:+.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Statistical Significance Test\nprint('\\ní†µê³„ì  ìœ ì˜ì„± ê²€ì • (Wilcoxon)...')\ncv_single = cross_val_score(best_single, X_train, y_train, cv=5, scoring=scorer)\ncv_voting = cross_val_score(voting, X_train, y_train, cv=5, scoring=scorer)\nstat, pval = wilcoxon(cv_voting, cv_single)\nprint(f'p-value: {pval:.4f}')\n\nif voting_pr > best_single_pr and pval < 0.05:\n    final_model = voting\n    final_name = 'VotingEnsemble'\n    reason = f'Ensembleì´ {voting_pr - best_single_pr:.4f} ë” ìš°ìˆ˜ (p={pval:.4f} < 0.05)'\nelse:\n    final_model = best_single\n    final_name = best_single_name\n    reason = f'Single ì„ íƒ (ì°¨ì´ ë¯¸ë¯¸ ë˜ëŠ” p={pval:.4f} >= 0.05)'\n\nprint(f'\\nâœ… ìµœì¢… ëª¨ë¸: {final_name}')\nprint(f'   ì´ìœ : {reason}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. â­ ì„ê³„ê°’ ìµœì í™” (Priority 2 - Validation + CV)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "y_val_prob_final = final_model.predict_proba(X_val)[:, 1]\nprec_v, rec_v, thr_v = precision_recall_curve(y_val, y_val_prob_final)\n\n# F2-Score ìµœì \nbeta = 2\nf2_scores = (1 + beta**2) * (prec_v[:-1] * rec_v[:-1]) / (beta**2 * prec_v[:-1] + rec_v[:-1] + 1e-10)\nf2_opt_idx = np.argmax(f2_scores)\nf2_opt_thr = thr_v[f2_opt_idx]\nprint(f'F2 ìµœì  ì„ê³„ê°’ (Val): {f2_opt_thr:.4f}')\n\n# Recall 80%\nidx_80 = np.where(rec_v[:-1] >= 0.80)[0]\nif len(idx_80) > 0:\n    r80_idx = idx_80[np.argmax(prec_v[:-1][idx_80])]\n    r80_thr = thr_v[r80_idx]\n    print(f'Recall 80% ì„ê³„ê°’ (Val): {r80_thr:.4f}')\nelse:\n    r80_thr = f2_opt_thr\n    print('Recall 80% ë¶ˆê°€, F2 ì‚¬ìš©')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.1 â­ CV ê¸°ë°˜ ì„ê³„ê°’ ê²€ì¦ (Priority 2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CV ì˜ˆì¸¡\ny_train_prob_cv = cross_val_predict(final_model, X_train, y_train, cv=5, method='predict_proba')[:, 1]\nprec_cv, rec_cv, thr_cv = precision_recall_curve(y_train, y_train_prob_cv)\n\nf2_cv = (1 + beta**2) * (prec_cv[:-1] * rec_cv[:-1]) / (beta**2 * prec_cv[:-1] + rec_cv[:-1] + 1e-10)\nf2_cv_thr = thr_cv[np.argmax(f2_cv)]\n\nidx_cv_80 = np.where(rec_cv[:-1] >= 0.80)[0]\nif len(idx_cv_80) > 0:\n    r80_cv_thr = thr_cv[idx_cv_80[np.argmax(prec_cv[:-1][idx_cv_80])]]\nelse:\n    r80_cv_thr = f2_cv_thr\n\nprint(f'CV F2 ì„ê³„ê°’: {f2_cv_thr:.4f}')\nprint(f'CV Recall 80% ì„ê³„ê°’: {r80_cv_thr:.4f}')\n\n# í‰ê· \nselected_threshold = (r80_thr + r80_cv_thr) / 2\nprint(f'\\nâœ… ìµœì¢… ì„ê³„ê°’: {selected_threshold:.4f} (Val+CV í‰ê· )')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Traffic Light ì‹œìŠ¤í…œ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Red: Recall 80%\nred_threshold = r80_thr\n\n# Yellow: Recall 95%\nidx_95 = np.where(rec_v[:-1] >= 0.95)[0]\nif len(idx_95) > 0:\n    yellow_threshold = thr_v[idx_95[np.argmax(prec_v[:-1][idx_95])]]\nelse:\n    yellow_threshold = thr_v[-1] if len(thr_v) > 0 else 0.01\n\nprint(f'Red >= {red_threshold:.4f}')\nprint(f'Yellow >= {yellow_threshold:.4f}')\n\ndef assign_traffic(prob, red, yellow):\n    if prob >= red: return 'Red'\n    elif prob >= yellow: return 'Yellow'\n    else: return 'Green'\n\n# Val í‰ê°€\ntraffic_val = pd.Series(y_val_prob_final).apply(lambda x: assign_traffic(x, red_threshold, yellow_threshold))\nfor grade in ['Red', 'Yellow', 'Green']:\n    mask = (traffic_val == grade)\n    cnt = mask.sum()\n    bk = y_val.values[mask].sum()\n    print(f'{grade}: {cnt} ê¸°ì—…, {bk} ë¶€ë„')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. â­ Test Set ìµœì¢… í‰ê°€ (Priority 2 - ì‹œê°í™”)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 8.1 Test Set ì˜ˆì¸¡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "y_test_prob = final_model.predict_proba(X_test)[:, 1]\ny_test_pred = (y_test_prob >= selected_threshold).astype(int)\n\ntest_pr_auc = average_precision_score(y_test, y_test_prob)\ntest_roc_auc = roc_auc_score(y_test, y_test_prob)\ntest_f2 = fbeta_score(y_test, y_test_pred, beta=2)\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\ntest_recall = tp / (tp + fn)\ntest_prec = tp / (tp + fp)\n\nprint('='*70)\nprint('ğŸ¯ Test Set ìµœì¢… í‰ê°€')\nprint('='*70)\nprint(f'PR-AUC: {test_pr_auc:.4f}')\nprint(f'ROC-AUC: {test_roc_auc:.4f}')\nprint(f'F2-Score: {test_f2:.4f}')\nprint(f'Precision: {test_prec:.2%}')\nprint(f'Recall: {test_recall:.2%}')\nprint(f'\\nConfusion Matrix:')\nprint(f'  TN: {tn:,}  |  FP: {fp:,}')\nprint(f'  FN: {fn:,}  |  TP: {tp:,}')\nprint('='*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 8.2 â­ PR-AUC Curve ì‹œê°í™” (Priority 2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "prec_test, rec_test, thr_test = precision_recall_curve(y_test, y_test_prob)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=rec_test, y=prec_test, mode='lines', name=f'PR-AUC={test_pr_auc:.4f}', line=dict(width=2)))\nfig.add_trace(go.Scatter(x=[0, 1], y=[y_test.mean(), y_test.mean()], mode='lines', name='Baseline', line=dict(dash='dash')))\nfig.update_layout(title='Precision-Recall Curve (Test Set)', xaxis_title='Recall', yaxis_title='Precision', height=500)\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 8.3 â­ Confusion Matrix ì‹œê°í™” (Priority 2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "cm = confusion_matrix(y_test, y_test_pred)\nfig = go.Figure(data=go.Heatmap(z=cm, x=['Pred 0', 'Pred 1'], y=['True 0', 'True 1'], text=cm, texttemplate='%{text}', colorscale='Blues'))\nfig.update_layout(title=f'Confusion Matrix (Threshold={selected_threshold:.4f})', height=400)\nfig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. â­ ëª¨ë¸ ì§„ë‹¨ (Priority 3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 9.1 â­ Calibration Check (Priority 3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "prob_true, prob_pred = calibration_curve(y_val, y_val_prob_final, n_bins=10, strategy='quantile')\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=prob_pred, y=prob_true, mode='markers+lines', name='Model', marker=dict(size=10)))\nfig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Perfect', line=dict(dash='dash')))\nfig.update_layout(title='Calibration Curve (Validation Set)', xaxis_title='Predicted Probability', yaxis_title='True Probability', height=500)\nfig.show()\nprint('\\nâœ… Calibration: ëŒ€ê°ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í™•ë¥  ì˜ˆì¸¡ ì •í™•')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 9.2 â­ Learning Curve (Priority 3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_sizes, train_scores, val_scores = learning_curve(\n    final_model, X_train, y_train,\n    cv=5, scoring=scorer,\n    train_sizes=np.linspace(0.1, 1.0, 10),\n    n_jobs=-1, random_state=RANDOM_STATE\n)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores.mean(axis=1), mode='lines+markers', name='Train', error_y=dict(array=train_scores.std(axis=1))))\nfig.add_trace(go.Scatter(x=train_sizes, y=val_scores.mean(axis=1), mode='lines+markers', name='CV', error_y=dict(array=val_scores.std(axis=1))))\nfig.update_layout(title='Learning Curve', xaxis_title='Training Size', yaxis_title='PR-AUC', height=500)\nfig.show()\nprint('\\nâœ… Learning Curve: Trainê³¼ CV ê²©ì°¨ê°€ ì‘ìœ¼ë©´ ê³¼ì í•© ì—†ìŒ')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. â­ Cumulative Gains Curve (Priority 2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Set Cumulative Gains\ndf_gains = pd.DataFrame({'prob': y_test_prob, 'y': y_test.values})\ndf_gains = df_gains.sort_values('prob', ascending=False).reset_index(drop=True)\ndf_gains['cum_y'] = df_gains['y'].cumsum()\ndf_gains['pct_captured'] = df_gains['cum_y'] / df_gains['y'].sum()\ndf_gains['pct_population'] = (df_gains.index + 1) / len(df_gains)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df_gains['pct_population'], y=df_gains['pct_captured'], mode='lines', name='Model', line=dict(width=2)))\nfig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Random', line=dict(dash='dash')))\nfig.update_layout(title='Cumulative Gains Curve (Test Set)', xaxis_title='% Population', yaxis_title='% Bankruptcies Captured', xaxis_tickformat='.0%', yaxis_tickformat='.0%', height=500)\nfig.show()\n\n# ìƒìœ„ 10% í¬ì°©ë¥ \ntop10_captured = df_gains[df_gains['pct_population'] <= 0.1]['pct_captured'].iloc[-1]\nprint(f'\\nâœ… ìƒìœ„ 10% ê¸°ì—…ì—ì„œ ë¶€ë„ì˜ {top10_captured:.1%} í¬ì°©')\nprint(f'   íš¨ìœ¨ì„±: Random ëŒ€ë¹„ {top10_captured/0.1:.1f}ë°°')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Feature Importance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if final_name == 'VotingEnsemble':\n    # Ensembleì´ë©´ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©\n    clf = final_model.estimators_[0].named_steps['clf']\nelse:\n    clf = final_model.named_steps['clf']\n\nif hasattr(clf, 'feature_importances_'):\n    imp = clf.feature_importances_\n    feat_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': imp}).sort_values('Importance', ascending=False).head(15)\n    \n    fig = go.Figure(go.Bar(x=feat_imp['Importance'], y=feat_imp['Feature'], orientation='h', marker=dict(color=feat_imp['Importance'], colorscale='Blues')))\n    fig.update_layout(title=f'Top 15 Feature Importance ({final_name})', xaxis_title='Importance', yaxis_title='Feature', height=600, yaxis={'categoryorder':'total ascending'})\n    fig.show()\nelse:\n    print('Feature Importance ì§€ì› ì•ˆ í•¨')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "save_files = {\n    f'{OUTPUT_PREFIX}_ìµœì¢…ëª¨ë¸.pkl': final_model,\n    f'{OUTPUT_PREFIX}_ì„ê³„ê°’.pkl': {'selected': selected_threshold, 'red': red_threshold, 'yellow': yellow_threshold},\n    f'{OUTPUT_PREFIX}_ê²°ê³¼.pkl': {'model_name': final_name, 'test_pr_auc': test_pr_auc, 'test_recall': test_recall, 'test_f2': test_f2}\n}\n\nfor fname, data in save_files.items():\n    joblib.dump(data, os.path.join(PROCESSED_DIR, fname))\n    print(f'âœ… {fname}')\n\nprint(f'\\nì €ì¥ ìœ„ì¹˜: {PROCESSED_DIR}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13. ìµœì¢… ìš”ì•½\n\n### âœ… v3 ê°œì„ ì‚¬í•­ ì™„ë£Œ\n\n**Priority 1:**  \n- Ensemble vs Single ë¹„êµ (Wilcoxon Test)  \n- n_iter=200 (ê¸°ì¡´ 30â†’200)  \n\n**Priority 2:**  \n- Cumulative Gains Curve  \n- PR-AUC/Confusion Matrix ì‹œê°í™”  \n- CV ê¸°ë°˜ ì„ê³„ê°’ ê²€ì¦  \n\n**Priority 3:**  \n- Winsorizer ì‹¤í—˜  \n- Calibration Check  \n- Learning Curve  \n\n### ğŸ¯ í•µì‹¬ ì„±ê³¼\n\nâœ… **Data Leakage ì™„ì „ ì œê±°**  \nâœ… **Statistical Significance ê²€ì¦**  \nâœ… **Robust ì„ê³„ê°’ (Val + CV í‰ê· )**  \nâœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì…ì¦** (Cumulative Gains)  \nâœ… **ëª¨ë¸ ì§„ë‹¨ ì™„ë£Œ** (Calibration, Learning Curve)  \n\n---\n\n**ë…¸íŠ¸ë¶ ì™„ë£Œ!** ğŸ‰"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}