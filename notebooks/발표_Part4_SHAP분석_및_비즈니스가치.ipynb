{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: SHAP ë¶„ì„ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ í‰ê°€\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "\n",
    "Part 3 v3ì—ì„œ ì„ ì •ëœ ìµœì¢… ëª¨ë¸ì˜ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ **SHAP (SHapley Additive exPlanations)**ìœ¼ë¡œ í•´ì„í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— í™œìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ ì„ í–‰ ì¡°ê±´\n",
    "\n",
    "### Part 3 v3 ì¶œë ¥ íŒŒì¼\n",
    "\n",
    "- `ë°œí‘œ_Part3_v3_ìµœì¢…ëª¨ë¸.pkl`\n",
    "- `ë°œí‘œ_Part3_v3_ì„ê³„ê°’.pkl`\n",
    "- `ë°œí‘œ_Part3_v3_ê²°ê³¼.pkl`\n",
    "- `domain_based_features_ì™„ì „íŒ.csv`\n",
    "\n",
    "### ë°ì´í„° ê·œëª¨\n",
    "\n",
    "- **ê¸°ì—… ìˆ˜**: 50,105ê°œ\n",
    "- **ë¶€ë„ìœ¨**: ~1.5%\n",
    "- **Train/Val/Test Split**: 60% / 20% / 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 0: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from IPython.display import display  # âœ… FIX 1: display() import ì¶”ê°€\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "import platform\n",
    "if platform.system() == 'Darwin':\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "else:\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# ì‹œê°í™” ìŠ¤íƒ€ì¼\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# âœ… FIX 4: ìƒ˜í”Œë§ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬)\n",
    "SHAP_MAX_SAMPLES = int(os.getenv('SHAP_MAX_SAMPLES', '5000'))  # SHAP ê³„ì‚°ìš© ìµœëŒ€ ìƒ˜í”Œ ìˆ˜\n",
    "BOOTSTRAP_ITERATIONS = int(os.getenv('BOOTSTRAP_ITERATIONS', '1000'))\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ\")\n",
    "print(f\"   - SHAP ë²„ì „: {shap.__version__}\")\n",
    "print(f\"   - Python í”Œë«í¼: {platform.system()}\")\n",
    "print(f\"   - SHAP ìµœëŒ€ ìƒ˜í”Œ: {SHAP_MAX_SAMPLES:,}ê°œ\")\n",
    "print(f\"   - Bootstrap ë°˜ë³µ: {BOOTSTRAP_ITERATIONS:,}íšŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 v3 ì¶œë ¥ ë¡œë“œ\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "\n",
    "try:\n",
    "    final_model = joblib.load(os.path.join(PROCESSED_DIR, 'ë°œí‘œ_Part3_v3_ìµœì¢…ëª¨ë¸.pkl'))\n",
    "    thresholds = joblib.load(os.path.join(PROCESSED_DIR, 'ë°œí‘œ_Part3_v3_ì„ê³„ê°’.pkl'))\n",
    "    results = joblib.load(os.path.join(PROCESSED_DIR, 'ë°œí‘œ_Part3_v3_ê²°ê³¼.pkl'))\n",
    "    \n",
    "    print(\"âœ… Part 3 v3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"   - ìµœì¢… ëª¨ë¸: {results['model_name']}\")\n",
    "    print(f\"   - Test PR-AUC: {results['test_pr_auc']:.4f}\")\n",
    "    print(f\"   - Test Recall: {results['test_recall']:.2%}\")\n",
    "    print(f\"   - Test F2-Score: {results['test_f2']:.4f}\")\n",
    "    print(f\"\\nâœ… ì„ê³„ê°’ ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"   - ì„ íƒëœ ì„ê³„ê°’: {thresholds['selected']:.4f}\")\n",
    "    print(f\"   - Red ì„ê³„ê°’: {thresholds['red']:.4f}\")\n",
    "    print(f\"   - Yellow ì„ê³„ê°’: {thresholds['yellow']:.4f}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ ì˜¤ë¥˜: Part 3 v3 ì¶œë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   - {e}\")\n",
    "    print(f\"\\në¨¼ì € 'ë°œí‘œ_Part3_v3_ì™„ì „íŒ.ipynb'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature ë°ì´í„° ë¡œë“œ\n",
    "FEATURES_PATH = '../data/features/domain_based_features_ì™„ì „íŒ.csv'\n",
    "\n",
    "try:\n",
    "    features_df = pd.read_csv(FEATURES_PATH, encoding='utf-8')\n",
    "    print(f\"âœ… Feature ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"   - ë°ì´í„° í¬ê¸°: {features_df.shape}\")\n",
    "    print(f\"   - ê¸°ì—… ìˆ˜: {len(features_df):,}ê°œ\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ì˜¤ë¥˜: {FEATURES_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"\\në¨¼ì € 'ë°œí‘œ_Part2_íŠ¹ì„±ì„ íƒ.ipynb'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    raise\n",
    "\n",
    "# íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "TARGET_COL = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'\n",
    "X = features_df.drop(columns=[TARGET_COL])\n",
    "y = features_df[TARGET_COL]\n",
    "\n",
    "print(f\"\\nâœ… íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ ì™„ë£Œ\")\n",
    "print(f\"   - Feature ìˆ˜: {X.shape[1]}ê°œ\")\n",
    "print(f\"   - ë¶€ë„ ê¸°ì—…: {y.sum():,}ê°œ ({y.mean():.2%})\")\n",
    "print(f\"   - ì •ìƒ ê¸°ì—…: {(~y.astype(bool)).sum():,}ê°œ ({(~y.astype(bool)).mean():.2%})\")\n",
    "\n",
    "# âœ… FIX 5: Feature ìˆ˜ ê²€ì¦\n",
    "EXPECTED_MIN_FEATURES = 20  # ìµœì†Œ ì˜ˆìƒ Feature ìˆ˜\n",
    "EXPECTED_MAX_FEATURES = 30  # ìµœëŒ€ ì˜ˆìƒ Feature ìˆ˜\n",
    "\n",
    "if not (EXPECTED_MIN_FEATURES <= X.shape[1] <= EXPECTED_MAX_FEATURES):\n",
    "    print(f\"\\nâš ï¸ ê²½ê³ : Feature ìˆ˜ê°€ ì˜ˆìƒ ë²”ìœ„({EXPECTED_MIN_FEATURES}-{EXPECTED_MAX_FEATURES})ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.\")\n",
    "    print(f\"   - ì‹¤ì œ Feature ìˆ˜: {X.shape[1]}ê°œ\")\n",
    "    print(f\"   - Part 2 íŠ¹ì„± ì„ íƒ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Feature ìˆ˜ ê²€ì¦ í†µê³¼: {X.shape[1]}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì¼í•œ 3-Way Split (Part 3ì™€ ë™ì¼í•œ random_state ì‚¬ìš©)\n",
    "# Train/Temp Split (80/20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train/Val Split (60/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ (Part 3ì™€ ë™ì¼)\")\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  - í¬ê¸°: {len(X_train):,}ê°œ\")\n",
    "print(f\"  - ë¶€ë„ìœ¨: {y_train.mean():.2%}\")\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  - í¬ê¸°: {len(X_val):,}ê°œ\")\n",
    "print(f\"  - ë¶€ë„ìœ¨: {y_val.mean():.2%}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  - í¬ê¸°: {len(X_test):,}ê°œ\")\n",
    "print(f\"  - ë¶€ë„ìœ¨: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 1: SHAP TreeExplainer ì´ˆê¸°í™”\n",
    "\n",
    "### SHAPë€?\n",
    "\n",
    "- **Shapley Value** ê¸°ë°˜ ëª¨ë¸ í•´ì„ ë°©ë²• (ê²Œì„ ì´ë¡ ì—ì„œ ìœ ë˜)\n",
    "- ê° Featureê°€ **ê°œë³„ ì˜ˆì¸¡ì— ê¸°ì—¬í•œ ì •ë„**ë¥¼ ì •ëŸ‰í™”\n",
    "- **ì–‘ìˆ˜**: ë¶€ë„ ìœ„í—˜ ì¦ê°€ / **ìŒìˆ˜**: ë¶€ë„ ìœ„í—˜ ê°ì†Œ\n",
    "- **ê³µì •í•œ ë¶„ë°°**: ëª¨ë“  Featureì˜ SHAP ê°’ í•© = ì˜ˆì¸¡ê°’ - ê¸°ì¤€ê°’\n",
    "\n",
    "### TreeExplainer vs KernelExplainer\n",
    "\n",
    "- **TreeExplainer**: íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ì „ìš©, ë¹ ë¥´ê³  ì •í™•\n",
    "- **KernelExplainer**: ëª¨ë“  ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥, ëŠë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… FIX 2: ëª¨ë¸ íƒ€ì… ì²´í¬ ë° ì ì ˆí•œ Explainer ì„ íƒ\n",
    "print(\"SHAP Explainer ì´ˆê¸°í™” ì¤‘...\")\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ì—ì„œ classifier ì¶”ì¶œ\n",
    "classifier = final_model.named_steps['classifier']\n",
    "model_type = type(classifier).__name__\n",
    "\n",
    "print(f\"\\nâœ… ë¶„ë¥˜ê¸° íƒ€ì…: {model_type}\")\n",
    "\n",
    "# íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸\n",
    "tree_models = [\n",
    "    'XGBClassifier', 'LGBMClassifier', 'CatBoostClassifier',\n",
    "    'RandomForestClassifier', 'GradientBoostingClassifier',\n",
    "    'DecisionTreeClassifier', 'ExtraTreesClassifier'\n",
    "]\n",
    "\n",
    "# ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ Explainer ì„ íƒ\n",
    "if any(tree_model in model_type for tree_model in tree_models):\n",
    "    print(f\"âœ… íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ ê°ì§€ â†’ TreeExplainer ì‚¬ìš©\")\n",
    "    explainer = shap.TreeExplainer(classifier)\n",
    "    use_tree_explainer = True\n",
    "else:\n",
    "    print(f\"âš ï¸ {model_type}ëŠ” íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì´ ì•„ë‹™ë‹ˆë‹¤.\")\n",
    "    print(f\"   â†’ KernelExplainer ì‚¬ìš© (ê³„ì‚° ì‹œê°„ ì¦ê°€ ì˜ˆìƒ)\")\n",
    "    print(f\"   â†’ ë°°ê²½ ë°ì´í„°ë¡œ Train Setì—ì„œ 100ê°œ ìƒ˜í”Œë§\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¤€ë¹„ (KernelExplainerìš©)\n",
    "    preprocessor = final_model[:-1]\n",
    "    X_train_preprocessed = preprocessor.transform(X_train)\n",
    "    \n",
    "    # ë°°ê²½ ë°ì´í„° ìƒ˜í”Œë§\n",
    "    background_data = shap.sample(X_train_preprocessed, 100, random_state=RANDOM_STATE)\n",
    "    explainer = shap.KernelExplainer(classifier.predict_proba, background_data)\n",
    "    use_tree_explainer = False\n",
    "\n",
    "print(f\"\\nâœ… Explainer ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "if hasattr(explainer, 'expected_value'):\n",
    "    exp_val = explainer.expected_value\n",
    "    if isinstance(exp_val, (list, np.ndarray)):\n",
    "        print(f\"   - ê¸°ì¤€ê°’ (Expected Value): {exp_val[1]:.4f} (ë¶€ë„ í´ë˜ìŠ¤)\")\n",
    "    else:\n",
    "        print(f\"   - ê¸°ì¤€ê°’ (Expected Value): {exp_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¤€ë¹„\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ì˜ ì „ì²˜ë¦¬ ë‹¨ê³„ë§Œ ì ìš© (classifier ì œì™¸)\n",
    "preprocessor = final_model[:-1]\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "print(f\"   - Train Shape: {X_train_preprocessed.shape}\")\n",
    "print(f\"   - Test Shape: {X_test_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… FIX 4: ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§\n",
    "# SHAP values ê³„ì‚° (Test Set - í•„ìš”ì‹œ ìƒ˜í”Œë§)\n",
    "if len(X_test) > SHAP_MAX_SAMPLES:\n",
    "    print(f\"âš ï¸ Test Set í¬ê¸°({len(X_test):,}ê°œ)ê°€ SHAP_MAX_SAMPLES({SHAP_MAX_SAMPLES:,}ê°œ)ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"   â†’ {SHAP_MAX_SAMPLES:,}ê°œë¡œ ìƒ˜í”Œë§í•˜ì—¬ SHAP ê³„ì‚° (ì¸µí™” ì¶”ì¶œ)\")\n",
    "    \n",
    "    # ì¸µí™” ìƒ˜í”Œë§ (ë¶€ë„/ì •ìƒ ë¹„ìœ¨ ìœ ì§€)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    _, X_test_shap, _, y_test_shap = train_test_split(\n",
    "        X_test_preprocessed, y_test,\n",
    "        test_size=SHAP_MAX_SAMPLES / len(X_test),\n",
    "        stratify=y_test,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(X_test_shap):,}ê°œ (ë¶€ë„ìœ¨: {y_test_shap.mean():.2%})\")\n",
    "else:\n",
    "    X_test_shap = X_test_preprocessed\n",
    "    y_test_shap = y_test\n",
    "    print(f\"âœ… ì „ì²´ Test Set ì‚¬ìš©: {len(X_test_shap):,}ê°œ\")\n",
    "\n",
    "# SHAP ê³„ì‚°\n",
    "print(f\"\\nSHAP values ê³„ì‚° ì¤‘... (ìˆ˜ ë¶„ ì†Œìš” ê°€ëŠ¥)\")\n",
    "\n",
    "if use_tree_explainer:\n",
    "    shap_values = explainer.shap_values(X_test_shap)\n",
    "else:\n",
    "    # KernelExplainerëŠ” ê° ìƒ˜í”Œë³„ë¡œ ê³„ì‚° (ë” ëŠë¦¼)\n",
    "    shap_values = explainer.shap_values(X_test_shap, nsamples=100)\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ ì‹œ shap_valuesê°€ [class0, class1] í˜•íƒœë©´ class1ë§Œ ì‚¬ìš©\n",
    "if isinstance(shap_values, list):\n",
    "    print(f\"   - SHAP valuesëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ (í´ë˜ìŠ¤ë³„ ë¶„ë¦¬)\")\n",
    "    print(f\"   - Class 0 shape: {shap_values[0].shape}\")\n",
    "    print(f\"   - Class 1 shape: {shap_values[1].shape}\")\n",
    "    shap_values = shap_values[1]  # ë¶€ë„(1) í´ë˜ìŠ¤\n",
    "    print(f\"   - ë¶€ë„ í´ë˜ìŠ¤(1) SHAP values ì„ íƒ\")\n",
    "\n",
    "print(f\"\\nâœ… SHAP Values ê³„ì‚° ì™„ë£Œ\")\n",
    "print(f\"   - Shape: {shap_values.shape}\")\n",
    "print(f\"   - ìƒ˜í”Œ ìˆ˜: {shap_values.shape[0]:,}ê°œ\")\n",
    "print(f\"   - Feature ìˆ˜: {shap_values.shape[1]}ê°œ\")\n",
    "\n",
    "# ì›ë³¸ Test Setì— ëŒ€í•œ ì˜ˆì¸¡ (ì „ì²´ í‰ê°€ìš©)\n",
    "y_test_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"\\nâœ… ì „ì²´ Test Set ì˜ˆì¸¡ ì™„ë£Œ ({len(X_test):,}ê°œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 2: Global Feature Importance (Summary Plot)\n",
    "\n",
    "ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” Featureë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•´ì„ ê°€ì´ë“œ\n",
    "\n",
    "- **Yì¶•**: Feature (ì¤‘ìš”ë„ ìˆœì„œ, ìœ„ë¡œ ê°ˆìˆ˜ë¡ ì¤‘ìš”)\n",
    "- **Xì¶•**: SHAP Value (ì–‘ìˆ˜ = ë¶€ë„ ìœ„í—˜â†‘, ìŒìˆ˜ = ë¶€ë„ ìœ„í—˜â†“)\n",
    "- **ìƒ‰ìƒ**: Feature ê°’ (ë¹¨ê°• = ë†’ìŒ, íŒŒë‘ = ë‚®ìŒ)\n",
    "- **ì ì˜ ë¶„í¬**: ë„“ê²Œ í¼ì§ˆìˆ˜ë¡ ë‹¤ì–‘í•œ ì˜í–¥ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Plot (Beeswarm)\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_test_shap, \n",
    "    feature_names=X.columns.tolist(),\n",
    "    show=False,\n",
    "    max_display=20\n",
    ")\n",
    "plt.title('SHAP Summary Plot: Global Feature Importance', fontsize=16, pad=20, weight='bold')\n",
    "plt.xlabel('SHAP Value (ë¶€ë„ ìœ„í—˜ì— ëŒ€í•œ ì˜í–¥)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_SHAP_Summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Summary Plot ì €ì¥ ì™„ë£Œ: ë°œí‘œ_Part4_SHAP_Summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì£¼ìš” íŒ¨í„´ í•´ì„\n",
    "\n",
    "1. **ìƒë‹¨ Featureë“¤**: ëª¨ë¸ì˜ í•µì‹¬ ì˜ì‚¬ê²°ì • ìš”ì†Œ\n",
    "2. **ë¹¨ê°„ ì ì´ ì˜¤ë¥¸ìª½**: í•´ë‹¹ Feature ê°’ì´ ë†’ì„ìˆ˜ë¡ ë¶€ë„ ìœ„í—˜ ì¦ê°€\n",
    "3. **íŒŒë€ ì ì´ ì™¼ìª½**: í•´ë‹¹ Feature ê°’ì´ ë‚®ì„ìˆ˜ë¡ ë¶€ë„ ìœ„í—˜ ê°ì†Œ\n",
    "4. **ìˆ˜í‰ ë¶„í¬**: Featureì™€ ë¶€ë„ ìœ„í—˜ì˜ ë¹„ì„ í˜• ê´€ê³„ ì‹œì‚¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 3: Top 10 Feature ìƒì„¸ ë¶„ì„\n",
    "\n",
    "ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” 10ê°œ Featureì˜ ì¬ë¬´ì  ì˜ë¯¸ë¥¼ í•´ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Feature ì¶”ì¶œ\n",
    "feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "top10_idx = np.argsort(feature_importance)[-10:][::-1]\n",
    "top10_features = X.columns[top10_idx]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Top 10 ì¤‘ìš” Feature (Mean |SHAP|)\")\n",
    "print(\"=\"*80)\n",
    "for i, feat in enumerate(top10_features, 1):\n",
    "    importance = feature_importance[top10_idx[i-1]]\n",
    "    print(f\"{i:2d}. {feat:50s} {importance:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_test_shap, \n",
    "    feature_names=X.columns.tolist(),\n",
    "    plot_type='bar', \n",
    "    show=False, \n",
    "    max_display=10\n",
    ")\n",
    "plt.title('Top 10 Feature Importance (Mean |SHAP|)', fontsize=16, pad=20, weight='bold')\n",
    "plt.xlabel('í‰ê·  ì ˆëŒ€ SHAP ê°’', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_Top10_Features.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Top 10 Bar Plot ì €ì¥ ì™„ë£Œ: ë°œí‘œ_Part4_Top10_Features.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… FIX 3: ì¬ë¬´ì  ì˜ë¯¸ í•´ì„ - ì™¸ë¶€ íŒŒì¼ì—ì„œ ë¡œë“œ (ì—†ìœ¼ë©´ ìŠ¤í‚µ)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 10 Feature ì¬ë¬´ì  ì˜ë¯¸\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature ì„¤ëª… íŒŒì¼ ë¡œë“œ ì‹œë„\n",
    "feature_desc_path = '../data/features/feature_descriptions.json'\n",
    "if os.path.exists(feature_desc_path):\n",
    "    with open(feature_desc_path, 'r', encoding='utf-8') as f:\n",
    "        feature_descriptions = json.load(f)\n",
    "    print(\"âœ… Feature ì„¤ëª… íŒŒì¼ ë¡œë“œ ì™„ë£Œ\\n\")\n",
    "else:\n",
    "    print(\"âš ï¸ Feature ì„¤ëª… íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Feature ì´ë¦„ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.\")\n",
    "    print(f\"   ê²½ë¡œ: {feature_desc_path}\\n\")\n",
    "    feature_descriptions = {}\n",
    "\n",
    "# Top 10 Feature í•´ì„\n",
    "for i, feat in enumerate(top10_features, 1):\n",
    "    importance = feature_importance[top10_idx[i-1]]\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "    print(f\"    ì¤‘ìš”ë„: {importance:.4f}\")\n",
    "    \n",
    "    # ì„¤ëª…ì´ ìˆìœ¼ë©´ í‘œì‹œ\n",
    "    if feat in feature_descriptions:\n",
    "        desc = feature_descriptions[feat]\n",
    "        print(f\"    ì˜ë¯¸: {desc.get('meaning', 'N/A')}\")\n",
    "        print(f\"    ìœ„í—˜: {desc.get('risk', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "if not feature_descriptions:\n",
    "    print(\"\\nğŸ’¡ Feature ì„¤ëª… íŒŒì¼ ìƒì„± ë°©ë²•:\")\n",
    "    print(\"   {\")\n",
    "    print('     \"Featureì´ë¦„\": {')\n",
    "    print('       \"meaning\": \"ì¬ë¬´ì  ì˜ë¯¸\",')\n",
    "    print('       \"risk\": \"ë¶€ë„ ìœ„í—˜ê³¼ì˜ ê´€ê³„\"')\n",
    "    print(\"     }\")\n",
    "    print(\"   }\")\n",
    "    print(f\"   â†’ {feature_desc_path} íŒŒì¼ë¡œ ì €ì¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 4: SHAP Dependence Plot (ê°œë³„ Feature ë¶„ì„)\n",
    "\n",
    "Top 3 Featureì˜ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•´ì„ ê°€ì´ë“œ\n",
    "\n",
    "- **Xì¶•**: Feature ê°’\n",
    "- **Yì¶•**: SHAP Value (ë¶€ë„ ìœ„í—˜ì— ëŒ€í•œ ê¸°ì—¬ë„)\n",
    "- **ìƒ‰ìƒ**: ìƒí˜¸ì‘ìš© Feature (ìë™ ì„ íƒ)\n",
    "- **íŒ¨í„´**: ë¹„ì„ í˜• ê´€ê³„, ì„ê³„ê°’ íš¨ê³¼, ìƒí˜¸ì‘ìš© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 Featureì— ëŒ€í•œ Dependence Plot\n",
    "top3_features = top10_features[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for i, feat in enumerate(top3_features):\n",
    "    feat_idx = list(X.columns).index(feat)\n",
    "    shap.dependence_plot(\n",
    "        feat_idx, \n",
    "        shap_values, \n",
    "        X_test_shap,\n",
    "        feature_names=X.columns.tolist(), \n",
    "        ax=axes[i], \n",
    "        show=False\n",
    "    )\n",
    "    axes[i].set_title(f'{feat}', fontsize=14, weight='bold')\n",
    "    axes[i].set_xlabel('Feature ê°’', fontsize=11)\n",
    "    axes[i].set_ylabel('SHAP Value', fontsize=11)\n",
    "\n",
    "plt.suptitle('Top 3 Feature Dependence Plot: ë¹„ì„ í˜• ê´€ê³„ ë¶„ì„', \n",
    "             fontsize=16, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_Dependence_Plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Dependence Plot ì €ì¥ ì™„ë£Œ: ë°œí‘œ_Part4_Dependence_Plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì£¼ìš” íŒ¨í„´\n",
    "\n",
    "1. **ì„ í˜• vs ë¹„ì„ í˜•**: ì§ì„ /ê³¡ì„  íŒ¨í„´ìœ¼ë¡œ ê´€ê³„ ìœ í˜• íŒŒì•…\n",
    "2. **ì„ê³„ê°’ íš¨ê³¼**: íŠ¹ì • ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê¸‰ê²©í•œ ë³€í™”\n",
    "3. **ìƒí˜¸ì‘ìš©**: ìƒ‰ìƒ ê·¸ë¼ë°ì´ì…˜ì´ ëšœë ·í•˜ë©´ ë‹¤ë¥¸ Featureì™€ì˜ ìƒí˜¸ì‘ìš© ê°•í•¨\n",
    "4. **ì´ìƒì¹˜**: ê·¹ë‹¨ê°’ì—ì„œì˜ SHAP ê°’ ë³€í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 5: ê°œë³„ ê¸°ì—… ì‚¬ë¡€ ë¶„ì„ (Waterfall Plot)\n",
    "\n",
    "ë¶€ë„ ê¸°ì—… 1ê°œ, ì •ìƒ ê¸°ì—… 1ê°œì˜ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ìƒì„¸íˆ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Waterfall Plot í•´ì„\n",
    "\n",
    "- **Base Value**: ì „ì²´ í‰ê·  ì˜ˆì¸¡ê°’ (ëª¨ë“  ê¸°ì—…ì˜ í‰ê·  ë¶€ë„ í™•ë¥ )\n",
    "- **í™”ì‚´í‘œ**: ê° Featureê°€ ì˜ˆì¸¡ê°’ì„ ì¦ê°€/ê°ì†Œì‹œí‚¤ëŠ” ì •ë„\n",
    "- **ë¹¨ê°„ìƒ‰**: ë¶€ë„ ìœ„í—˜ ì¦ê°€\n",
    "- **íŒŒë€ìƒ‰**: ë¶€ë„ ìœ„í—˜ ê°ì†Œ\n",
    "- **ìµœì¢…ê°’ f(x)**: í•´ë‹¹ ê¸°ì—…ì˜ ì˜ˆì¸¡ í™•ë¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP ìƒ˜í”Œê³¼ ì „ì²´ Test Set ë§¤í•‘ (ìƒ˜í”Œë§ëœ ê²½ìš°)\n",
    "if len(X_test_shap) < len(X_test):\n",
    "    # ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "    shap_indices = y_test_shap.index.values\n",
    "    \n",
    "    # ë¶€ë„ ê¸°ì—… ì¤‘ í™•ë¥ ì´ ë†’ì€ ì‚¬ë¡€ (ìƒ˜í”Œ ë‚´ì—ì„œ)\n",
    "    bankrupt_mask = y_test_shap == 1\n",
    "    bankrupt_probs = y_test_prob[shap_indices][bankrupt_mask]\n",
    "    if len(bankrupt_probs) > 0:\n",
    "        high_risk_idx_in_shap = np.where(bankrupt_mask)[0][np.argmax(bankrupt_probs)]\n",
    "    else:\n",
    "        print(\"âš ï¸ ìƒ˜í”Œì— ë¶€ë„ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ì—ì„œ ì„ íƒ\")\n",
    "        bankrupt_idx_full = np.where(y_test == 1)[0]\n",
    "        high_risk_idx_in_shap = 0  # fallback\n",
    "    \n",
    "    # ì •ìƒ ê¸°ì—… ì¤‘ í™•ë¥ ì´ ë‚®ì€ ì‚¬ë¡€\n",
    "    normal_mask = y_test_shap == 0\n",
    "    normal_probs = y_test_prob[shap_indices][normal_mask]\n",
    "    if len(normal_probs) > 0:\n",
    "        low_risk_idx_in_shap = np.where(normal_mask)[0][np.argmin(normal_probs)]\n",
    "    else:\n",
    "        low_risk_idx_in_shap = 0  # fallback\n",
    "    \n",
    "    high_risk_prob = y_test_prob[shap_indices][high_risk_idx_in_shap]\n",
    "    low_risk_prob = y_test_prob[shap_indices][low_risk_idx_in_shap]\n",
    "else:\n",
    "    # ìƒ˜í”Œë§ ì•ˆ ëœ ê²½ìš° (ì „ì²´)\n",
    "    bankrupt_idx = np.where(y_test == 1)[0]\n",
    "    high_risk_idx_in_shap = bankrupt_idx[np.argmax(y_test_prob[bankrupt_idx])]\n",
    "    \n",
    "    normal_idx = np.where(y_test == 0)[0]\n",
    "    low_risk_idx_in_shap = normal_idx[np.argmin(y_test_prob[normal_idx])]\n",
    "    \n",
    "    high_risk_prob = y_test_prob[high_risk_idx_in_shap]\n",
    "    low_risk_prob = y_test_prob[low_risk_idx_in_shap]\n",
    "\n",
    "print(f\"ì„ íƒëœ ì‚¬ë¡€:\")\n",
    "print(f\"  - ë¶€ë„ ê¸°ì—… (ê³ ìœ„í—˜): SHAP ì¸ë±ìŠ¤ {high_risk_idx_in_shap}, ì˜ˆì¸¡ í™•ë¥  {high_risk_prob:.2%}\")\n",
    "print(f\"  - ì •ìƒ ê¸°ì—… (ì €ìœ„í—˜): SHAP ì¸ë±ìŠ¤ {low_risk_idx_in_shap}, ì˜ˆì¸¡ í™•ë¥  {low_risk_prob:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Value ì²˜ë¦¬\n",
    "if isinstance(explainer.expected_value, (list, np.ndarray)):\n",
    "    expected_value = explainer.expected_value[1]  # ë¶€ë„ í´ë˜ìŠ¤\n",
    "else:\n",
    "    expected_value = explainer.expected_value\n",
    "\n",
    "# Waterfall Plot: ë¶€ë„ ê¸°ì—…\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values[high_risk_idx_in_shap],\n",
    "        base_values=expected_value,\n",
    "        data=X_test_shap[high_risk_idx_in_shap],\n",
    "        feature_names=X.columns.tolist()\n",
    "    ),\n",
    "    show=False,\n",
    "    max_display=15\n",
    ")\n",
    "plt.title(f'ë¶€ë„ ê¸°ì—… ì˜ˆì¸¡ ê·¼ê±° (ì‹¤ì œ: ë¶€ë„, ì˜ˆì¸¡ í™•ë¥ : {high_risk_prob:.2%})', \n",
    "          fontsize=14, weight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_Waterfall_ë¶€ë„ê¸°ì—….png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ë¶€ë„ ê¸°ì—… Waterfall Plot ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall Plot: ì •ìƒ ê¸°ì—…\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values[low_risk_idx_in_shap],\n",
    "        base_values=expected_value,\n",
    "        data=X_test_shap[low_risk_idx_in_shap],\n",
    "        feature_names=X.columns.tolist()\n",
    "    ),\n",
    "    show=False,\n",
    "    max_display=15\n",
    ")\n",
    "plt.title(f'ì •ìƒ ê¸°ì—… ì˜ˆì¸¡ ê·¼ê±° (ì‹¤ì œ: ì •ìƒ, ì˜ˆì¸¡ í™•ë¥ : {low_risk_prob:.2%})', \n",
    "          fontsize=14, weight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_Waterfall_ì •ìƒê¸°ì—….png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ì •ìƒ ê¸°ì—… Waterfall Plot ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ë¡€ í•´ì„\n",
    "\n",
    "**ë¶€ë„ ê¸°ì—…**\n",
    "- Base Valueì—ì„œ ì‹œì‘í•˜ì—¬ ìœ„í—˜ ìš”ì¸(ë¹¨ê°„ìƒ‰)ì´ ëˆ„ì \n",
    "- ì£¼ìš” ìœ„í—˜ Featureë“¤ì´ ì˜ˆì¸¡ê°’ì„ ëŒì–´ì˜¬ë¦¼\n",
    "- ì¼ë¶€ ì–‘í˜¸í•œ ì§€í‘œ(íŒŒë€ìƒ‰)ë„ ìˆì§€ë§Œ ìœ„í—˜ ìš”ì¸ì´ ì••ë„\n",
    "\n",
    "**ì •ìƒ ê¸°ì—…**\n",
    "- ê±´ì „í•œ ì¬ë¬´ ì§€í‘œ(íŒŒë€ìƒ‰)ê°€ ì˜ˆì¸¡ê°’ì„ ë‚®ì¶¤\n",
    "- ìœ„í—˜ ìš”ì¸ì´ ìˆì–´ë„ ì „ì²´ì ìœ¼ë¡œ ì•ˆì •ì \n",
    "- ìµœì¢… ì˜ˆì¸¡ í™•ë¥ ì´ ë‚®ê²Œ ìœ ì§€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 6: Force Plot (Interactive Visualization)\n",
    "\n",
    "ì—¬ëŸ¬ ìƒ˜í”Œì˜ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ í•œ ëˆˆì— ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ì˜**: Force Plotì€ Jupyter ë…¸íŠ¸ë¶ì—ì„œ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ í‘œì‹œë©ë‹ˆë‹¤. HTML íŒŒì¼ë¡œ ì €ì¥ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶€ë„ ê¸°ì—… ì„ íƒ (ìƒ˜í”Œë§ ê³ ë ¤)\n",
    "if len(X_test_shap) < len(X_test):\n",
    "    # ìƒ˜í”Œë§ëœ ê²½ìš°: ìƒ˜í”Œ ë‚´ì—ì„œ ë¶€ë„ ê¸°ì—… ì°¾ê¸°\n",
    "    shap_indices = y_test_shap.index.values\n",
    "    bankrupt_mask_shap = y_test_shap == 1\n",
    "    if bankrupt_mask_shap.sum() >= 20:\n",
    "        # ë¶€ë„ ê¸°ì—…ì´ 20ê°œ ì´ìƒì´ë©´ ìƒìœ„ 20ê°œ ì„ íƒ\n",
    "        bankrupt_probs_shap = y_test_prob[shap_indices][bankrupt_mask_shap]\n",
    "        top20_indices = np.where(bankrupt_mask_shap)[0][np.argsort(bankrupt_probs_shap)[-20:]]\n",
    "    else:\n",
    "        # ë¶€ë„ ê¸°ì—…ì´ 20ê°œ ë¯¸ë§Œì´ë©´ ì „ë¶€ ì‚¬ìš©\n",
    "        top20_indices = np.where(bankrupt_mask_shap)[0]\n",
    "else:\n",
    "    # ìƒ˜í”Œë§ ì•ˆ ëœ ê²½ìš°: ì „ì²´ì—ì„œ ì„ íƒ\n",
    "    bankrupt_idx = np.where(y_test == 1)[0]\n",
    "    if len(bankrupt_idx) >= 20:\n",
    "        top20_indices = bankrupt_idx[np.argsort(y_test_prob[bankrupt_idx])[-20:]]\n",
    "    else:\n",
    "        top20_indices = bankrupt_idx\n",
    "\n",
    "print(f\"Force Plotìš© ë¶€ë„ ê¸°ì—…: {len(top20_indices)}ê°œ ì„ íƒ\")\n",
    "if len(top20_indices) > 0:\n",
    "    selected_probs = y_test_prob[y_test_shap.index[top20_indices]] if len(X_test_shap) < len(X_test) else y_test_prob[top20_indices]\n",
    "    print(f\"ì˜ˆì¸¡ í™•ë¥  ë²”ìœ„: {selected_probs.min():.2%} ~ {selected_probs.max():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Plot\n",
    "if len(top20_indices) > 0:\n",
    "    shap.initjs()\n",
    "    force_plot = shap.force_plot(\n",
    "        expected_value,\n",
    "        shap_values[top20_indices],\n",
    "        X_test_shap[top20_indices],\n",
    "        feature_names=X.columns.tolist()\n",
    "    )\n",
    "    \n",
    "    # ë…¸íŠ¸ë¶ì— í‘œì‹œ\n",
    "    display(force_plot)\n",
    "    \n",
    "    print(\"\\nâœ… Force Plot ìƒì„± ì™„ë£Œ (ìœ„ ì¸í„°ë™í‹°ë¸Œ í”Œë¡¯ ì°¸ì¡°)\")\n",
    "    print(\"   - ë¹¨ê°„ìƒ‰: ë¶€ë„ ìœ„í—˜ ì¦ê°€ Feature\")\n",
    "    print(\"   - íŒŒë€ìƒ‰: ë¶€ë„ ìœ„í—˜ ê°ì†Œ Feature\")\n",
    "    print(\"   - ê° í–‰ì€ í•˜ë‚˜ì˜ ê¸°ì—…ì„ ë‚˜íƒ€ëƒ„\")\n",
    "else:\n",
    "    print(\"âš ï¸ Force Plot ìƒì„±í•  ë¶€ë„ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ì„ íƒì‚¬í•­) HTMLë¡œ ì €ì¥\n",
    "if len(top20_indices) > 0:\n",
    "    try:\n",
    "        shap.save_html(\n",
    "            '../data/processed/ë°œí‘œ_Part4_Force_Plot.html',\n",
    "            force_plot\n",
    "        )\n",
    "        print(\"âœ… Force Plot HTML ì €ì¥ ì™„ë£Œ: ë°œí‘œ_Part4_Force_Plot.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ë…¸íŠ¸ë¶ì—ì„œ ì¸í„°ë™í‹°ë¸Œ í”Œë¡¯ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 7: Traffic Light êµ¬ê°„ë³„ SHAP íŒ¨í„´ ë¶„ì„\n",
    "\n",
    "ê° ìœ„í—˜ êµ¬ê°„(Red/Yellow/Green)ì—ì„œ ì–´ë–¤ Featureê°€ ì£¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Traffic Light ì‹œìŠ¤í…œ\n",
    "\n",
    "- **Red (ê³ ìœ„í—˜)**: ì˜ˆì¸¡ í™•ë¥  â‰¥ Red ì„ê³„ê°’\n",
    "- **Yellow (ì¤‘ìœ„í—˜)**: Yellow ì„ê³„ê°’ â‰¤ ì˜ˆì¸¡ í™•ë¥  < Red ì„ê³„ê°’\n",
    "- **Green (ì €ìœ„í—˜)**: ì˜ˆì¸¡ í™•ë¥  < Yellow ì„ê³„ê°’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ Test Setì— ëŒ€í•´ Traffic Light êµ¬ê°„ ë¶„ë¥˜\n",
    "red_threshold = thresholds['red']\n",
    "yellow_threshold = thresholds['yellow']\n",
    "\n",
    "red_mask = y_test_prob >= red_threshold\n",
    "yellow_mask = (y_test_prob >= yellow_threshold) & (y_test_prob < red_threshold)\n",
    "green_mask = y_test_prob < yellow_threshold\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Traffic Light êµ¬ê°„ë³„ ë¶„í¬ (ì „ì²´ Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Red (ê³ ìœ„í—˜):    {red_mask.sum():5d}ê°œ ({red_mask.mean():6.2%}) - ì„ê³„ê°’: {red_threshold:.4f}\")\n",
    "print(f\"Yellow (ì¤‘ìœ„í—˜): {yellow_mask.sum():5d}ê°œ ({yellow_mask.mean():6.2%}) - ì„ê³„ê°’: {yellow_threshold:.4f}\")\n",
    "print(f\"Green (ì €ìœ„í—˜):  {green_mask.sum():5d}ê°œ ({green_mask.mean():6.2%})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ê° êµ¬ê°„ì˜ ì‹¤ì œ ë¶€ë„ìœ¨\n",
    "print(\"\\nê° êµ¬ê°„ì˜ ì‹¤ì œ ë¶€ë„ìœ¨:\")\n",
    "if red_mask.sum() > 0:\n",
    "    print(f\"Red:    {y_test[red_mask].mean():.2%}\")\n",
    "if yellow_mask.sum() > 0:\n",
    "    print(f\"Yellow: {y_test[yellow_mask].mean():.2%}\")\n",
    "if green_mask.sum() > 0:\n",
    "    print(f\"Green:  {y_test[green_mask].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP ìƒ˜í”Œì—ì„œ êµ¬ê°„ë³„ SHAP ê°’ ê³„ì‚°\n",
    "# ìƒ˜í”Œë§ëœ ê²½ìš° ë§¤í•‘ í•„ìš”\n",
    "if len(X_test_shap) < len(X_test):\n",
    "    shap_indices = y_test_shap.index.values\n",
    "    y_test_prob_shap = y_test_prob[shap_indices]\n",
    "else:\n",
    "    y_test_prob_shap = y_test_prob\n",
    "\n",
    "# SHAP ìƒ˜í”Œ ê¸°ì¤€ êµ¬ê°„ ë¶„ë¥˜\n",
    "red_mask_shap = y_test_prob_shap >= red_threshold\n",
    "yellow_mask_shap = (y_test_prob_shap >= yellow_threshold) & (y_test_prob_shap < red_threshold)\n",
    "green_mask_shap = y_test_prob_shap < yellow_threshold\n",
    "\n",
    "print(f\"\\nSHAP ìƒ˜í”Œ ê¸°ì¤€ êµ¬ê°„ë³„ ë¶„í¬:\")\n",
    "print(f\"Red:    {red_mask_shap.sum()}ê°œ\")\n",
    "print(f\"Yellow: {yellow_mask_shap.sum()}ê°œ\")\n",
    "print(f\"Green:  {green_mask_shap.sum()}ê°œ\")\n",
    "\n",
    "# êµ¬ê°„ë³„ í‰ê·  SHAP ê°’\n",
    "segments = {\n",
    "    'Red (ê³ ìœ„í—˜)': red_mask_shap,\n",
    "    'Yellow (ì¤‘ìœ„í—˜)': yellow_mask_shap,\n",
    "    'Green (ì €ìœ„í—˜)': green_mask_shap\n",
    "}\n",
    "\n",
    "segment_shap_means = {}\n",
    "for seg_name, mask in segments.items():\n",
    "    if mask.sum() > 0:\n",
    "        segment_shap_means[seg_name] = np.abs(shap_values[mask]).mean(axis=0)\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df_segment = pd.DataFrame(segment_shap_means, index=X.columns).T\n",
    "\n",
    "# Top 10 Feature (ì „ì²´ í‰ê·  ê¸°ì¤€)\n",
    "top10_seg_features = df_segment.mean(axis=0).nlargest(10).index\n",
    "\n",
    "print(f\"\\nêµ¬ê°„ë³„ Top 10 Feature (ì „ì²´ í‰ê·  ê¸°ì¤€):\")\n",
    "for i, feat in enumerate(top10_seg_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(14, 7))\n",
    "df_plot = df_segment[top10_seg_features].T\n",
    "ax = df_plot.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "\n",
    "plt.title('Traffic Light êµ¬ê°„ë³„ Top 10 Feature Importance', fontsize=16, weight='bold', pad=20)\n",
    "plt.xlabel('Feature', fontsize=12)\n",
    "plt.ylabel('Mean |SHAP Value|', fontsize=12)\n",
    "plt.legend(title='ìœ„í—˜ êµ¬ê°„', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_Segment_SHAP.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… êµ¬ê°„ë³„ SHAP ë¶„ì„ ì €ì¥ ì™„ë£Œ: ë°œí‘œ_Part4_Segment_SHAP.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµ¬ê°„ë³„ ì°¨ì´ ë¶„ì„\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"êµ¬ê°„ë³„ Feature Importance ì°¨ì´\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'Red (ê³ ìœ„í—˜)' in df_segment.index and 'Green (ì €ìœ„í—˜)' in df_segment.index:\n",
    "    diff = df_segment.loc['Red (ê³ ìœ„í—˜)'] - df_segment.loc['Green (ì €ìœ„í—˜)']\n",
    "    top5_diff = diff.nlargest(5)\n",
    "    \n",
    "    print(\"\\nRed vs Green ì°¨ì´ê°€ í° Feature (Redì—ì„œ ë” ì¤‘ìš”):\")\n",
    "    for i, (feat, val) in enumerate(top5_diff.items(), 1):\n",
    "        print(f\"{i}. {feat:50s} +{val:.4f}\")\n",
    "    \n",
    "    bottom5_diff = diff.nsmallest(5)\n",
    "    print(\"\\nRed vs Green ì°¨ì´ê°€ í° Feature (Greenì—ì„œ ë” ì¤‘ìš”):\")\n",
    "    for i, (feat, val) in enumerate(bottom5_diff.items(), 1):\n",
    "        print(f\"{i}. {feat:50s} {val:.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "- **Red êµ¬ê°„**: êµ¬ì¡°ì  ë¬¸ì œ (ìë³¸ì ì‹, ì´ìë³´ìƒ ë“±) ì¤‘ìš”\n",
    "- **Yellow êµ¬ê°„**: ìœ ë™ì„± ê²½ê³  ì‹ í˜¸ (ìœ ë™ë¹„ìœ¨, ë§¤ì¶œì±„ê¶Œ íšŒì „ìœ¨ ë“±)\n",
    "- **Green êµ¬ê°„**: ì „ë°˜ì ìœ¼ë¡œ ê±´ì „í•œ ì¬ë¬´ ì§€í‘œ\n",
    "- **ì°¨ì´ ë¶„ì„**: ê° êµ¬ê°„ì„ êµ¬ë¶„í•˜ëŠ” í•µì‹¬ Feature íŒŒì•…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 8: Bootstrap ì‹ ë¢°êµ¬ê°„ ì¶”ê°€ â­\n",
    "\n",
    "SHAP Feature Importanceì˜ í†µê³„ì  ì•ˆì •ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Bootstrap ë°©ë²•ë¡ \n",
    "\n",
    "- ë³µì› ì¶”ì¶œë¡œ ìƒ˜í”Œ ìƒì„± (ê¸°ë³¸ 1,000íšŒ)\n",
    "- ê° ìƒ˜í”Œì—ì„œ Feature Importance ê³„ì‚°\n",
    "- 95% ì‹ ë¢°êµ¬ê°„ ì‚°ì¶œ\n",
    "- ì‹ ë¢°êµ¬ê°„ì´ ì¢ì„ìˆ˜ë¡ ì•ˆì •ì ì¸ Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap\n",
    "n_bootstrap = BOOTSTRAP_ITERATIONS\n",
    "bootstrap_importance = []\n",
    "\n",
    "print(f\"Bootstrap ì‹œì‘: {n_bootstrap:,}íšŒ ë°˜ë³µ\")\n",
    "print(\"ì§„í–‰ ìƒí™©:\")\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # ë³µì› ì¶”ì¶œ\n",
    "    n_samples = shap_values.shape[0]\n",
    "    indices = resample(range(n_samples), n_samples=n_samples, random_state=i)\n",
    "    shap_boot = shap_values[indices]\n",
    "    \n",
    "    # Feature Importance ê³„ì‚°\n",
    "    importance = np.abs(shap_boot).mean(axis=0)\n",
    "    bootstrap_importance.append(importance)\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  {i + 1:4d}/{n_bootstrap} ì™„ë£Œ ({(i+1)/n_bootstrap*100:.0f}%)\")\n",
    "\n",
    "bootstrap_importance = np.array(bootstrap_importance)\n",
    "\n",
    "print(f\"\\nâœ… Bootstrap ì™„ë£Œ\")\n",
    "print(f\"   - Shape: {bootstrap_importance.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% ì‹ ë¢°êµ¬ê°„\n",
    "lower = np.percentile(bootstrap_importance, 2.5, axis=0)\n",
    "upper = np.percentile(bootstrap_importance, 97.5, axis=0)\n",
    "mean_importance = bootstrap_importance.mean(axis=0)\n",
    "\n",
    "# Top 10 Feature CI\n",
    "top10_idx_boot = np.argsort(mean_importance)[-10:][::-1]\n",
    "top10_feat_boot = X.columns[top10_idx_boot]\n",
    "\n",
    "df_ci = pd.DataFrame({\n",
    "    'Feature': top10_feat_boot,\n",
    "    'Mean': mean_importance[top10_idx_boot],\n",
    "    'Lower': lower[top10_idx_boot],\n",
    "    'Upper': upper[top10_idx_boot],\n",
    "    'CI_Width': upper[top10_idx_boot] - lower[top10_idx_boot]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"Top 10 Feature Importance with Bootstrap 95% CI\")\n",
    "print(\"=\"*100)\n",
    "print(df_ci.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "print(\"\\nì‹ ë¢°êµ¬ê°„ í­(CI_Width)ì´ ì‘ì„ìˆ˜ë¡ ì•ˆì •ì ì¸ Featureì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "y_pos = np.arange(len(df_ci))\n",
    "plt.barh(\n",
    "    y_pos, \n",
    "    df_ci['Mean'], \n",
    "    xerr=[df_ci['Mean'] - df_ci['Lower'], df_ci['Upper'] - df_ci['Mean']],\n",
    "    capsize=5, \n",
    "    alpha=0.7, \n",
    "    color='steelblue',\n",
    "    ecolor='darkred',\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "plt.yticks(y_pos, df_ci['Feature'])\n",
    "plt.xlabel('Mean |SHAP Value| (95% CI)', fontsize=12, weight='bold')\n",
    "plt.title('Top 10 Feature Importance with Bootstrap Confidence Interval', \n",
    "          fontsize=16, weight='bold', pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/ë°œí‘œ_Part4_Bootstrap_CI.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Bootstrap CI í”Œë¡¯ ì €ì¥ ì™„ë£Œ: ë°œí‘œ_Part4_Bootstrap_CI.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í†µê³„ì  ì•ˆì •ì„± í‰ê°€\n",
    "\n",
    "- ì‹ ë¢°êµ¬ê°„ì´ ì¢ì€ Feature: ìƒ˜í”Œë§ì— ê´€ê³„ì—†ì´ ì¼ê´€ë˜ê²Œ ì¤‘ìš”\n",
    "- ì‹ ë¢°êµ¬ê°„ì´ ë„“ì€ Feature: ìƒ˜í”Œì— ë”°ë¼ ì¤‘ìš”ë„ê°€ ë³€ë™\n",
    "- ëª¨ë¸ ë°°í¬ ì‹œ ì•ˆì •ì ì¸ Featureë¥¼ ìš°ì„  ê³ ë ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 9: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ì¢…í•©\n",
    "\n",
    "ì˜ì‚¬ê²°ì •ìë¥¼ ìœ„í•œ í•µì‹¬ ë©”ì‹œì§€ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ìš”ì•½ í…Œì´ë¸”\n",
    "summary = pd.DataFrame({\n",
    "    'í•­ëª©': [\n",
    "        'ëª¨ë¸',\n",
    "        'Test PR-AUC',\n",
    "        'Test Recall',\n",
    "        'Test F2-Score',\n",
    "        'ì„ íƒëœ ì„ê³„ê°’',\n",
    "        'Red êµ¬ê°„ ê¸°ì—… ìˆ˜',\n",
    "        'Yellow êµ¬ê°„ ê¸°ì—… ìˆ˜',\n",
    "        'Green êµ¬ê°„ ê¸°ì—… ìˆ˜',\n",
    "        'Top 1 Feature',\n",
    "        'Top 2 Feature',\n",
    "        'Top 3 Feature'\n",
    "    ],\n",
    "    'ê°’': [\n",
    "        results['model_name'],\n",
    "        f\"{results['test_pr_auc']:.4f}\",\n",
    "        f\"{results['test_recall']:.2%}\",\n",
    "        f\"{results['test_f2']:.4f}\",\n",
    "        f\"{thresholds['selected']:.4f}\",\n",
    "        f\"{red_mask.sum():,}ê°œ ({red_mask.mean():.2%})\",\n",
    "        f\"{yellow_mask.sum():,}ê°œ ({yellow_mask.mean():.2%})\",\n",
    "        f\"{green_mask.sum():,}ê°œ ({green_mask.mean():.2%})\",\n",
    "        top10_features[0],\n",
    "        top10_features[1],\n",
    "        top10_features[2]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë° ìœ„í—˜ êµ¬ê°„ ë¶„í¬\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸\n",
    "top3_importance_sum = feature_importance[top10_idx[:3]].sum()\n",
    "total_importance = feature_importance.sum()\n",
    "top3_contribution = top3_importance_sum / total_importance * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. í•µì‹¬ ìœ„í—˜ ì§€í‘œ\")\n",
    "print(f\"   - Top 3 Featureê°€ ì „ì²´ ì˜ˆì¸¡ì˜ {top3_contribution:.1f}% ì„¤ëª…\")\n",
    "print(f\"   - 1ìˆœìœ„: {top3_features[0]}\")\n",
    "print(f\"   - 2ìˆœìœ„: {top3_features[1]}\")\n",
    "print(f\"   - 3ìˆœìœ„: {top3_features[2]}\")\n",
    "print(\"   âœ ì´ 3ê°œ ì§€í‘œë§Œ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§í•´ë„ ëŒ€ë¶€ë¶„ì˜ ë¶€ë„ ìœ„í—˜ íŒŒì•… ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n2. ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ\")\n",
    "print(f\"   - Yellow êµ¬ê°„: {yellow_mask.sum():,}ê°œ ê¸°ì—… (ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ)\")\n",
    "print(f\"   - Red êµ¬ê°„: {red_mask.sum():,}ê°œ ê¸°ì—… (ì¦‰ê° ëŒ€ì‘ í•„ìš”)\")\n",
    "if yellow_mask.sum() > 0:\n",
    "    print(f\"   - Yellow êµ¬ê°„ ì‹¤ì œ ë¶€ë„ìœ¨: {y_test[yellow_mask].mean():.2%}\")\n",
    "if red_mask.sum() > 0:\n",
    "    print(f\"   - Red êµ¬ê°„ ì‹¤ì œ ë¶€ë„ìœ¨: {y_test[red_mask].mean():.2%}\")\n",
    "print(\"   âœ Yellow êµ¬ê°„ ê¸°ì—… ëŒ€ìƒ ì‚¬ì „ ê°œì…ìœ¼ë¡œ ë¶€ë„ ì˜ˆë°© ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n3. í•´ì„ ê°€ëŠ¥ì„±\")\n",
    "print(\"   - SHAPìœ¼ë¡œ ê°œë³„ ê¸°ì—…ì˜ ë¶€ë„ ìœ„í—˜ ê·¼ê±° ëª…í™•íˆ ì œì‹œ\")\n",
    "print(\"   - ê¸ˆìœµê¸°ê´€: ëŒ€ì¶œ ì‹¬ì‚¬ ê·¼ê±° ë§ˆë ¨ (ì„¤ëª… ê°€ëŠ¥í•œ AI)\")\n",
    "print(\"   - íˆ¬ìì: í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ ê´€ë¦¬ (ìœ„í—˜ ìš”ì¸ ì‚¬ì „ íŒŒì•…)\")\n",
    "print(\"   - ê·œì œê¸°ê´€: ê³µì •ì„± ë° íˆ¬ëª…ì„± í™•ë³´ (ë¸”ë™ë°•ìŠ¤ í•´ì†Œ)\")\n",
    "print(\"   âœ ë‹¨ìˆœ ì ìˆ˜ê°€ ì•„ë‹Œ 'ì™œ ê·¸ëŸ°ì§€' ì„¤ëª… ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n4. ì‹¤ë¬´ í™œìš© ë°©ì•ˆ\")\n",
    "print(\"   - ì‹ ê·œ ëŒ€ì¶œ ì‹¬ì‚¬: Red/Yellow ê¸°ì—… ê±°ì ˆ ë˜ëŠ” ê³ ê¸ˆë¦¬ ì ìš©\")\n",
    "print(\"   - ê¸°ì¡´ ì—¬ì‹  ê´€ë¦¬: Yellow â†’ Red ì´ë™ ê¸°ì—… ì¡°ê¸° ë°œê²¬\")\n",
    "print(\"   - ê²½ì˜ ì»¨ì„¤íŒ…: Waterfall Plotìœ¼ë¡œ ê°œì„  í¬ì¸íŠ¸ ì œì‹œ\")\n",
    "print(\"   - ì—…ì¢…ë³„ ì „ëµ: êµ¬ê°„ë³„ Feature ì°¨ì´ ë¶„ì„ìœ¼ë¡œ ë§ì¶¤ ëŒ€ì‘\")\n",
    "\n",
    "print(\"\\n5. ë¹„ìš©-íš¨ê³¼ ë¶„ì„\")\n",
    "recall = results['test_recall']\n",
    "precision = results.get('test_precision', 0.1)  # ê¸°ë³¸ê°’\n",
    "print(f\"   - Recall {recall:.1%}: ì‹¤ì œ ë¶€ë„ ê¸°ì—…ì˜ {recall:.1%} ì‚¬ì „ íƒì§€\")\n",
    "print(f\"   - ëŒ€ì† íšŒí”¼ ê°€ì¹˜: ì˜ˆìƒ ë¶€ë„ì•¡ì˜ {recall:.1%} ì ˆê° ê°€ëŠ¥\")\n",
    "print(f\"   - False Positive: ì •ìƒ ê¸°ì—… ì¤‘ ì¼ë¶€ ì˜¤íŒ (Yellow êµ¬ê°„ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì™„í™”)\")\n",
    "print(\"   âœ ëŒ€ì† ë°©ì§€ í¸ìµ >> ëª¨ë‹ˆí„°ë§ ë¹„ìš©\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 10: ëª¨ë¸ í•œê³„ì  ë° ê°œì„  ë°©í–¥\n",
    "\n",
    "### 10.1 ë°ì´í„° í•œê³„\n",
    "\n",
    "#### ì‹œì  ì œì•½\n",
    "- **í˜„í™©**: 2021ë…„ 8ì›” ë‹¨ì¼ ì‹œì  ìŠ¤ëƒ…ìƒ· ë°ì´í„°\n",
    "- **ë¬¸ì œ**: ì‹œê³„ì—´ íŒ¨í„´ (ì¬ë¬´ ìƒíƒœ ì¶”ì´) ë°˜ì˜ ë¶ˆê°€\n",
    "- **ê°œì„  ë°©í–¥**: \n",
    "  - ë‹¤ë…„ë„ íŒ¨ë„ ë°ì´í„° ìˆ˜ì§‘ (2018-2024ë…„)\n",
    "  - ë³€í™”ìœ¨(Î”) Feature ì¶”ê°€ (YoY ë§¤ì¶œ ì¦ê°€ìœ¨, ë¶€ì±„ë¹„ìœ¨ ë³€í™” ë“±)\n",
    "\n",
    "#### í‘œë³¸ í¸í–¥\n",
    "- **í˜„í™©**: ì™¸ê° ê¸°ì—… ì¤‘ì‹¬ (ì†Œê·œëª¨ ê¸°ì—… ê³¼ì†Œ ëŒ€í‘œ)\n",
    "- **ë¬¸ì œ**: ìƒì¡´ í¸í–¥ (ì´ë¯¸ íì—…í•œ ê¸°ì—… ë¯¸í¬í•¨)\n",
    "- **ê°œì„  ë°©í–¥**:\n",
    "  - ë¹„ì™¸ê° ê¸°ì—… ë°ì´í„° ë³´ì™„\n",
    "  - íì—… ê¸°ì—… ì‚¬í›„ ë°ì´í„° ìˆ˜ì§‘\n",
    "\n",
    "### 10.2 ëª¨ë¸ í•œê³„\n",
    "\n",
    "#### í´ë˜ìŠ¤ ë¶ˆê· í˜•\n",
    "- **í˜„í™©**: ë¶€ë„ìœ¨ 1.5% â†’ SMOTEë¡œ ì™„í™”í–ˆìœ¼ë‚˜ Precision ë‚®ìŒ\n",
    "- **ë¬¸ì œ**: Recall 80% ë‹¬ì„± ì‹œ Precision 5-10%\n",
    "- **ê°œì„  ë°©í–¥**: Cost-Sensitive Learning, Focal Loss\n",
    "\n",
    "#### Feature ëˆ„ë½\n",
    "- **í˜„í™©**: ê±°ì‹œê²½ì œ ë³€ìˆ˜ (ê¸ˆë¦¬, GDP) ë¯¸í¬í•¨\n",
    "- **ê°œì„  ë°©í–¥**: ì™¸ë¶€ ë°ì´í„° ê²°í•©, Industry-Specific Features\n",
    "\n",
    "### 10.3 í•´ì„ í•œê³„\n",
    "\n",
    "#### SHAP ê³„ì‚° ë¹„ìš©\n",
    "- **í˜„í™©**: ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œëŠ” ëŠë¦¼\n",
    "- **ê°œì„  ë°©í–¥**: Approximate SHAP, ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹±\n",
    "\n",
    "#### ìƒí˜¸ì‘ìš© íš¨ê³¼\n",
    "- **í˜„í™©**: ê°œë³„ Feature ê¸°ì—¬ë„ë§Œ í‘œì‹œ\n",
    "- **ê°œì„  ë°©í–¥**: SHAP Interaction Values ì¶”ê°€ ë¶„ì„\n",
    "\n",
    "### 10.4 ìš´ì˜ í•œê³„\n",
    "\n",
    "#### ëª¨ë¸ ë“œë¦¬í”„íŠ¸\n",
    "- **í˜„í™©**: ê²½ì œ í™˜ê²½ ë³€í™” ì‹œ ì„±ëŠ¥ ì €í•˜\n",
    "- **ê°œì„  ë°©í–¥**: ë¶„ê¸°ë³„ Monitoring, ìë™ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "#### ê·œì œ ì¤€ìˆ˜\n",
    "- **í˜„í™©**: Basel III ì¶©ì¡± ì—¬ë¶€ ë¯¸ê²€ì¦\n",
    "- **ê°œì„  ë°©í–¥**: Regulatory Compliance Check, Fairness Metrics\n",
    "\n",
    "### 10.5 ë¹„ì¦ˆë‹ˆìŠ¤ í•œê³„\n",
    "\n",
    "#### ì˜¤ë¶„ë¥˜ ë¹„ìš© ë¶ˆê· í˜•\n",
    "- **Type II Error** (ë¶€ë„â†’ì •ìƒ ì˜¤íŒ): ëŒ€ì† ë°œìƒ (í° ë¹„ìš©)\n",
    "- **ê°œì„  ë°©í–¥**: ì‹¤ì œ ëŒ€ì†ìœ¨ ê¸°ë°˜ Cost Matrix ì„¤ê³„\n",
    "\n",
    "#### ì„¤ëª… ê°€ëŠ¥ì„± vs ì„±ëŠ¥\n",
    "- **í˜„í™©**: Tree ëª¨ë¸ ì„ íƒ â†’ ë”¥ëŸ¬ë‹ë³´ë‹¤ ì„±ëŠ¥ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
    "- **ê°œì„  ë°©í–¥**: TabNet, FT-Transformer ì‹¤í—˜\n",
    "\n",
    "### 10.6 í–¥í›„ ì—°êµ¬ ë°©í–¥\n",
    "\n",
    "1. **ì‹œê³„ì—´ í™•ì¥**: LSTM/Transformer ê¸°ë°˜ ë‹¤ë…„ë„ ì˜ˆì¸¡\n",
    "2. **ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**: ë¶€ë„ + ì‹ ìš©ë“±ê¸‰ + ì¬ë¬´ì¡°ì‘ ë™ì‹œ ì˜ˆì¸¡\n",
    "3. **ê°•ê±´ì„± í‰ê°€**: Adversarial Examples, OOD Detection\n",
    "4. **Causal Inference**: ì •ì±… ê°œì… íš¨ê³¼ ì˜ˆì¸¡\n",
    "5. **ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ**: FastAPI + Redis + Celery ì•„í‚¤í…ì²˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ì„¹ì…˜ 11: ìµœì¢… ì €ì¥\n",
    "\n",
    "ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "analysis_results = {\n",
    "    'model_info': {\n",
    "        'model_name': results['model_name'],\n",
    "        'test_pr_auc': float(results['test_pr_auc']),\n",
    "        'test_recall': float(results['test_recall']),\n",
    "        'test_f2': float(results['test_f2'])\n",
    "    },\n",
    "    'thresholds': {\n",
    "        'selected': float(thresholds['selected']),\n",
    "        'red': float(thresholds['red']),\n",
    "        'yellow': float(thresholds['yellow'])\n",
    "    },\n",
    "    'feature_importance': {\n",
    "        'top10_features': top10_features.tolist(),\n",
    "        'importance_values': {\n",
    "            feat: float(feature_importance[list(X.columns).index(feat)])\n",
    "            for feat in top10_features\n",
    "        },\n",
    "        'top3_contribution_pct': float(top3_contribution)\n",
    "    },\n",
    "    'segment_distribution': {\n",
    "        'red': {\n",
    "            'count': int(red_mask.sum()),\n",
    "            'percentage': float(red_mask.mean()),\n",
    "            'actual_bankruptcy_rate': float(y_test[red_mask].mean()) if red_mask.sum() > 0 else 0\n",
    "        },\n",
    "        'yellow': {\n",
    "            'count': int(yellow_mask.sum()),\n",
    "            'percentage': float(yellow_mask.mean()),\n",
    "            'actual_bankruptcy_rate': float(y_test[yellow_mask].mean()) if yellow_mask.sum() > 0 else 0\n",
    "        },\n",
    "        'green': {\n",
    "            'count': int(green_mask.sum()),\n",
    "            'percentage': float(green_mask.mean()),\n",
    "            'actual_bankruptcy_rate': float(y_test[green_mask].mean()) if green_mask.sum() > 0 else 0\n",
    "        }\n",
    "    },\n",
    "    'bootstrap_ci': df_ci.to_dict('records'),\n",
    "    'shap_summary': {\n",
    "        'n_samples': int(shap_values.shape[0]),\n",
    "        'n_features': int(shap_values.shape[1]),\n",
    "        'expected_value': float(expected_value)\n",
    "    }\n",
    "}\n",
    "\n",
    "# JSON ì €ì¥\n",
    "output_path = '../data/processed/ë°œí‘œ_Part4_SHAP_ë¶„ì„ê²°ê³¼.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ\")\n",
    "print(f\"   - íŒŒì¼: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±ëœ íŒŒì¼ ëª©ë¡\n",
    "output_files = [\n",
    "    'ë°œí‘œ_Part4_SHAP_Summary.png',\n",
    "    'ë°œí‘œ_Part4_Top10_Features.png',\n",
    "    'ë°œí‘œ_Part4_Dependence_Plot.png',\n",
    "    'ë°œí‘œ_Part4_Waterfall_ë¶€ë„ê¸°ì—….png',\n",
    "    'ë°œí‘œ_Part4_Waterfall_ì •ìƒê¸°ì—….png',\n",
    "    'ë°œí‘œ_Part4_Segment_SHAP.png',\n",
    "    'ë°œí‘œ_Part4_Bootstrap_CI.png',\n",
    "    'ë°œí‘œ_Part4_SHAP_ë¶„ì„ê²°ê³¼.json'\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… Part 4 ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nìƒì„±ëœ íŒŒì¼ (../data/processed/):\")\n",
    "for i, f in enumerate(output_files, 1):\n",
    "    file_path = os.path.join(PROCESSED_DIR, f)\n",
    "    exists = \"âœ“\" if os.path.exists(file_path) else \"âœ—\"\n",
    "    print(f\"  {exists} {i}. {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ë‹¤ìŒ ë‹¨ê³„: ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°œí‘œ ìë£Œ ì‘ì„±\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ ìƒì„±ëœ ì‹œê°í™” ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "- [x] Summary Plot (Beeswarm) - Global Feature Importance\n",
    "- [x] Top 10 Bar Chart - ì£¼ìš” Feature ìˆœìœ„\n",
    "- [x] Dependence Plot (Top 3) - ë¹„ì„ í˜• ê´€ê³„ ë¶„ì„\n",
    "- [x] Waterfall Plot (ë¶€ë„ ê¸°ì—…) - ê°œë³„ ì˜ˆì¸¡ ê·¼ê±°\n",
    "- [x] Waterfall Plot (ì •ìƒ ê¸°ì—…) - ëŒ€ì¡° ë¶„ì„\n",
    "- [x] Traffic Light êµ¬ê°„ë³„ SHAP - ìœ„í—˜ êµ¬ê°„ë³„ íŒ¨í„´\n",
    "- [x] Bootstrap ì‹ ë¢°êµ¬ê°„ - í†µê³„ì  ì•ˆì •ì„±\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- SHAP ê³µì‹ ë¬¸ì„œ: https://shap.readthedocs.io/\n",
    "- Lundberg, S. M., & Lee, S. I. (2017). \"A Unified Approach to Interpreting Model Predictions.\" NeurIPS.\n",
    "- Molnar, C. (2022). \"Interpretable Machine Learning.\"\n",
    "- Shapley, L. S. (1953). \"A value for n-person games.\"\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ í•µì‹¬ ê²°ë¡ \n",
    "\n",
    "1. **ëª¨ë¸ ì„±ëŠ¥**: PR-AUC ê¸°ì¤€ ìš°ìˆ˜í•œ ì˜ˆì¸¡ë ¥ ë‹¬ì„±\n",
    "2. **í•´ì„ ê°€ëŠ¥ì„±**: SHAPìœ¼ë¡œ ë¸”ë™ë°•ìŠ¤ í•´ì†Œ, ì„¤ëª… ê°€ëŠ¥í•œ AI êµ¬í˜„\n",
    "3. **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜**: Traffic Light ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤ë¬´ í™œìš© ê°€ëŠ¥\n",
    "4. **í†µê³„ì  ì•ˆì •ì„±**: Bootstrap CIë¡œ Feature Importance ì‹ ë¢°ì„± ê²€ì¦\n",
    "5. **ê°œì„  ë°©í–¥**: ì‹œê³„ì—´ í™•ì¥, ì™¸ë¶€ ë°ì´í„° ê²°í•©, ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 5ê°€ì§€ ê°œì„ ì‚¬í•­ ì ìš© ì™„ë£Œ\n",
    "\n",
    "1. âœ… **display() import ì¶”ê°€**: `from IPython.display import display`\n",
    "2. âœ… **ëª¨ë¸ íƒ€ì… ì²´í¬**: TreeExplainer vs KernelExplainer ìë™ ì„ íƒ\n",
    "3. âœ… **ì¬ë¬´ í•´ì„ í•˜ë“œì½”ë”© ì œê±°**: `feature_descriptions.json` ì™¸ë¶€ íŒŒì¼ë¡œ ë¶„ë¦¬\n",
    "4. âœ… **ìƒ˜í”Œë§ ì˜µì…˜ ì¶”ê°€**: í™˜ê²½ë³€ìˆ˜ë¡œ `SHAP_MAX_SAMPLES` ê´€ë¦¬\n",
    "5. âœ… **Feature ìˆ˜ ê²€ì¦**: 20-30ê°œ ë²”ìœ„ ìë™ ì²´í¬\n",
    "\n",
    "---\n",
    "\n",
    "**ê°ì‚¬í•©ë‹ˆë‹¤!** ğŸ™"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}