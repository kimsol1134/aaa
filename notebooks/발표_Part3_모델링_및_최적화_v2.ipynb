{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n\n## 11. ìµœì¢… ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n\n### ğŸ“Š Part 3 í•µì‹¬ ì„±ê³¼\n\nâœ… **Data Leakage ì™„ì „ ì œê±°**\n- Test Setì€ ìµœì¢… í‰ê°€ ë‹¨ í•œ ë²ˆë§Œ ì‚¬ìš©\n- ëª¨ë“  ì˜ì‚¬ê²°ì •ì€ Train/Validationì—ì„œë§Œ ìˆ˜í–‰\n\nâœ… **3-Way Split ì„±ê³µ**\n- Train (60%) / Validation (20%) / Test (20%)\n- ëª¨ë“  ì„¸íŠ¸ì—ì„œ ë¶€ë„ìœ¨ ê· ì¼ ìœ ì§€\n\nâœ… **ë¦¬ìƒ˜í”Œë§ ì „ëµ ì„ íƒ**\n- SMOTE vs Class Weight ëŒ€ì¡° ì‹¤í—˜\n- Validation Set ê¸°ë°˜ ì„ íƒ\n\nâœ… **ìµœì  ëª¨ë¸ ì„ íƒ**\n- 4ê°œ ëª¨ë¸ íŠœë‹ ë° ë¹„êµ\n- Validation Set ê¸°ë°˜ ìµœì¢… ì„ íƒ\n\nâœ… **ì„ê³„ê°’ ìµœì í™”**\n- F2-Score ìµœì  ì„ê³„ê°’\n- Recall 80% ë³´ì¥ ì„ê³„ê°’\n- Validation Setì—ì„œë§Œ ê²°ì •\n\nâœ… **Traffic Light ì‹œìŠ¤í…œ**\n- ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’ (Recall 80%/95%)\n- ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ì§€ì›\n\nâœ… **ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸**\n- Test Set PR-AUC, Recall, F2-Score\n- Traffic Light ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨\n\n### ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (Part 4)\n\n- SHAP ë¶„ì„ì„ í†µí•œ ëª¨ë¸ í•´ì„\n- ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…\n- ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ\n\n---\n\n**ë…¸íŠ¸ë¶ ì‘ì„± ì™„ë£Œ!** ğŸ‰",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ì €ì¥í•  íŒŒì¼ë“¤\nsave_files = {\n    # ëª¨ë¸\n    f'{OUTPUT_PREFIX}_ìµœì¢…ëª¨ë¸.pkl': final_model,\n    f'{OUTPUT_PREFIX}_ë¶„ë¥˜ê¸°.pkl': classifier,  # SHAPìš©\n    \n    # ë°ì´í„°\n    f'{OUTPUT_PREFIX}_X_train.csv': X_train,\n    f'{OUTPUT_PREFIX}_X_val.csv': X_val,\n    f'{OUTPUT_PREFIX}_X_test.csv': X_test,\n    f'{OUTPUT_PREFIX}_y_train.csv': pd.DataFrame({'target': y_train}),\n    f'{OUTPUT_PREFIX}_y_val.csv': pd.DataFrame({'target': y_val}),\n    f'{OUTPUT_PREFIX}_y_test.csv': pd.DataFrame({'target': y_test}),\n    \n    # ì„ê³„ê°’ ë° ê²°ê³¼\n    f'{OUTPUT_PREFIX}_ì„ê³„ê°’.pkl': {\n        'selected_threshold': selected_threshold,\n        'red_threshold': red_threshold,\n        'yellow_threshold': yellow_threshold,\n        'f2_optimal_threshold': f2_optimal_threshold\n    },\n    \n    f'{OUTPUT_PREFIX}_ê²°ê³¼ìš”ì•½.pkl': {\n        'model_name': final_model_name,\n        'validation_pr_auc': best_val_pr_auc,\n        'test_pr_auc': test_pr_auc,\n        'test_recall': test_recall,\n        'test_precision': test_precision,\n        'test_f2_score': test_f2_score,\n        'test_type2_error': test_type2_error,\n        'risk_captured_test': risk_captured_test\n    }\n}\n\n# íŒŒì¼ ì €ì¥\nprint(\"=\" * 70)\nprint(\"ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...\")\nprint(\"=\" * 70)\n\nfor filename, data in save_files.items():\n    filepath = os.path.join(PROCESSED_DIR, filename)\n    \n    if filename.endswith('.pkl'):\n        joblib.dump(data, filepath)\n    elif filename.endswith('.csv'):\n        data.to_csv(filepath, index=False, encoding='utf-8-sig')\n    \n    print(f\"âœ… {filename}\")\n\nprint(\"=\" * 70)\nprint(f\"\\\\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {PROCESSED_DIR}\")\nprint(\"âœ… ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 10. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥\n\n### Part 4 SHAP ë¶„ì„ì„ ìœ„í•œ íŒŒì¼ ì €ì¥",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Feature Importance ì¶”ì¶œ (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„)\nclassifier = final_model.named_steps['classifier']\n\nif hasattr(classifier, 'feature_importances_'):\n    # Tree ê¸°ë°˜ ëª¨ë¸ (LightGBM, XGBoost, CatBoost, RandomForest)\n    importance = classifier.feature_importances_\n    feature_names = X_train.columns\nelif hasattr(classifier, 'coef_'):\n    # ì„ í˜• ëª¨ë¸ (Logistic Regression)\n    importance = np.abs(classifier.coef_[0])\n    feature_names = X_train.columns\nelse:\n    print(\"âš ï¸  Feature Importanceë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\")\n    importance = None\n    feature_names = None\n\nif importance is not None:\n    # Feature Importance DataFrame\n    feat_imp_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importance\n    }).sort_values('Importance', ascending=False).reset_index(drop=True)\n    \n    print(\"=\" * 70)\n    print(f\"ğŸ† Top 15 Feature Importance ({final_model_name})\")\n    print(\"=\" * 70)\n    print(feat_imp_df.head(15).to_string(index=False))\n    print(\"=\" * 70)\n    \n    # ì‹œê°í™”\n    fig = go.Figure()\n    \n    top_n = 15\n    top_features = feat_imp_df.head(top_n)\n    \n    fig.add_trace(go.Bar(\n        x=top_features['Importance'],\n        y=top_features['Feature'],\n        orientation='h',\n        marker=dict(\n            color=top_features['Importance'],\n            colorscale='Blues',\n            showscale=True,\n            colorbar=dict(title='Importance')\n        )\n    ))\n    \n    fig.update_layout(\n        title=f'Top {top_n} Feature Importance ({final_model_name})',\n        xaxis_title='Importance',\n        yaxis_title='Feature',\n        height=600,\n        yaxis={'categoryorder':'total ascending'}\n    )\n    \n    fig.show()\n    \n    print(\"\\\\nğŸ“Š Feature Importance ë¶„ì„ ì™„ë£Œ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 9. Feature Importance ë¶„ì„\n\n### Top íŠ¹ì„± ì‹œê°í™”",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test Setì— Traffic Light ì ìš© (Validationì—ì„œ ê²°ì •ëœ ì„ê³„ê°’ ì‚¬ìš©)\ntraffic_light_test = pd.Series(y_test_prob).apply(\n    lambda x: assign_traffic_light(x, red_threshold, yellow_threshold)\n)\n\n# ë“±ê¸‰ë³„ ì„±ëŠ¥ ë¶„ì„\ntraffic_summary_test = []\nfor grade in ['Red', 'Yellow', 'Green']:\n    mask = (traffic_light_test == grade)\n    count = mask.sum()\n    bankruptcy_count = y_test.values[mask].sum()\n    bankruptcy_rate = y_test.values[mask].mean() if count > 0 else 0\n    \n    traffic_summary_test.append({\n        'ë“±ê¸‰': grade,\n        'ê¸°ì—… ìˆ˜': count,\n        'ë¹„ìœ¨': count / len(y_test),\n        'ì‹¤ì œ ë¶€ë„': bankruptcy_count,\n        'ì •ë°€ë„': bankruptcy_rate,\n        'í¬ì°©ë¥ ': bankruptcy_count / y_test.sum() if y_test.sum() > 0 else 0\n    })\n\ntraffic_df_test = pd.DataFrame(traffic_summary_test)\n\nprint(\"=\" * 70)\nprint(\"ğŸš¦ Traffic Light ì‹œìŠ¤í…œ (Test Set ìµœì¢… í‰ê°€)\")\nprint(\"=\" * 70)\nprint(f\"Red ì„ê³„ê°’:    {red_threshold:.4f} (Validationì—ì„œ ê²°ì •)\")\nprint(f\"Yellow ì„ê³„ê°’: {yellow_threshold:.4f} (Validationì—ì„œ ê²°ì •)\")\nprint(\"=\" * 70)\nprint(traffic_df_test.to_string(index=False))\nprint(\"=\" * 70)\n\n# ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨\nrisk_captured_test = traffic_df_test[traffic_df_test['ë“±ê¸‰'].isin(['Red', 'Yellow'])]['í¬ì°©ë¥ '].sum()\nprint(f\"\\nâœ… ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨: {risk_captured_test:.1%} (Red+Yellowì—ì„œ ë¶€ë„ í¬ì°©)\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.2 Traffic Light ì‹œìŠ¤í…œ Test Set í‰ê°€",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test Set ì˜ˆì¸¡ (Validationì—ì„œ ê²°ì •ëœ ëª¨ë¸ ì‚¬ìš©)\ny_test_prob = final_model.predict_proba(X_test)[:, 1]\ny_test_pred = (y_test_prob >= selected_threshold).astype(int)\n\n# ë©”íŠ¸ë¦­ ê³„ì‚°\ntest_pr_auc = average_precision_score(y_test, y_test_prob)\ntest_roc_auc = roc_auc_score(y_test, y_test_prob)\ntest_f2_score = fbeta_score(y_test, y_test_pred, beta=2)\n\n# Confusion Matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\ntest_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\ntest_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\ntest_type2_error = fn / (tp + fn) if (tp + fn) > 0 else 0\n\nprint(\"=\" * 70)\nprint(\"ğŸ¯ Test Set ìµœì¢… í‰ê°€ ê²°ê³¼\")\nprint(\"=\" * 70)\nprint(f\"âš ï¸  ì´ ê²°ê³¼ëŠ” ìµœì¢… ë³´ê³ ìš©ì´ë©°, ì´ì „ ë‹¨ê³„ì—ì„œ Test Setì„\")\nprint(f\"   ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤.\")\nprint(f\"\\nì‚¬ìš© ëª¨ë¸:      {final_model_name}\")\nprint(f\"ì‚¬ìš© ì„ê³„ê°’:    {selected_threshold:.4f} (Validationì—ì„œ ê²°ì •)\")\nprint(\"=\" * 70)\nprint(f\"\\nPR-AUC:         {test_pr_auc:.4f}\")\nprint(f\"ROC-AUC:        {test_roc_auc:.4f}\")\nprint(f\"F2-Score:       {test_f2_score:.4f}\")\nprint(f\"Precision:      {test_precision:.2%}\")\nprint(f\"Recall:         {test_recall:.2%}\")\nprint(f\"Type II Error:  {test_type2_error:.2%}\")\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"  TN: {tn:,}  |  FP: {fp:,}\")\nprint(f\"  FN: {fn:,}  |  TP: {tp:,}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.1 Test Set ì˜ˆì¸¡ (Validationì—ì„œ ê²°ì •ëœ ëª¨ë¸ ë° ì„ê³„ê°’ ì ìš©)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. ğŸ¯ Test Set ìµœì¢… í‰ê°€ (ë‹¨ í•œ ë²ˆë§Œ!) ğŸ¯\n\n### âš ï¸âš ï¸âš ï¸ ì¤‘ìš”í•œ ê³µì§€ âš ï¸âš ï¸âš ï¸\n\n```\nì´ ì„¹ì…˜ë¶€í„° Test Setì„ ì²˜ìŒìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤!\n\nâœ… ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ì˜ì‚¬ê²°ì •:\n   - ë¦¬ìƒ˜í”Œë§ ì „ëµ ì„ íƒ\n   - ëª¨ë¸ ì„ íƒ\n   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n   - ì„ê³„ê°’ ìµœì í™”\n   - Traffic Light ì„ê³„ê°’\n   â†’ ëª¨ë‘ Train/Validation Setì—ì„œë§Œ ìˆ˜í–‰ë¨!\n\nâœ… Test Set ì‚¬ìš© ëª©ì :\n   - ìµœì¢… ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸\n   - ì‹¤ì „ ë°°í¬ ì„±ëŠ¥ ì¶”ì •\n   - ì˜ì‚¬ê²°ì • ì•„ë‹˜, í‰ê°€ë§Œ!\n\nâš ï¸  ì´ ê²°ê³¼ë¥¼ ë³´ê³  ëª¨ë¸ì„ ë‹¤ì‹œ íŠœë‹í•˜ë©´ Data Leakage!\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Validation Setì— Traffic Light ì ìš© (âš ï¸ Test Set ì‚¬ìš© ì•ˆ í•¨!)\ndef assign_traffic_light(prob, red_th, yellow_th):\n    if prob >= red_th:\n        return 'Red'\n    elif prob >= yellow_th:\n        return 'Yellow'\n    else:\n        return 'Green'\n\n# Traffic Light ë“±ê¸‰ í• ë‹¹\ntraffic_light_val = pd.Series(y_val_prob_final).apply(\n    lambda x: assign_traffic_light(x, red_threshold, yellow_threshold)\n)\n\n# ë“±ê¸‰ë³„ ì„±ëŠ¥ ë¶„ì„\ntraffic_summary = []\nfor grade in ['Red', 'Yellow', 'Green']:\n    mask = (traffic_light_val == grade)\n    count = mask.sum()\n    bankruptcy_count = y_val[mask].sum()\n    bankruptcy_rate = y_val[mask].mean() if count > 0 else 0\n    \n    traffic_summary.append({\n        'ë“±ê¸‰': grade,\n        'ê¸°ì—… ìˆ˜': count,\n        'ë¹„ìœ¨': count / len(y_val),\n        'ì‹¤ì œ ë¶€ë„': bankruptcy_count,\n        'ì •ë°€ë„': bankruptcy_rate,\n        'í¬ì°©ë¥ ': bankruptcy_count / y_val.sum() if y_val.sum() > 0 else 0\n    })\n\ntraffic_df = pd.DataFrame(traffic_summary)\n\nprint(\"=\" * 70)\nprint(\"ğŸš¦ Traffic Light ì‹œìŠ¤í…œ ì„±ëŠ¥ (Validation Set)\")\nprint(\"=\" * 70)\nprint(traffic_df.to_string(index=False))\nprint(\"=\" * 70)\n\n# ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨\nrisk_captured = traffic_df[traffic_df['ë“±ê¸‰'].isin(['Red', 'Yellow'])]['í¬ì°©ë¥ '].sum()\nprint(f\"\\nâœ… ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨: {risk_captured:.1%} (Red+Yellowì—ì„œ ë¶€ë„ í¬ì°©)\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Traffic Light ì„±ëŠ¥ ê²€ì¦ (Validation Set)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Traffic Light ì„ê³„ê°’ ê³„ì‚° (Validation Set ê¸°ë°˜)\n# Red: Recall 80% ë³´ì¥ (ì´ë¯¸ ê³„ì‚°ë¨)\nred_threshold = recall_80_threshold if recall_80_threshold is not None else selected_threshold\n\n# Yellow: Recall 95% ë³´ì¥\ntarget_recall_yellow = 0.95\nidx_recall_95 = np.where(recalls_val[:-1] >= target_recall_yellow)[0]\n\nif len(idx_recall_95) > 0:\n    recall_95_idx = idx_recall_95[np.argmax(precisions_val[:-1][idx_recall_95])]\n    yellow_threshold = thresholds_val[recall_95_idx]\n    yellow_precision = precisions_val[recall_95_idx]\n    yellow_recall = recalls_val[recall_95_idx]\nelse:\n    # Recall 95% ë‹¬ì„± ë¶ˆê°€ëŠ¥í•˜ë©´ ê°€ì¥ ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš©\n    yellow_threshold = thresholds_val[-1] if len(thresholds_val) > 0 else 0.01\n    yellow_precision = precisions_val[-1] if len(precisions_val) > 1 else 0\n    yellow_recall = recalls_val[-1] if len(recalls_val) > 1 else 1\n\nprint(\"=\" * 70)\nprint(\"ğŸš¦ Traffic Light ì„ê³„ê°’ (Validation Set ê¸°ë°˜)\")\nprint(\"=\" * 70)\nprint(f\"ğŸ”´ Red (High Risk):   í™•ë¥  >= {red_threshold:.4f}\")\nprint(f\"   ëª©í‘œ: ë¶€ë„ ê¸°ì—…ì˜ 80% í¬ì°©\")\nprint(f\"\\nğŸŸ¡ Yellow (Medium):    {yellow_threshold:.4f} <= í™•ë¥  < {red_threshold:.4f}\")\nprint(f\"   ëª©í‘œ: ë¶€ë„ ê¸°ì—…ì˜ 95% í¬ì°© (Red+Yellow)\")\nprint(f\"\\nğŸŸ¢ Green (Low Risk):  í™•ë¥  < {yellow_threshold:.4f}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.1 Traffic Light ì„ê³„ê°’ ê³„ì‚° (Validation Set)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. â­ Traffic Light ì‹œìŠ¤í…œ (ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’) â­\n\n### ì„ê³„ê°’ ì„¤ì • ë…¼ë¦¬ (Validation Set ê¸°ë°˜)\n\n```\nğŸ”´ Red (High Risk):    Recall 80% ë³´ì¥ ì„ê³„ê°’ ì´ìƒ\nğŸŸ¡ Yellow (Medium):    Recall 95% ë³´ì¥ ì„ê³„ê°’ ì´ìƒ\nğŸŸ¢ Green (Low Risk):   ê·¸ ì™¸\n```\n\n**ë…¼ë¦¬ì  ê·¼ê±°**: ë¶€ë„ ê¸°ì—…ì˜ 80%ëŠ” Red, 95%ëŠ” Red+Yellowì—ì„œ í¬ì°©",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Recall 80% ì´ìƒ ë‹¬ì„±í•˜ëŠ” ì„ê³„ê°’ ì¤‘ Precision ìµœëŒ€\ntarget_recall = 0.80\nidx_recall_80 = np.where(recalls_val[:-1] >= target_recall)[0]\n\nif len(idx_recall_80) > 0:\n    recall_80_idx = idx_recall_80[np.argmax(precisions_val[:-1][idx_recall_80])]\n    recall_80_threshold = thresholds_val[recall_80_idx]\n    recall_80_precision = precisions_val[recall_80_idx]\n    recall_80_recall = recalls_val[recall_80_idx]\n    recall_80_f2 = f2_scores[recall_80_idx]\n    \n    print(\"=\" * 70)\n    print(f\"ğŸ“Š Recall {target_recall:.0%} ë³´ì¥ ì„ê³„ê°’ (Validation Set ê¸°ë°˜)\")\n    print(\"=\" * 70)\n    print(f\"ì„ê³„ê°’:   {recall_80_threshold:.4f}\")\n    print(f\"Precision: {recall_80_precision:.2%}\")\n    print(f\"Recall:    {recall_80_recall:.2%}\")\n    print(f\"F2-Score:  {recall_80_f2:.4f}\")\n    print(\"=\" * 70)\nelse:\n    recall_80_threshold = None\n    print(\"âš ï¸  Recall 80% ë‹¬ì„± ë¶ˆê°€ëŠ¥\")\n\n# ìµœì¢… ì‚¬ìš© ì„ê³„ê°’ ì„ íƒ\nselected_threshold = recall_80_threshold if recall_80_threshold is not None else f2_optimal_threshold\nselected_threshold_type = f\"Recall {target_recall:.0%} ë³´ì¥\" if recall_80_threshold is not None else \"F2-Score ìµœì \"\n\nprint(f\"\\nâœ… ìµœì¢… ì„ íƒ ì„ê³„ê°’: {selected_threshold:.4f} ({selected_threshold_type})\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.2 Recall 80% ë³´ì¥ ì„ê³„ê°’ (Validation Set)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Validation Setì—ì„œ Precision-Recall ì»¤ë¸Œ ê³„ì‚° (âš ï¸ Test Set ì‚¬ìš© ì•ˆ í•¨!)\nprecisions_val, recalls_val, thresholds_val = precision_recall_curve(y_val, y_val_prob_final)\n\n# F2-Score ê³„ì‚° (Recall ìš°ì„ , beta=2)\nbeta = 2\nf2_scores = (1 + beta**2) * (precisions_val[:-1] * recalls_val[:-1]) / \\\n            (beta**2 * precisions_val[:-1] + recalls_val[:-1] + 1e-10)\n\n# F2-Score ìµœëŒ€ ì„ê³„ê°’\nf2_optimal_idx = np.argmax(f2_scores)\nf2_optimal_threshold = thresholds_val[f2_optimal_idx]\nf2_optimal_score = f2_scores[f2_optimal_idx]\nf2_optimal_precision = precisions_val[f2_optimal_idx]\nf2_optimal_recall = recalls_val[f2_optimal_idx]\n\nprint(\"=\" * 70)\nprint(\"ğŸ“Š F2-Score ìµœì  ì„ê³„ê°’ (Validation Set ê¸°ë°˜)\")\nprint(\"=\" * 70)\nprint(f\"ì„ê³„ê°’:   {f2_optimal_threshold:.4f}\")\nprint(f\"F2-Score: {f2_optimal_score:.4f}\")\nprint(f\"Precision: {f2_optimal_precision:.2%}\")\nprint(f\"Recall:    {f2_optimal_recall:.2%}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.1 F2-Score ìµœì  ì„ê³„ê°’ (Validation Set)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. â­ ì„ê³„ê°’ ìµœì í™” (Validation Set ê¸°ë°˜) â­\n\n### í•µì‹¬ ì›ì¹™\n\n```\nâœ… Validation Setì—ì„œë§Œ ì„ê³„ê°’ ìµœì í™”\nâœ… F2-Score ìµœì í™” (Recall ìš°ì„ )\nâœ… Recall 80% ëª©í‘œ ì„ê³„ê°’ë„ ê³„ì‚°\nâš ï¸ Test Setì—ëŠ” ê²°ì •ëœ ì„ê³„ê°’ì„ \"ì ìš©\"ë§Œ í•¨!\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ìµœì¢… ëª¨ë¸ ì„ íƒ (ë‹¨ì¼ ëª¨ë¸ ìš°ì„ )\nfinal_model = tuning_results[best_val_model_name]['best_estimator']\nfinal_model_name = best_val_model_name\n\nprint(\"=\" * 70)\nprint(\"âœ… ìµœì¢… ëª¨ë¸ ì„ íƒ ì™„ë£Œ\")\nprint(\"=\" * 70)\nprint(f\"ëª¨ë¸: {final_model_name}\")\nprint(f\"ì„ íƒ ê·¼ê±°: Validation Setì—ì„œ PR-AUC ìµœê³  ì„±ëŠ¥ ({best_val_pr_auc:.4f})\")\nprint(f\"ì¶”ê°€ ê³ ë ¤ì‚¬í•­: Part 4 SHAP ë¶„ì„ì„ ìœ„í•´ ë‹¨ì¼ ëª¨ë¸ ì„ íƒ\")\nprint(\"=\" * 70)\n\n# ì„ íƒëœ ëª¨ë¸ì˜ Validation í™•ë¥  ì €ì¥\ny_val_prob_final = final_model.predict_proba(X_val)[:, 1]\n\nprint(f\"\\nğŸ“Š ìµœì¢… ëª¨ë¸ì˜ Validation Set ì„±ëŠ¥:\")\nprint(val_results_df[val_results_df['Model'] == final_model_name].T.to_string(header=False))\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 ìµœì¢… ëª¨ë¸ ì„ íƒ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 70)\nprint(\"ğŸ“Š ì „ì²´ ëª¨ë¸ Validation Set í‰ê°€ (âš ï¸ Test Set ì‚¬ìš© ì•ˆ í•¨!)\")\nprint(\"=\" * 70)\n\n# Validation í‰ê°€ ê²°ê³¼ ì €ì¥\nvalidation_results = []\n\nfor model_name, result in tuning_results.items():\n    model = result['best_estimator']\n    \n    # Validation Set ì˜ˆì¸¡ (âš ï¸ Test Set ì ˆëŒ€ ì‚¬ìš© ì•ˆ í•¨!)\n    y_val_prob = model.predict_proba(X_val)[:, 1]\n    y_val_pred = (y_val_prob >= 0.5).astype(int)  # ê¸°ë³¸ ì„ê³„ê°’ 0.5\n    \n    # ë©”íŠ¸ë¦­ ê³„ì‚°\n    pr_auc = average_precision_score(y_val, y_val_prob)\n    roc_auc = roc_auc_score(y_val, y_val_prob)\n    f2_score = fbeta_score(y_val, y_val_pred, beta=2)\n    \n    # Confusion Matrix\n    tn, fp, fn, tp = confusion_matrix(y_val, y_val_pred).ravel()\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    type2_error = fn / (tp + fn) if (tp + fn) > 0 else 0\n    \n    validation_results.append({\n        'Model': model_name,\n        'PR_AUC': pr_auc,\n        'ROC_AUC': roc_auc,\n        'F2_Score': f2_score,\n        'Precision': precision,\n        'Recall': recall,\n        'Type_II_Error': type2_error,\n        'TP': tp,\n        'FP': fp,\n        'TN': tn,\n        'FN': fn\n    })\n\n# DataFrameìœ¼ë¡œ ë³€í™˜\nval_results_df = pd.DataFrame(validation_results).sort_values('PR_AUC', ascending=False).reset_index(drop=True)\n\nprint(val_results_df[['Model', 'PR_AUC', 'F2_Score', 'Recall', 'Type_II_Error']].to_string(index=False))\nprint(\"=\" * 70)\n\n# ìµœê³  ì„±ëŠ¥ ëª¨ë¸\nbest_val_model_name = val_results_df.iloc[0]['Model']\nbest_val_pr_auc = val_results_df.iloc[0]['PR_AUC']\n\nprint(f\"\\nâœ… Validation Set ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_val_model_name}\")\nprint(f\"   PR-AUC: {best_val_pr_auc:.4f}\")\nprint(f\"   F2-Score: {val_results_df.iloc[0]['F2_Score']:.4f}\")\nprint(f\"   Recall: {val_results_df.iloc[0]['Recall']:.2%}\")\nprint(f\"   Type II Error: {val_results_df.iloc[0]['Type_II_Error']:.2%}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.1 ì „ì²´ ëª¨ë¸ Validation Set í‰ê°€",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. â­ ëª¨ë¸ ì„ íƒ (Validation Set ê¸°ë°˜) â­\n\n### í•µì‹¬ ì›ì¹™\n\n```\nâœ… Validation Setì—ì„œë§Œ í‰ê°€\nâœ… Test Set ì ˆëŒ€ ì‚¬ìš© ì•ˆ í•¨!\nâœ… PR-AUC, F2-Score, Recall ì¢…í•© í‰ê°€\nâœ… ë‹¨ì¼ ëª¨ë¸ ìš°ì„  (Part 4 SHAP ë¶„ì„ ê³ ë ¤)\n```\n\n**ë¹„ìœ **: ëª¨ì˜ê³ ì‚¬(Validation)ë¡œ ìµœì¢… ëª¨ë¸ì„ ì„ íƒí•˜ê³ , ìˆ˜ëŠ¥(Test)ì€ ë‹¨ í•œ ë²ˆë§Œ!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ê²°ê³¼ ìš”ì•½ DataFrame\nsummary_df = pd.DataFrame({\n    'Model': list(tuning_results.keys()),\n    'CV_PR_AUC': [result['best_cv_score'] for result in tuning_results.values()]\n}).sort_values('CV_PR_AUC', ascending=False).reset_index(drop=True)\n\nprint(\"=\" * 70)\nprint(\"ğŸ“Š ëª¨ë¸ë³„ CV PR-AUC ìˆœìœ„ (Train Set 5-Fold CV)\")\nprint(\"=\" * 70)\nprint(summary_df.to_string(index=False))\nprint(\"=\" * 70)\n\n# ìµœê³  ì„±ëŠ¥ ëª¨ë¸\nbest_model_name = summary_df.iloc[0]['Model']\nbest_model = tuning_results[best_model_name]['best_estimator']\nbest_cv_score = summary_df.iloc[0]['CV_PR_AUC']\n\nprint(f\"\\nâœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\nprint(f\"   CV PR-AUC: {best_cv_score:.4f}\")\nprint(\"=\" * 70)\n\n# ì‹œê°í™”\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x=summary_df['Model'],\n    y=summary_df['CV_PR_AUC'],\n    text=[f\"{score:.4f}\" for score in summary_df['CV_PR_AUC']],\n    textposition='outside',\n    marker=dict(\n        color=summary_df['CV_PR_AUC'],\n        colorscale='Blues',\n        showscale=True,\n        colorbar=dict(title='PR-AUC')\n    )\n))\n\nfig.update_layout(\n    title='ëª¨ë¸ë³„ CV PR-AUC ì„±ëŠ¥ ë¹„êµ (Train Set 5-Fold CV)',\n    xaxis_title='ëª¨ë¸',\n    yaxis_title='PR-AUC',\n    height=500\n)\n\nfig.show()\n\nprint(\"\\nâš ï¸  ì´ ê²°ê³¼ëŠ” Train Set CV ê¸°ë°˜ì…ë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ ì„ íƒì€ Validation Setì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 íŠœë‹ ê²°ê³¼ ìš”ì•½",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜\nmodels_to_tune = {\n    'LightGBM': (lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1), lgbm_param_grid),\n    'XGBoost': (xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'), xgb_param_grid),\n    'CatBoost': (CatBoostClassifier(random_state=RANDOM_STATE, verbose=0), catboost_param_grid),\n    'LogisticRegression': (LogisticRegression(random_state=RANDOM_STATE, max_iter=1000), lr_param_grid)\n}\n\n# íŠœë‹ ê²°ê³¼ ì €ì¥\ntuning_results = {}\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nscorer = make_scorer(average_precision_score, needs_proba=True)\n\nprint(\"=\" * 70)\nprint(\"AutoML í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ (Train + 5-Fold CV)\")\nprint(\"=\" * 70)\n\nfor model_name, (model, param_grid) in models_to_tune.items():\n    print(f\"\\nğŸ”„ {model_name} íŠœë‹ ì¤‘...\")\n    start_time = datetime.now()\n    \n    # íŒŒì´í”„ë¼ì¸ ìƒì„±\n    pipeline = create_pipeline(model, use_resampler=selected_resampler)\n    \n    # íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œë¥¼ íŒŒì´í”„ë¼ì¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n    pipeline_param_grid = {f'classifier__{k}': v for k, v in param_grid.items()}\n    \n    # RandomizedSearchCV\n    search = RandomizedSearchCV(\n        estimator=pipeline,\n        param_distributions=pipeline_param_grid,\n        n_iter=30,  # ë¹ ë¥¸ íƒìƒ‰ì„ ìœ„í•´ 30íšŒë¡œ ì œí•œ\n        scoring=scorer,\n        cv=cv,\n        n_jobs=-1,\n        random_state=RANDOM_STATE,\n        verbose=0\n    )\n    \n    # Train Setì—ì„œë§Œ í•™ìŠµ (âš ï¸ Test Set ì‚¬ìš© ì•ˆ í•¨!)\n    search.fit(X_train, y_train)\n    \n    # ê²°ê³¼ ì €ì¥\n    tuning_results[model_name] = {\n        'best_estimator': search.best_estimator_,\n        'best_params': search.best_params_,\n        'best_cv_score': search.best_score_,  # CV PR-AUC\n        'cv_results': search.cv_results_\n    }\n    \n    elapsed = (datetime.now() - start_time).total_seconds()\n    print(f\"   âœ… ì™„ë£Œ ({elapsed:.1f}ì´ˆ)\")\n    print(f\"   ìµœê³  CV PR-AUC: {search.best_score_:.4f}\")\n    print(f\"   ìµœì  íŒŒë¼ë¯¸í„°: {search.best_params_}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"AutoML íŠœë‹ ì™„ë£Œ\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 ëª¨ë¸ë³„ íŠœë‹ ì‹¤í–‰ (Train + CVë§Œ ì‚¬ìš©)\n\n**ì¤‘ìš”**: Test Setì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ (ê°„ëµí™”)\nlgbm_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [31, 63, 127],\n    'min_child_samples': [20, 50, 100],\n    'scale_pos_weight': [scale_pos_weight] if selected_resampler is None else [1]\n}\n\n# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\nxgb_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'min_child_weight': [1, 3, 5],\n    'subsample': [0.8, 0.9, 1.0],\n    'scale_pos_weight': [scale_pos_weight] if selected_resampler is None else [1]\n}\n\n# CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\ncatboost_param_grid = {\n    'iterations': [100, 200, 300],\n    'depth': [4, 6, 8],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'l2_leaf_reg': [1, 3, 5],\n    'scale_pos_weight': [scale_pos_weight] if selected_resampler is None else [1]\n}\n\n# Logistic Regression í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\nlr_param_grid = {\n    'C': [0.01, 0.1, 1, 10],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear'],\n    'class_weight': ['balanced'] if selected_resampler is None else [None]\n}\n\nprint(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜ ì™„ë£Œ\")\nprint(f\"   ì„ íƒëœ ë¦¬ìƒ˜í”Œë§ ì „ëµ: {selected_strategy}\")\nprint(f\"   Class Weight ì ìš©: {'Yes' if selected_resampler is None else 'No'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.1 í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. AutoML: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n\n### íŠœë‹ ì „ëµ\n\nâœ… **ì„ íƒëœ ë¦¬ìƒ˜í”Œë§ ì „ëµ** ì‚¬ìš©  \nâœ… **Train Set + 5-Fold CV**ë¡œë§Œ í•™ìŠµ  \nâœ… **RandomizedSearchCV** (ë¹ ë¥¸ íƒìƒ‰)  \nâœ… **PR-AUC** ìµœì í™”  \nâš ï¸ **Test Set ì ˆëŒ€ ì‚¬ìš© ì•ˆ í•¨!**\n\n### ëª¨ë¸ ëª©ë¡\n\n- LightGBM (ë¹ ë¥´ê³  íš¨ìœ¨ì )\n- XGBoost (ê°•ë ¥í•œ ì„±ëŠ¥)\n- CatBoost (ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ ìš°ìˆ˜)\n- Logistic Regression (ì„¤ëª…ë ¥ ìš°ìˆ˜, ë² ì´ìŠ¤ë¼ì¸)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ì „ëµ ë¹„êµ\nprint(\"=\" * 70)\nprint(\"ğŸ“Š ë¦¬ìƒ˜í”Œë§ ì „ëµ Validation í‰ê°€ ê²°ê³¼\")\nprint(\"=\" * 70)\n\n# Strategy A ìµœê³  ì„±ëŠ¥\nbest_smote = max(strategy_a_results.items(), key=lambda x: x['pr_auc'])\nbest_smote_name, best_smote_result = best_smote\n\nprint(f\"\\nStrategy A (SMOTE ê³„ì—´):\")\nprint(f\"  ìµœê³  ì„±ëŠ¥: {best_smote_name.upper()} - PR-AUC = {best_smote_result['pr_auc']:.4f}\")\n\nprint(f\"\\nStrategy B (Class Weight):\")\nprint(f\"  PR-AUC = {pr_auc_b:.4f}\")\n\n# ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°\nperformance_diff = pr_auc_b - best_smote_result['pr_auc']\npercentage_diff = (performance_diff / best_smote_result['pr_auc']) * 100\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"ì„±ëŠ¥ ì°¨ì´: {performance_diff:+.4f} ({percentage_diff:+.2f}%)\")\nprint(f\"{'=' * 70}\")\n\n# ìµœì¢… ì„ íƒ\nif pr_auc_b > best_smote_result['pr_auc']:\n    selected_strategy = 'Class Weight'\n    selected_resampler = None\n    selected_model = pipeline_b\n    selected_pr_auc = pr_auc_b\n    reason = f\"Class Weightê°€ {abs(performance_diff):.4f} ë” ìš°ìˆ˜\"\nelse:\n    selected_strategy = f'SMOTE ({best_smote_name})'\n    selected_resampler = best_smote_name\n    selected_model = best_smote_result['model']\n    selected_pr_auc = best_smote_result['pr_auc']\n    reason = f\"{best_smote_name}ê°€ {abs(performance_diff):.4f} ë” ìš°ìˆ˜\"\n\nprint(f\"\\nâœ… ìµœì¢… ì„ íƒ: {selected_strategy}\")\nprint(f\"   ì´ìœ : {reason}\")\nprint(f\"   Validation PR-AUC: {selected_pr_auc:.4f}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.4 ì „ëµ ë¹„êµ ë° ìµœì¢… ì„ íƒ (Validation ê¸°ë°˜)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 70)\nprint(\"Strategy B: Class Weight Only\")\nprint(\"=\" * 70)\n\n# LightGBM with Class Weight\nlgbm_weighted = lgb.LGBMClassifier(\n    n_estimators=100,\n    max_depth=5,\n    learning_rate=0.05,\n    scale_pos_weight=scale_pos_weight,  # â­ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\n    random_state=RANDOM_STATE,\n    verbose=-1\n)\n\n# íŒŒì´í”„ë¼ì¸ ìƒì„± (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)\npipeline_b = create_pipeline(lgbm_weighted, use_resampler=None)\n\n# Train set í•™ìŠµ\npipeline_b.fit(X_train, y_train)\n\n# Validation set í‰ê°€ (âš ï¸ Test set ì‚¬ìš© ì•ˆ í•¨!)\ny_val_prob_b = pipeline_b.predict_proba(X_val)[:, 1]\npr_auc_b = average_precision_score(y_val, y_val_prob_b)\n\nprint(f\"\\nâœ… Class Weight Only Validation PR-AUC: {pr_auc_b:.4f}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.3 Strategy B: Class Weight Only (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 70)\nprint(\"Strategy A: SMOTE ê³„ì—´ í…ŒìŠ¤íŠ¸\")\nprint(\"=\" * 70)\n\n# ê¸°ë³¸ LightGBM ëª¨ë¸ (Class Weight ì—†ìŒ)\nlgbm_base = lgb.LGBMClassifier(\n    n_estimators=100,\n    max_depth=5,\n    learning_rate=0.05,\n    random_state=RANDOM_STATE,\n    verbose=-1\n)\n\n# SMOTE ë³€í˜• í…ŒìŠ¤íŠ¸\nsmote_variants = ['smote', 'borderline', 'smote_tomek']\nstrategy_a_results = {}\n\nfor variant in smote_variants:\n    print(f\"\\nğŸ”„ {variant.upper()} í…ŒìŠ¤íŠ¸ ì¤‘...\")\n    \n    # íŒŒì´í”„ë¼ì¸ ìƒì„±\n    pipeline = create_pipeline(lgbm_base, use_resampler=variant)\n    \n    # Train set í•™ìŠµ\n    pipeline.fit(X_train, y_train)\n    \n    # Validation set í‰ê°€ (âš ï¸ Test set ì‚¬ìš© ì•ˆ í•¨!)\n    y_val_prob = pipeline.predict_proba(X_val)[:, 1]\n    pr_auc = average_precision_score(y_val, y_val_prob)\n    \n    strategy_a_results[variant] = {\n        'model': pipeline,\n        'pr_auc': pr_auc\n    }\n    \n    print(f\"   âœ… Validation PR-AUC: {pr_auc:.4f}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Strategy A ê²°ê³¼ ìš”ì•½:\")\nprint(\"=\" * 70)\nfor variant, result in strategy_a_results.items():\n    print(f\"{variant.upper():15s}: PR-AUC = {result['pr_auc']:.4f}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Strategy A: SMOTE ê³„ì—´ (Class Weight ì—†ìŒ)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° (Class Weightìš©)\nneg_count = (y_train == 0).sum()\npos_count = (y_train == 1).sum()\nscale_pos_weight = neg_count / pos_count\n\nprint(f\"ğŸ“Š Train Set ë¶ˆê· í˜• ì •ë³´:\")\nprint(f\"   ì •ìƒ ê¸°ì—…: {neg_count:,}ê°œ\")\nprint(f\"   ë¶€ë„ ê¸°ì—…: {pos_count:,}ê°œ\")\nprint(f\"   ë¶ˆê· í˜• ë¹„ìœ¨: 1:{scale_pos_weight:.1f}\")\nprint(f\"   scale_pos_weight: {scale_pos_weight:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 3. ë¦¬ìƒ˜í”Œë§ ì „ëµ ëŒ€ì¡° ì‹¤í—˜ â­\n\n### í•µì‹¬ ì§ˆë¬¸\n\n**SMOTE vs Class Weight, ì–´ëŠ ê²ƒì´ ë” íš¨ê³¼ì ì¸ê°€?**\n\n- **Strategy A**: SMOTE ê³„ì—´ (Class Weight ì—†ìŒ)\n- **Strategy B**: Class Weight Only (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)\n\n### ì‹¤í—˜ ì„¤ê³„\n\nâœ… **Train Set**ìœ¼ë¡œ í•™ìŠµ  \nâœ… **Validation Set**ìœ¼ë¡œ í‰ê°€ (Test set ì ˆëŒ€ ì‚¬ìš© ì•ˆ í•¨!)  \nâœ… **PR-AUC**ë¡œ ë¹„êµ (ë¶ˆê· í˜• ë°ì´í„° í•µì‹¬ ì§€í‘œ)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_pipeline(classifier, use_resampler=None):\n    \"\"\"\n    ì „ì²˜ë¦¬ + ë¦¬ìƒ˜í”Œë§ + ë¶„ë¥˜ê¸° íŒŒì´í”„ë¼ì¸ ìƒì„±\n    \n    Parameters:\n    -----------\n    classifier : estimator\n        ë¶„ë¥˜ ëª¨ë¸\n    use_resampler : str or None\n        ë¦¬ìƒ˜í”Œë§ ë°©ë²• ('smote', 'borderline', 'smote_tomek', None)\n    \n    Returns:\n    --------\n    pipeline : ImbPipeline\n        ì „ì²´ íŒŒì´í”„ë¼ì¸\n    \"\"\"\n    \n    # ë¦¬ìƒ˜í”ŒëŸ¬ ì„ íƒ\n    resampler_map = {\n        'smote': SMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE),\n        'borderline': BorderlineSMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE),\n        'smote_tomek': SMOTETomek(sampling_strategy=0.2, random_state=RANDOM_STATE),\n        None: 'passthrough'\n    }\n    \n    resampler = resampler_map.get(use_resampler, 'passthrough')\n    \n    # íŒŒì´í”„ë¼ì¸ êµ¬ì„±\n    pipeline = ImbPipeline([\n        ('inf_handler', InfiniteHandler()),\n        ('imputer', SimpleImputer(strategy='median')),\n        ('log_transformer', LogTransformer()),\n        ('scaler', RobustScaler()),\n        ('resampler', resampler),\n        ('classifier', classifier)\n    ])\n    \n    return pipeline\n\nprint(\"âœ… íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\nprint(\"   ìˆœì„œ: InfiniteHandler â†’ Imputer â†’ LogTransformer â†’ RobustScaler â†’ Resampler â†’ Classifier\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\nclass InfiniteHandler(BaseEstimator, TransformerMixin):\n    \"\"\"ë¬´í•œëŒ€ ê°’ì„ ì²˜ë¦¬í•˜ëŠ” ë³€í™˜ê¸°\"\"\"\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜\n        X_copy = X_copy.replace([np.inf, -np.inf], np.nan)\n        return X_copy\n\nclass LogTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"ì–‘ìˆ˜ íŠ¹ì„±ì— ë¡œê·¸ ë³€í™˜ ì ìš©\"\"\"\n    def __init__(self, epsilon=1e-10):\n        self.epsilon = epsilon\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        # ëª¨ë“  ê°’ì´ 0 ì´ìƒì¸ íŠ¹ì„±ì—ë§Œ ë¡œê·¸ ë³€í™˜\n        for col in X_copy.columns:\n            if (X_copy[col] >= 0).all():\n                X_copy[col] = np.log1p(X_copy[col] + self.epsilon)\n        return X_copy\n\nprint(\"âœ… ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\nprint(\"   - InfiniteHandler: ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬\")\nprint(\"   - LogTransformer: ë¡œê·¸ ë³€í™˜ (ì–‘ìˆ˜ íŠ¹ì„±ë§Œ)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“— ë°œí‘œìš© Part 3: ëª¨ë¸ë§ ë° ìµœì í™” v2\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Part 3 ëª©í‘œ ë° ì´ì „ Part ìš”ì•½\n",
    "\n",
    "### Part 1-2 ì™„ë£Œ ì‚¬í•­\n",
    "\n",
    "**Part 1: ë¬¸ì œ ì •ì˜ ë° í•µì‹¬ ë°œê²¬**\n",
    "- 50,105ê°œ ê¸°ì—…, ë¶€ë„ìœ¨ 1.51% (1:66 ë¶ˆê· í˜•)\n",
    "- **ìœ ë™ì„±**ì´ ê°€ì¥ ê°•ë ¥í•œ ì˜ˆì¸¡ ë³€ìˆ˜ ë°œê²¬\n",
    "- ì—…ì¢…ë³„ ë¶€ë„ìœ¨ ìµœëŒ€ 2ë°° ì°¨ì´ (ê±´ì„¤ì—… 2.1% vs ì œì¡°ì—… 1.1%)\n",
    "\n",
    "**Part 2: ë„ë©”ì¸ íŠ¹ì„± ê³µí•™**\n",
    "- 52ê°œ ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± ìƒì„±\n",
    "- VIF/IV/AUC ê¸°ë°˜ ê²€ì¦ â†’ **27ê°œ ìµœì¢… ì„ íƒ**\n",
    "- `domain_based_features_ì™„ì „íŒ.csv` ì¶œë ¥\n",
    "\n",
    "### Part 3 í•µì‹¬ ëª©í‘œ\n",
    "\n",
    "1. âœ… **Data Leakage ì™„ì „ ì œê±°** - Test setì€ ìµœì¢… í‰ê°€ ë‹¨ í•œ ë²ˆë§Œ\n",
    "2. âœ… **3-Way Split** (Train 60% / Validation 20% / Test 20%)\n",
    "3. âœ… **ë¦¬ìƒ˜í”Œë§ ì „ëµ ëŒ€ì¡° ì‹¤í—˜** (SMOTE vs Class Weight)\n",
    "4. âœ… **Validation ê¸°ë°˜ ëª¨ë“  ì˜ì‚¬ê²°ì •** (ëª¨ë¸ ì„ íƒ, ì„ê³„ê°’ ìµœì í™”)\n",
    "5. âœ… **F2-Score ìµœì í™”** (Recall ìš°ì„ , Type II Error ìµœì†Œí™”)\n",
    "6. âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì…ì¦** (Traffic Light ì‹œìŠ¤í…œ)\n",
    "\n",
    "### âš ï¸ Critical Requirements\n",
    "\n",
    "```\n",
    "âŒ ì ˆëŒ€ ê¸ˆì§€:\n",
    "   - Test setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ\n",
    "   - Test setìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™”\n",
    "   - Test setìœ¼ë¡œ Traffic Light ê¸°ì¤€ ê²°ì •\n",
    "   \n",
    "âœ… í•„ìˆ˜:\n",
    "   - Test setì€ ìµœì¢… ë³´ê³  ì§ì „ ë‹¨ í•œ ë²ˆë§Œ í‰ê°€\n",
    "   - ëª¨ë“  ì˜ì‚¬ê²°ì •ì€ Train/Validationì—ì„œë§Œ\n",
    "   - Validation set ë˜ëŠ” CVë¡œ ì„ê³„ê°’ ìµœì í™”\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì²˜ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "import platform\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "else:  # Linux\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    StratifiedKFold, \n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    fbeta_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# ë¶ˆê· í˜• ì²˜ë¦¬\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# ëª¨ë¸\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# í†µê³„\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# ìœ í‹¸ë¦¬í‹°\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ì¬í˜„ì„± ì„¤ì •\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"â° ì‹¤í–‰ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ í•˜ë“œì½”ë”© ê¸ˆì§€!)\n",
    "DATA_DIR = '../data'\n",
    "FEATURES_DIR = os.path.join(DATA_DIR, 'features')\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "\n",
    "# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# íŒŒì¼ëª… ì„¤ì •\n",
    "INPUT_FILE = os.path.join(FEATURES_DIR, 'domain_based_features_ì™„ì „íŒ.csv')\n",
    "OUTPUT_PREFIX = 'Part3_v2'\n",
    "\n",
    "print(f\"âœ… ì…ë ¥ íŒŒì¼: {INPUT_FILE}\")\n",
    "print(f\"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ë°ì´í„° ë¡œë”© ë° 3-Way Split â­\n",
    "\n",
    "### í•µì‹¬ ì›ì¹™\n",
    "\n",
    "```\n",
    "ì „ì²´ ë°ì´í„° (50,105)\n",
    "â”œâ”€ Train Set (60%, ~30,063): ëª¨ë¸ í•™ìŠµ + CV íŠœë‹\n",
    "â”œâ”€ Validation Set (20%, ~10,021): ëª¨ë¸ ì„ íƒ, ì„ê³„ê°’ ìµœì í™”, ì˜ì‚¬ê²°ì •\n",
    "â””â”€ Test Set (20%, ~10,021): ìµœì¢… í‰ê°€ë§Œ (ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ!)\n",
    "```\n",
    "\n",
    "**ë¹„ìœ **: Test setì€ \"ë´‰ì¸ëœ ì‹œí—˜ì§€\". ëª¨ë¸ ê°œë°œì´ ì™„ì „íˆ ëë‚œ í›„ í•œ ë²ˆë§Œ ê°œë´‰."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ë°ì´í„° ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë”©\n",
    "df = pd.read_csv(INPUT_FILE, encoding='utf-8')\n",
    "\n",
    "print(f\"ğŸ“Š ë°ì´í„° í˜•íƒœ: {df.shape}\")\n",
    "print(f\"\\nğŸ“‹ ì»¬ëŸ¼ ëª©ë¡ ({len(df.columns)}ê°œ):\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸\n",
    "TARGET_COL = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"íƒ€ê²Ÿ ë³€ìˆ˜ '{TARGET_COL}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "print(f\"\\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {TARGET_COL}\")\n",
    "print(f\"   ë¶€ë„ ê¸°ì—…: {df[TARGET_COL].sum():,}ê°œ ({df[TARGET_COL].mean():.2%})\")\n",
    "print(f\"   ì •ìƒ ê¸°ì—…: {(~df[TARGET_COL].astype(bool)).sum():,}ê°œ ({(1-df[TARGET_COL].mean()):.2%})\")\n",
    "print(f\"   ë¶ˆê· í˜• ë¹„ìœ¨: 1:{int(1/df[TARGET_COL].mean())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 íŠ¹ì„±-íƒ€ê²Ÿ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¹ì„±-íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(f\"âœ… íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}ê°œ\")\n",
    "print(f\"âœ… ìƒ˜í”Œ ìˆ˜: {len(X):,}ê°œ\")\n",
    "print(f\"\\nğŸ“Š íŠ¹ì„± ëª©ë¡:\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 â­ 3-Way Split (Train/Validation/Test)\n",
    "\n",
    "**ì¤‘ìš”**: ì´ ë‹¨ê³„ì—ì„œ ë¶„í• í•œ Test setì€ **ë…¸íŠ¸ë¶ ë§ˆì§€ë§‰ ì„¹ì…˜ ì „ê¹Œì§€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ì°¨ ë¶„í• : Train+Val (80%) vs Test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2ì°¨ ë¶„í• : Train (75% of 80% = 60%) vs Val (25% of 80% = 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.25, \n",
    "    stratify=y_temp, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… 3-Way Split ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Train Set:      {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%, ë¶€ë„ìœ¨: {y_train.mean():.2%})\")\n",
    "print(f\"Validation Set: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%, ë¶€ë„ìœ¨: {y_val.mean():.2%})\")\n",
    "print(f\"Test Set:       {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%, ë¶€ë„ìœ¨: {y_test.mean():.2%})\")\n",
    "print(\"=\"*70)\n",
    "print(\"âš ï¸  Test Setì€ ìµœì¢… í‰ê°€ ì „ê¹Œì§€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ë¶€ë„ ê¸°ì—… ìˆ˜ í™•ì¸\n",
    "print(f\"\\nğŸ“Š ë¶€ë„ ê¸°ì—… ë¶„í¬:\")\n",
    "print(f\"   Train:      {y_train.sum():,}ê°œ\")\n",
    "print(f\"   Validation: {y_val.sum():,}ê°œ\")\n",
    "print(f\"   Test:       {y_test.sum():,}ê°œ\")\n",
    "print(f\"   í•©ê³„:       {y.sum():,}ê°œ âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ë°ì´í„° ë¶„í•  ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í•  ì‹œê°í™”\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['ë°ì´í„° ë¶„í•  ë¹„ìœ¨', 'ë¶€ë„ìœ¨ ë¶„í¬'],\n",
    "    specs=[[{'type': 'pie'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# íŒŒì´ ì°¨íŠ¸: ë°ì´í„° ë¶„í• \n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=['Train (60%)', 'Validation (20%)', 'Test (20%)'],\n",
    "        values=[len(X_train), len(X_val), len(X_test)],\n",
    "        marker=dict(colors=['#3498db', '#f39c12', '#e74c3c']),\n",
    "        textposition='inside',\n",
    "        textinfo='label+percent+value'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# ë°” ì°¨íŠ¸: ë¶€ë„ìœ¨\n",
    "bankruptcy_rates = [y_train.mean(), y_val.mean(), y_test.mean()]\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['Train', 'Validation', 'Test'],\n",
    "        y=bankruptcy_rates,\n",
    "        marker=dict(color=['#3498db', '#f39c12', '#e74c3c']),\n",
    "        text=[f\"{rate:.2%}\" for rate in bankruptcy_rates],\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='3-Way Split ê²°ê³¼ (Stratified)',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text='ë¶€ë„ìœ¨', tickformat='.1%', row=1, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì„¸íŠ¸ì—ì„œ ë¶€ë„ìœ¨ì´ ê· ì¼í•˜ê²Œ ìœ ì§€ë¨ (Stratified Split ì„±ê³µ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜\n\n### í•µì‹¬ ê°œì„  ì‚¬í•­\n\n1. **ìˆœì„œ ìµœì í™”**: Imputer â†’ Log Transform â†’ Scaler\n2. **Winsorizer ì œê±°**: Tree ëª¨ë¸ì€ ì´ìƒì¹˜ì— ê°•ê±´, ê·¹ë‹¨ê°’ì€ ë¶€ë„ ì‹œê·¸ë„ì¼ ìˆ˜ ìˆìŒ\n3. **RobustScaler ì‚¬ìš©**: ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ë§\n4. **í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ìš°ì„ **: SMOTEë³´ë‹¤ ì•ˆì •ì ",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 2.1 ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}