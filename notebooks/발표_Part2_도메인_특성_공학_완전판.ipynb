{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“— Part 2: ë„ë©”ì¸ íŠ¹ì„± ê³µí•™ - ì™„ì „íŒ\n",
    "\n",
    "## ì‹œë‹ˆì–´ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ì˜ ì¬ë¬´ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ì´ì „ Part ìš”ì•½\n",
    "\n",
    "Part 1ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. âœ… **ìœ ë™ì„±ì´ ê°€ì¥ ê°•ë ¥í•œ ì˜ˆì¸¡ ë³€ìˆ˜**\n",
    "   - ìœ ë™ë¹„ìœ¨, ë‹¹ì¢Œë¹„ìœ¨, í˜„ê¸ˆë¹„ìœ¨ â†’ ë¶€ë„ ê¸°ì—…ê³¼ ì •ìƒ ê¸°ì—… ê°„ ëª…í™•í•œ ì°¨ì´\n",
    "   - Mann-Whitney U test: p < 0.001\n",
    "\n",
    "2. âœ… **ì—…ì¢…ë³„ ë¶€ë„ìœ¨ 2ë°° ì°¨ì´**\n",
    "   - ê±´ì„¤ì—… 2.8% vs ê¸ˆìœµì—… 0.9%\n",
    "   - ì œì¡°ì—… ì¤‘ì‹¬ ì‚°ì—…êµ¬ì¡°ì˜ ìœ„í—˜ì„±\n",
    "\n",
    "3. âœ… **ì™¸ê° ì—¬ë¶€ê°€ ì¤‘ìš”**\n",
    "   - ì™¸ê° ëŒ€ìƒ ê¸°ì—…ì˜ ë¶€ë„ìœ¨ì´ ë” ë‚®ìŒ\n",
    "   - íšŒê³„ ì‹ ë¢°ì„±ì´ ë¶€ë„ ì˜ˆì¸¡ì— ì˜í–¥\n",
    "\n",
    "---\n",
    "\n",
    "í•˜ì§€ë§Œ **í•œê³„**ê°€ ìˆì—ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- âŒ **ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ë ¥ ì œí•œì ** (AUC < 0.7)\n",
    "  - ê°œë³„ ì¬ë¬´ ë¹„ìœ¨ë§Œìœ¼ë¡œëŠ” ë¶€ë„ ì˜ˆì¸¡ ë¶ˆì¶©ë¶„\n",
    "  - ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ê²°í•©í•œ ë³µí•© ì§€í‘œ í•„ìš”\n",
    "\n",
    "- âŒ **ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© ë¯¸ê³ ë ¤**\n",
    "  - ìœ ë™ì„± Ã— ìˆ˜ìµì„±, ë ˆë²„ë¦¬ì§€ Ã— ì„±ì¥ì„± ë“±\n",
    "  - ë¹„ì„ í˜• ê´€ê³„ í¬ì°© í•„ìš”\n",
    "\n",
    "---\n",
    "\n",
    "**ì´ì œ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ë³µí•© íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Why: ì™œ ë„ë©”ì¸ íŠ¹ì„±ì´ í•„ìš”í•œê°€?\n",
    "\n",
    "### 1ï¸âƒ£ ë¬¸ì œ ì¸ì‹: ì›ë³¸ ë°ì´í„°ì˜ í•œê³„\n",
    "\n",
    "**ì›ë³¸ ë°ì´í„° (159ê°œ ë³€ìˆ˜)ì˜ ë¬¸ì œì :**\n",
    "\n",
    "- âŒ **ì •ì  ìŠ¤ëƒ…ìƒ·ì— ë¶ˆê³¼**: ì¬ë¬´ì œí‘œ í•­ëª© ì¤‘ì‹¬ (ìì‚°, ë¶€ì±„, ë§¤ì¶œ ë“±) â†’ íŠ¹ì • ì‹œì ì˜ ì¬ë¬´ ìƒíƒœë§Œ ë³´ì—¬ì¤Œ\n",
    "- âŒ **ë¶€ë„ì˜ \"ì›ì¸\"ì„ ì§ì ‘ ì„¤ëª…í•˜ì§€ ëª»í•¨**: \"ìœ ë™ìì‚° = 1ì–µì›\"ì´ë¼ëŠ” ì •ë³´ë§Œìœ¼ë¡œëŠ” ê¸°ì—…ì´ ìœ„í—˜í•œì§€ ì•Œ ìˆ˜ ì—†ìŒ\n",
    "- âŒ **í•œêµ­ ì‹œì¥ íŠ¹ì„± ë¯¸ë°˜ì˜**: ì™¸ë¶€ê°ì‚¬ ì˜ë¬´, ì œì¡°ì—… ì¤‘ì‹¬ ì‚°ì—…êµ¬ì¡°, ëŒ€ê¸°ì—… ì˜ì¡´ë„ ë“± í•œêµ­ íŠ¹ìœ ì˜ ë¦¬ìŠ¤í¬ ìš”ì¸ ëˆ„ë½\n",
    "\n",
    "**ì˜ˆì‹œë¡œ ë³´ëŠ” í•œê³„:**\n",
    "```\n",
    "ê¸°ì—… A: ìœ ë™ìì‚° 1ì–µì›, ìœ ë™ë¶€ì±„ 5ì²œë§Œì›\n",
    "â†’ ì´ê²ƒë§Œìœ¼ë¡œëŠ” ì•ˆì „í•œì§€ ìœ„í—˜í•œì§€ íŒë‹¨ ë¶ˆê°€\n",
    "â†’ ìœ ë™ë¹„ìœ¨(200%)ì„ ê³„ì‚°í•´ì•¼ í•¨ â†’ í•˜ì§€ë§Œ ì´ê²ƒë„ ë¶€ì¡±\n",
    "â†’ í˜„ê¸ˆë¹„ìœ¨, í˜„ê¸ˆì†Œì§„ì¼ìˆ˜, ìš´ì „ìë³¸ íšŒì „ìœ¨ ë“± ì¶”ê°€ ì§€í‘œ í•„ìš”\n",
    "```\n",
    "\n",
    "**ê²°ë¡ : ì›ë³¸ ë°ì´í„°ëŠ” \"ì¬ë£Œ\"ì¼ ë¿, \"ë¶€ë„ ìœ„í—˜\"ì„ ì§ì ‘ ì¸¡ì •í•˜ëŠ” \"ì§€í‘œ\"ê°€ ì•„ë‹˜**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ ë„ë©”ì¸ ì§€ì‹: ê¸°ì—…ì´ ë¶€ë„ë‚˜ëŠ” 3ê°€ì§€ ê²½ë¡œ\n",
    "\n",
    "**í•™ê³„ ë° ì‹¤ë¬´ ì—°êµ¬ ê¸°ë°˜ (Altman 1968; Ohlson 1980; í•œêµ­ì€í–‰ 2020)**\n",
    "\n",
    "#### ğŸ”´ ê²½ë¡œ 1: ìœ ë™ì„± ìœ„ê¸° (Liquidity Crisis) - **ë¶€ë„ì˜ 70%**\n",
    "\n",
    "**ì •ì˜:** í˜„ê¸ˆì´ ê³ ê°ˆë˜ì–´ ë‹¨ê¸° ì±„ë¬´ë¥¼ ê°šì§€ ëª»í•˜ëŠ” ìƒí™©\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- ì¥ë¶€ìƒ í‘ìì—¬ë„ ë°œìƒ ê°€ëŠ¥ (**í‘ìë„ì‚°**)\n",
    "- ë§¤ì¶œì€ ìˆì§€ë§Œ í˜„ê¸ˆ íšŒìˆ˜ê°€ ëŠ¦ì–´ì§€ë©´ ë¶€ë„\n",
    "- ë¶€ë„ ë°œìƒ **3ê°œì›” ì „**ì— ê¸‰ê²©íˆ ì•…í™”ë˜ëŠ” ì§€í‘œë“¤\n",
    "\n",
    "**ìœ„í—˜ ì‹ í˜¸:**\n",
    "- í˜„ê¸ˆì†Œì§„ì¼ìˆ˜ < 30ì¼ (í•œ ë‹¬ë„ ëª» ë²„íŒ€)\n",
    "- ìœ ë™ë¹„ìœ¨ < 100% (ë‹¨ê¸° ë¶€ì±„ê°€ ìœ ë™ìì‚°ë³´ë‹¤ ë§ìŒ)\n",
    "- ìš´ì „ìë³¸ ìŒìˆ˜ (ìœ ë™ë¶€ì±„ > ìœ ë™ìì‚°)\n",
    "\n",
    "#### ğŸŸ  ê²½ë¡œ 2: ì§€ê¸‰ë¶ˆëŠ¥ (Insolvency) - **ë¶€ë„ì˜ 20%**\n",
    "\n",
    "**ì •ì˜:** ë¶€ì±„ê°€ ìì‚°ì„ ì´ˆê³¼í•˜ì—¬ êµ¬ì¡°ì ìœ¼ë¡œ íšŒìƒ ë¶ˆê°€ëŠ¥í•œ ìƒí™©\n",
    "\n",
    "**ìœ„í—˜ ì‹ í˜¸:**\n",
    "- ìë³¸ì ì‹ë„ > 50% (ìë³¸ì˜ ì ˆë°˜ ì´ìƒ ì†ì‹¤)\n",
    "- ì´ìë³´ìƒë°°ìœ¨ < 1.0 (ì˜ì—…ì´ìµ < ì´ìë¹„ìš©)\n",
    "- ë¶€ì±„ìƒí™˜ë…„ìˆ˜ > 10ë…„ (í˜„ê¸ˆíë¦„ìœ¼ë¡œ ë¶€ì±„ ìƒí™˜ ë¶ˆê°€)\n",
    "\n",
    "#### ğŸŸ¡ ê²½ë¡œ 3: ì‹ ë¢° ìƒì‹¤ (Loss of Confidence) - **ë¶€ë„ì˜ 10%**\n",
    "\n",
    "**ì •ì˜:** ì—°ì²´Â·ì²´ë‚© ì´ë ¥ìœ¼ë¡œ ê¸ˆìœµê¸°ê´€ê³¼ ê±°ë˜ì²˜ê°€ ìê¸ˆì¤„ì„ ì°¨ë‹¨\n",
    "\n",
    "**ìœ„í—˜ ì‹ í˜¸:**\n",
    "- ì—°ì²´ ì´ë ¥ 1íšŒ ì´ìƒ\n",
    "- ì„¸ê¸ˆ ì²´ë‚© ë°œìƒ\n",
    "- ì‹ ìš©ë“±ê¸‰ BB ì´í•˜ (ë“±ê¸‰ 5 ì´ìƒ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ íŠ¹ì„± ê³µí•™ ì „ëµ: ê²½ë¡œë³„ ì¡°ê¸° ê°ì§€ ì§€í‘œ ê°œë°œ\n",
    "\n",
    "**ëª©í‘œ:** ë¶€ë„ 3~6ê°œì›” ì „ì— ë¯¸ë¦¬ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ì‹ í˜¸ í¬ì°©\n",
    "\n",
    "| ì¹´í…Œê³ ë¦¬ | íŠ¹ì„± ìˆ˜ | ëª©ì  | ëŒ€í‘œ ì§€í‘œ | ë¹„ì¦ˆë‹ˆìŠ¤ ì§ˆë¬¸ |\n",
    "|----------|---------|------|-----------|---------------|\n",
    "| **ìœ ë™ì„± ìœ„ê¸°** | 10ê°œ | ë‹¨ê¸° ìƒì¡´ ê°€ëŠ¥ì„± | í˜„ê¸ˆì†Œì§„ì¼ìˆ˜, ìš´ì „ìë³¸ë¹„ìœ¨ | \"3ê°œì›” ë‚´ ì‚´ì•„ë‚¨ì„ ìˆ˜ ìˆëŠ”ê°€?\" |\n",
    "| **ì§€ê¸‰ë¶ˆëŠ¥** | 11ê°œ | ì¥ê¸° íšŒìƒ ê°€ëŠ¥ì„± | ìë³¸ì ì‹ë„, ë¶€ì±„ìƒí™˜ë…„ìˆ˜ | \"êµ¬ì¡°ì ìœ¼ë¡œ íšŒìƒ ê°€ëŠ¥í•œê°€?\" |\n",
    "| **ì¬ë¬´ì¡°ì‘ íƒì§€** | 15ê°œ | íšŒê³„ ì‹ ë¢°ì„± ê²€ì¦ | M-Score, ë°œìƒì•¡ í’ˆì§ˆ | \"ì¬ë¬´ì œí‘œë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€?\" |\n",
    "| **ì´í•´ê´€ê³„ì í–‰ë™** | 10ê°œ | ì‹ ìš© í–‰ë™ íŒ¨í„´ | ì—°ì²´, ì‹ ìš©ë“±ê¸‰ | \"ì´ ê¸°ì—…ì„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€?\" |\n",
    "| **í•œêµ­ ì‹œì¥ íŠ¹í™”** | 6ê°œ | í•œêµ­ ê¸°ì—… íŠ¹ì„± | ì™¸ê° ì—¬ë¶€, ì œì¡°ì—… ë¦¬ìŠ¤í¬ | \"í•œêµ­ ì‹œì¥ì˜ ìœ„í—˜ì„ ë°˜ì˜í–ˆëŠ”ê°€?\" |\n",
    "\n",
    "**ì´ 50+ ê°œ íŠ¹ì„± ìƒì„±**\n",
    "\n",
    "#### ğŸ¯ ì™œ í†µê³„ì  íŠ¹ì„±ì´ ì•„ë‹Œ ë„ë©”ì¸ íŠ¹ì„±ì¸ê°€?\n",
    "\n",
    "**ë„ë©”ì¸ ì ‘ê·¼ì˜ ì¥ì :**\n",
    "- âœ… **í•´ì„ ê°€ëŠ¥**: \"í˜„ê¸ˆì†Œì§„ì¼ìˆ˜ê°€ 15ì¼ì´ë¼ ìœ„í—˜í•©ë‹ˆë‹¤\"\n",
    "- âœ… **ì‹¤ë¬´ ì ìš©**: ì‹¬ì‚¬ ê¸°ì¤€ìœ¼ë¡œ ì§ì ‘ í™œìš© ê°€ëŠ¥\n",
    "- âœ… **ë…¼ë¦¬ì  ì„¤ë“ë ¥**: \"ì™œ ì´ ì§€í‘œê°€ ì¤‘ìš”í•œê°€?\"ì— ëŒ€í•œ ì´ë¡ ì  ê·¼ê±° ì¡´ì¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ íŠ¹ì„± ìƒì„± ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "import platform\n",
    "if platform.system() == 'Darwin':\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "else:\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "df = pd.read_csv('../data/ê¸°ì—…ì‹ ìš©í‰ê°€ì •ë³´_210801.csv', encoding='utf-8')\n",
    "target_col = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {df.shape[0]:,} ê¸°ì—…, {df.shape[1]:,} ë³€ìˆ˜\")\n",
    "print(f\"âœ… ë¶€ë„ìœ¨: {df[target_col].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í…Œê³ ë¦¬ 1: ìœ ë™ì„± ìœ„ê¸° íŠ¹ì„± (10ê°œ)\n",
    "\n",
    "**ğŸ’¡ ì™œ ìœ ë™ì„±ì´ ê°€ì¥ ì¤‘ìš”í•œê°€?**\n",
    "\n",
    "ê²½ì œì  ê°€ì„¤: \"ë¶€ë„ëŠ” ì§€ê¸‰ë¶ˆëŠ¥ì´ ì•„ë‹Œ ìœ ë™ì„± ìœ„ê¸°ë¡œ ì‹œì‘ëœë‹¤\"\n",
    "\n",
    "**í•™ìˆ ì  ë°°ê²½ (Whitaker 1999):**\n",
    "- ë¶€ë„ ê¸°ì—…ì˜ **67%ëŠ” í‘ì**ì˜€ìŒ (ì¥ë¶€ìƒ ì´ìµ ë°œìƒ)\n",
    "- í•˜ì§€ë§Œ **í˜„ê¸ˆì´ ì—†ì–´ì„œ** ê¸‰ì—¬/ì„¸ê¸ˆ/ì´ìë¥¼ ì§€ê¸‰í•˜ì§€ ëª»í•¨\n",
    "- ìœ ë™ì„± ìœ„ê¸°ëŠ” ë¶€ë„ **3~6ê°œì›” ì „**ì— ë‚˜íƒ€ë‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_liquidity_crisis_features(df):\n",
    "    \"\"\"ìœ ë™ì„± ìœ„ê¸°ë¥¼ ì¡°ê¸°ì— ê°ì§€í•˜ëŠ” íŠ¹ì„± ìƒì„±\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. ì¦‰ê°ì  ì§€ê¸‰ëŠ¥ë ¥\n",
    "    if 'í˜„ê¸ˆ' in df.columns and 'ìœ ë™ë¶€ì±„' in df.columns:\n",
    "        features['ì¦‰ê°ì§€ê¸‰ëŠ¥ë ¥'] = (df['í˜„ê¸ˆ'] + df.get('í˜„ê¸ˆì„±ìì‚°', 0)) / (df['ìœ ë™ë¶€ì±„'] + 1)\n",
    "        features['í˜„ê¸ˆì†Œì§„ì¼ìˆ˜'] = (df['í˜„ê¸ˆ'] + df.get('í˜„ê¸ˆì„±ìì‚°', 0)) / (df.get('ì˜ì—…ë¹„ìš©', df['ë§¤ì¶œì›ê°€']) / 365 + 1)\n",
    "    \n",
    "    # 2. ìš´ì „ìë³¸ ê±´ì „ì„±\n",
    "    if 'ìœ ë™ìì‚°' in df.columns and 'ìœ ë™ë¶€ì±„' in df.columns:\n",
    "        features['ìš´ì „ìë³¸'] = df['ìœ ë™ìì‚°'] - df['ìœ ë™ë¶€ì±„']\n",
    "        features['ìš´ì „ìë³¸ë¹„ìœ¨'] = features['ìš´ì „ìë³¸'] / (df.get('ë§¤ì¶œì•¡', 1) + 1)\n",
    "        features['ìš´ì „ìë³¸_ëŒ€_ìì‚°'] = features['ìš´ì „ìë³¸'] / (df.get('ìì‚°ì´ê³„', 1) + 1)\n",
    "    \n",
    "    # 3. ê¸´ê¸‰ ìê¸ˆì¡°ë‹¬ ì—¬ë ¥\n",
    "    if 'ë§¤ì¶œì±„ê¶Œ' in df.columns and 'ë‹¨ê¸°ì°¨ì…ê¸ˆ' in df.columns:\n",
    "        features['ê¸´ê¸‰ìœ ë™ì„±'] = (df['í˜„ê¸ˆ'] + df.get('í˜„ê¸ˆì„±ìì‚°', 0) + df['ë§¤ì¶œì±„ê¶Œ'] * 0.8) / (df['ë‹¨ê¸°ì°¨ì…ê¸ˆ'] + 1)\n",
    "    \n",
    "    # 4. ìœ ë™ì„± ì••ë°• ì§€í‘œ\n",
    "    if 'ìœ ë™ë¶€ì±„' in df.columns and 'ë¶€ì±„ì´ê³„' in df.columns:\n",
    "        features['ìœ ë™ì„±ì••ë°•ì§€ìˆ˜'] = (df['ìœ ë™ë¶€ì±„'] / (df['ìœ ë™ìì‚°'] + 1)) * (df['ë¶€ì±„ì´ê³„'] / (df['ìì‚°ì´ê³„'] + 1))\n",
    "    \n",
    "    # 5. í˜„ê¸ˆíë¦„ ê¸°ë°˜ ìœ ë™ì„±\n",
    "    if 'ì˜ì—…í™œë™í˜„ê¸ˆíë¦„' in df.columns:\n",
    "        features['OCF_ëŒ€_ìœ ë™ë¶€ì±„'] = df['ì˜ì—…í™œë™í˜„ê¸ˆíë¦„'] / (df.get('ìœ ë™ë¶€ì±„', 1) + 1)\n",
    "        features['í˜„ê¸ˆì°½ì¶œëŠ¥ë ¥'] = df['ì˜ì—…í™œë™í˜„ê¸ˆíë¦„'] / (df.get('ë§¤ì¶œì•¡', 1) + 1)\n",
    "    \n",
    "    print(f\"âœ… ìœ ë™ì„± ìœ„ê¸° íŠ¹ì„± {features.shape[1]}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "    return features\n",
    "\n",
    "liquidity_features = create_liquidity_crisis_features(df)\n",
    "print(\"\\nìƒì„±ëœ ìœ ë™ì„± íŠ¹ì„±:\")\n",
    "print(liquidity_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í…Œê³ ë¦¬ 2: ì§€ê¸‰ë¶ˆëŠ¥ íŒ¨í„´ íŠ¹ì„± (11ê°œ)\n",
    "\n",
    "**ğŸ’¡ ìœ ë™ì„± ìœ„ê¸° vs ì§€ê¸‰ë¶ˆëŠ¥**\n",
    "\n",
    "**ì°¨ì´ì :**\n",
    "- **ìœ ë™ì„± ìœ„ê¸°**: ì¼ì‹œì  í˜„ê¸ˆ ë¶€ì¡± (ë‹¨ê¸° ë¬¸ì œ)\n",
    "- **ì§€ê¸‰ë¶ˆëŠ¥**: êµ¬ì¡°ì  ë¶€ì±„ ì´ˆê³¼ (ì¥ê¸° ë¬¸ì œ)\n",
    "\n",
    "**ê²½ì œì  ê°€ì„¤: \"ìë³¸ì ì‹ + ê³¼ë‹¤ë¶€ì±„ = íšŒìƒ ë¶ˆê°€ëŠ¥\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_insolvency_features(df):\n",
    "    \"\"\"ì§€ê¸‰ë¶ˆëŠ¥ ìœ„í—˜ì„ í¬ì°©í•˜ëŠ” íŠ¹ì„± ìƒì„±\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. ìë³¸ ì ì‹ë„\n",
    "    if 'ìë³¸ì´ê³„' in df.columns:\n",
    "        features['ìë³¸ì ì‹ì—¬ë¶€'] = (df['ìë³¸ì´ê³„'] < 0).astype(int)\n",
    "        features['ìë³¸ì ì‹ë„'] = np.where(df.get('ë‚©ì…ìë³¸ê¸ˆ', 1) > 0, \n",
    "                                       np.maximum(0, 1 - df['ìë³¸ì´ê³„'] / df.get('ë‚©ì…ìë³¸ê¸ˆ', 1)), 0)\n",
    "    \n",
    "    # 2. ì°¨ì…ê¸ˆ ì˜ì¡´ë„\n",
    "    if 'ë‹¨ê¸°ì°¨ì…ê¸ˆ' in df.columns and 'ì¥ê¸°ì°¨ì…ê¸ˆ' in df.columns:\n",
    "        features['ì´ì°¨ì…ê¸ˆ'] = df['ë‹¨ê¸°ì°¨ì…ê¸ˆ'] + df['ì¥ê¸°ì°¨ì…ê¸ˆ']\n",
    "        features['ì°¨ì…ê¸ˆì˜ì¡´ë„'] = features['ì´ì°¨ì…ê¸ˆ'] / (df.get('ìì‚°ì´ê³„', 1) + 1)\n",
    "        features['ì°¨ì…ê¸ˆ_ëŒ€_ë§¤ì¶œ'] = features['ì´ì°¨ì…ê¸ˆ'] / (df.get('ë§¤ì¶œì•¡', 1) + 1)\n",
    "    \n",
    "    # 3. ì´ìë³´ìƒëŠ¥ë ¥\n",
    "    if 'ì˜ì—…ì†ìµ' in df.columns and 'ê¸ˆìœµë¹„ìš©' in df.columns:\n",
    "        features['ì´ìë³´ìƒë°°ìœ¨'] = (df['ì˜ì—…ì†ìµ'] + df.get('ê°ê°€ìƒê°ë¹„', 0)) / (df['ê¸ˆìœµë¹„ìš©'] + 1)\n",
    "        features['ì´ìë¶€ë‹´ë¥ '] = df['ê¸ˆìœµë¹„ìš©'] / (df.get('ë§¤ì¶œì•¡', 1) + 1)\n",
    "    \n",
    "    # 4. ë¶€ì±„ ìƒí™˜ ëŠ¥ë ¥\n",
    "    if 'ë‹¹ê¸°ìˆœì´ìµ' in df.columns and 'ë¶€ì±„ì´ê³„' in df.columns:\n",
    "        features['ë¶€ì±„ìƒí™˜ë…„ìˆ˜'] = df['ë¶€ì±„ì´ê³„'] / (df['ë‹¹ê¸°ìˆœì´ìµ'] + df.get('ê°ê°€ìƒê°ë¹„', 0) + 1)\n",
    "        features['ìˆœë¶€ì±„ë¹„ìœ¨'] = (df['ë¶€ì±„ì´ê³„'] - df.get('í˜„ê¸ˆ', 0)) / (df.get('ìë³¸ì´ê³„', 1) + 1)\n",
    "    \n",
    "    # 5. ë ˆë²„ë¦¬ì§€ ìœ„í—˜\n",
    "    if 'ìì‚°ì´ê³„' in df.columns and 'ìë³¸ì´ê³„' in df.columns:\n",
    "        features['ì¬ë¬´ë ˆë²„ë¦¬ì§€'] = df['ìì‚°ì´ê³„'] / (df['ìë³¸ì´ê³„'].abs() + 1)\n",
    "        features['ë¶€ì±„ë ˆë²„ë¦¬ì§€'] = df.get('ë¶€ì±„ì´ê³„', 0) / (df['ìë³¸ì´ê³„'].abs() + 1)\n",
    "    \n",
    "    print(f\"âœ… ì§€ê¸‰ë¶ˆëŠ¥ íŒ¨í„´ íŠ¹ì„± {features.shape[1]}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "    return features\n",
    "\n",
    "insolvency_features = create_insolvency_features(df)\n",
    "print(\"\\nìƒì„±ëœ ì§€ê¸‰ë¶ˆëŠ¥ íŠ¹ì„±:\")\n",
    "print(insolvency_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í…Œê³ ë¦¬ 3: ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„± - ì™„ì „íŒ (15ê°œ) â­\n",
    "\n",
    "**ğŸ’¡ í•œêµ­í˜• Beneish M-Score ì™„ì „ êµ¬í˜„**\n",
    "\n",
    "**ê²½ì œì  ê°€ì„¤: \"ë¶€ë„ ì§ì „ ê¸°ì—…ì€ ì‹¤ì ì„ ë¶€í’€ë¦°ë‹¤\"**\n",
    "\n",
    "**í•™ìˆ ì  ë°°ê²½ (Beneish 1999):**\n",
    "- **M-Score**: ì¬ë¬´ì œí‘œ ì¡°ì‘ ê°€ëŠ¥ì„±ì„ ìˆ˜ì¹˜í™”í•œ ì§€í‘œ\n",
    "- 8ê°œ ì¬ë¬´ ë¹„ìœ¨ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ ê³„ì‚°\n",
    "- M-Score > -2.22: ì¡°ì‘ ì˜ì‹¬ (76% ì •í™•ë„)\n",
    "\n",
    "**Beneish M-Score 8ê°œ êµ¬ì„± ìš”ì†Œ:**\n",
    "\n",
    "| ì§€í‘œ | ì˜ë¯¸ | ì¡°ì‘ ì‹ í˜¸ |\n",
    "|------|------|----------|\n",
    "| DSRI | ë§¤ì¶œì±„ê¶Œ / ë§¤ì¶œ ì¦ê°€ìœ¨ | ë†’ì„ìˆ˜ë¡ ê°€ê³µë§¤ì¶œ ì˜ì‹¬ |\n",
    "| GMI | ë§¤ì¶œì´ì´ìµë¥  ë³€í™” | ê°ì†Œ ì‹œ ì¡°ì‘ ê°€ëŠ¥ì„± |\n",
    "| AQI | ìì‚° í’ˆì§ˆ ì§€ìˆ˜ | ë†’ì„ìˆ˜ë¡ ìì‚° ë¶€í’€ë¦¬ê¸° ì˜ì‹¬ |\n",
    "| SGI | ë§¤ì¶œ ì„±ì¥ë¥  | ê³¼ë„í•œ ì„±ì¥ ì‹œ ì˜ì‹¬ |\n",
    "| DEPI | ê°ê°€ìƒê°ë¥  ë³€í™” | ê°ì†Œ ì‹œ ì´ìµ ë¶€í’€ë¦¬ê¸° ì˜ì‹¬ |\n",
    "| SGAI | íŒê´€ë¹„ / ë§¤ì¶œ ë³€í™” | ì¦ê°€ ì‹œ ë¹„íš¨ìœ¨ ì˜ì‹¬ |\n",
    "| LVGI | ë ˆë²„ë¦¬ì§€ ì¦ê°€ìœ¨ | ì¦ê°€ ì‹œ ì¬ë¬´ìœ„í—˜ ì¦ê°€ |\n",
    "| TATA | ë°œìƒì•¡ / ì´ìì‚° | ë†’ì„ìˆ˜ë¡ í˜„ê¸ˆ ì—†ëŠ” ì´ìµ ì˜ì‹¬ |\n",
    "\n",
    "**M-Score ê³„ì‚°ì‹:**\n",
    "```\n",
    "M-Score = -4.84 + 0.92*DSRI + 0.528*GMI + 0.404*AQI + 0.892*SGI \n",
    "          + 0.115*DEPI - 0.172*SGAI + 4.679*TATA - 0.327*LVGI\n",
    "\n",
    "í•´ì„:\n",
    "- M-Score > -2.22: ì¡°ì‘ ê°€ëŠ¥ì„± ë†’ìŒ\n",
    "- M-Score â‰¤ -2.22: ì •ìƒ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manipulation_detection_features_complete(df):\n",
    "    \"\"\"ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„± ìƒì„± - ì™„ì „íŒ (Beneish M-Score ì™„ì „ êµ¬í˜„)\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # ê³µí†µ ë³€ìˆ˜ ì•ˆì „í•˜ê²Œ í™•ë³´\n",
    "    if 'ë¶€ì±„ë¹„ìœ¨' in df.columns:\n",
    "        ë¶€ì±„ë¹„ìœ¨ = df['ë¶€ì±„ë¹„ìœ¨']\n",
    "    elif 'ë¶€ì±„ì´ê³„' in df.columns and 'ìë³¸ì´ê³„' in df.columns:\n",
    "        ë¶€ì±„ë¹„ìœ¨ = df['ë¶€ì±„ì´ê³„'] / (df['ìë³¸ì´ê³„'].abs() + 1) * 100\n",
    "    else:\n",
    "        ë¶€ì±„ë¹„ìœ¨ = 100  # ê¸°ë³¸ê°’\n",
    "    \n",
    "    # 1. ë§¤ì¶œì±„ê¶Œ ì´ìƒ ì¦ê°€ (DSRI ê´€ë ¨)\n",
    "    if 'ë§¤ì¶œì±„ê¶Œ' in df.columns and 'ë§¤ì¶œì•¡' in df.columns:\n",
    "        features['ë§¤ì¶œì±„ê¶ŒíšŒì „ìœ¨'] = df['ë§¤ì¶œì•¡'] / (df['ë§¤ì¶œì±„ê¶Œ'] + 1)\n",
    "        features['ë§¤ì¶œì±„ê¶Œë¹„ìœ¨'] = df['ë§¤ì¶œì±„ê¶Œ'] / (df['ë§¤ì¶œì•¡'] + 1)\n",
    "        features['ë§¤ì¶œì±„ê¶Œ_ì´ìƒì§€í‘œ'] = features['ë§¤ì¶œì±„ê¶Œë¹„ìœ¨'] * (ë¶€ì±„ë¹„ìœ¨ / 100)\n",
    "    \n",
    "    # 2. ì¬ê³ ìì‚° ì´ìƒ ì ì²´\n",
    "    if 'ì¬ê³ ìì‚°' in df.columns and 'ë§¤ì¶œì›ê°€' in df.columns:\n",
    "        features['ì¬ê³ íšŒì „ìœ¨'] = df['ë§¤ì¶œì›ê°€'] / (df['ì¬ê³ ìì‚°'] + 1)\n",
    "        features['ì¬ê³ ë³´ìœ ì¼ìˆ˜'] = 365 / (features['ì¬ê³ íšŒì „ìœ¨'] + 0.1)\n",
    "        features['ì¬ê³ _ì´ìƒì§€í‘œ'] = df['ì¬ê³ ìì‚°'] / (df.get('ìì‚°ì´ê³„', 1) + 1) * 100\n",
    "    \n",
    "    # 3. ë°œìƒì•¡(Accruals) í’ˆì§ˆ (TATA)\n",
    "    if 'ë‹¹ê¸°ìˆœì´ìµ' in df.columns and 'ì˜ì—…í™œë™í˜„ê¸ˆíë¦„' in df.columns:\n",
    "        features['ì´ë°œìƒì•¡'] = df['ë‹¹ê¸°ìˆœì´ìµ'] - df['ì˜ì—…í™œë™í˜„ê¸ˆíë¦„']\n",
    "        features['ë°œìƒì•¡ë¹„ìœ¨'] = features['ì´ë°œìƒì•¡'] / (df.get('ìì‚°ì´ê³„', 1) + 1)\n",
    "        features['í˜„ê¸ˆíë¦„í’ˆì§ˆ'] = df['ì˜ì—…í™œë™í˜„ê¸ˆíë¦„'] / (df['ë‹¹ê¸°ìˆœì´ìµ'] + 1)\n",
    "    \n",
    "    # 4. ë¹„ìš© ìë³¸í™” ì˜ì‹¬ (AQI ê´€ë ¨)\n",
    "    if 'ë¬´í˜•ìì‚°' in df.columns:\n",
    "        features['ë¬´í˜•ìì‚°ë¹„ìœ¨'] = df['ë¬´í˜•ìì‚°'] / (df.get('ìì‚°ì´ê³„', 1) + 1)\n",
    "        if 'ì˜ì—…ë¹„ìš©' in df.columns:\n",
    "            features['ë¹„ìš©ìë³¸í™”ì§€í‘œ'] = df['ë¬´í˜•ìì‚°'] / (df.get('ì˜ì—…ë¹„ìš©', df['ë§¤ì¶œì›ê°€']) + 1)\n",
    "    \n",
    "    # 5. ë§¤ì¶œì´ì´ìµë¥  (GMI)\n",
    "    if 'ë§¤ì¶œì´ì´ìµ' in df.columns and 'ë§¤ì¶œì•¡' in df.columns:\n",
    "        features['ë§¤ì¶œì´ì´ìµë¥ '] = df['ë§¤ì¶œì´ì´ìµ'] / (df['ë§¤ì¶œì•¡'] + 1) * 100\n",
    "        features['ì˜ì—…ë ˆë²„ë¦¬ì§€'] = df.get('ì˜ì—…ì†ìµ', 0) / (df['ë§¤ì¶œì´ì´ìµ'] + 1)\n",
    "    \n",
    "    # 6. íŒê´€ë¹„ ì´ìƒ ì¦ê°€ (SGAI)\n",
    "    if 'íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„' in df.columns and 'ë§¤ì¶œì•¡' in df.columns:\n",
    "        features['íŒê´€ë¹„ìœ¨'] = df['íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„'] / (df['ë§¤ì¶œì•¡'] + 1) * 100\n",
    "        features['íŒê´€ë¹„íš¨ìœ¨ì„±'] = df.get('ì˜ì—…ì†ìµ', 0) / (df['íŒë§¤ë¹„ì™€ê´€ë¦¬ë¹„'] + 1)\n",
    "    \n",
    "    # 7. M-Score ì¢…í•© (í•œêµ­í˜•)\n",
    "    m_score = 0\n",
    "    if 'ë§¤ì¶œì±„ê¶Œë¹„ìœ¨' in features.columns:\n",
    "        m_score += features['ë§¤ì¶œì±„ê¶Œë¹„ìœ¨'] * 0.92  # DSRI ëŒ€ì²´\n",
    "    if 'ì¬ê³ _ì´ìƒì§€í‘œ' in features.columns:\n",
    "        m_score += features['ì¬ê³ _ì´ìƒì§€í‘œ'] * 0.528  # GMI ëŒ€ì²´\n",
    "    if 'ë°œìƒì•¡ë¹„ìœ¨' in features.columns:\n",
    "        m_score += features['ë°œìƒì•¡ë¹„ìœ¨'] * 4.679  # TATA\n",
    "    if 'ë¬´í˜•ìì‚°ë¹„ìœ¨' in features.columns:\n",
    "        m_score += features['ë¬´í˜•ìì‚°ë¹„ìœ¨'] * 0.404  # AQI ëŒ€ì²´\n",
    "    \n",
    "    features['M_Score_í•œêµ­í˜•'] = m_score - 2.22  # í•œêµ­ ì‹œì¥ ì¡°ì •\n",
    "    features['ì¬ë¬´ì¡°ì‘ìœ„í—˜'] = (features['M_Score_í•œêµ­í˜•'] > 0).astype(int)\n",
    "    \n",
    "    print(f\"âœ… ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„± {features.shape[1]}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "    return features\n",
    "\n",
    "manipulation_features = create_manipulation_detection_features_complete(df)\n",
    "print(\"\\nìƒì„±ëœ ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„±:\")\n",
    "print(manipulation_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í…Œê³ ë¦¬ 4: ì´í•´ê´€ê³„ì í–‰ë™ íŠ¹ì„± (10ê°œ)\n",
    "\n",
    "**ğŸ’¡ ì¬ë¬´ì œí‘œë³´ë‹¤ í–‰ë™ íŒ¨í„´ì´ ë” ì¤‘ìš”í•  ë•Œ**\n",
    "\n",
    "- ì—°ì²´ ì´ë ¥\n",
    "- ì„¸ê¸ˆ ì²´ë‚©\n",
    "- ì‹ ìš©ë“±ê¸‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stakeholder_features(df):\n",
    "    \"\"\"ì´í•´ê´€ê³„ì í–‰ë™ íŠ¹ì„± ìƒì„± (íŒ¨í„´ ë§¤ì¹­ + ì§‘ê³„)\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. ì‹ ìš© í–‰ë™ íŒ¨í„´ - ëª¨ë“  ì—°ì²´ ê´€ë ¨ ì»¬ëŸ¼ ì§‘ê³„\n",
    "    credit_cols = [col for col in df.columns if 'ì—°ì²´' in col]\n",
    "    if credit_cols:\n",
    "        features['ì´ì—°ì²´ê±´ìˆ˜'] = df[credit_cols].sum(axis=1)\n",
    "        features['ì—°ì²´ì—¬ë¶€'] = (features['ì´ì—°ì²´ê±´ìˆ˜'] > 0).astype(int)\n",
    "        # ë¶€ì±„ë¹„ìœ¨ ì•ˆì „í•˜ê²Œ í™•ë³´\n",
    "        if 'ë¶€ì±„ë¹„ìœ¨' in df.columns:\n",
    "            ë¶€ì±„ë¹„ìœ¨ = df['ë¶€ì±„ë¹„ìœ¨']\n",
    "        elif 'ë¶€ì±„ì´ê³„' in df.columns and 'ìë³¸ì´ê³„' in df.columns:\n",
    "            ë¶€ì±„ë¹„ìœ¨ = df['ë¶€ì±„ì´ê³„'] / (df['ìë³¸ì´ê³„'].abs() + 1) * 100\n",
    "        else:\n",
    "            ë¶€ì±„ë¹„ìœ¨ = 100\n",
    "        features['ì—°ì²´ì‹¬ê°ë„'] = features['ì´ì—°ì²´ê±´ìˆ˜'] * ë¶€ì±„ë¹„ìœ¨ / 100\n",
    "    \n",
    "    # 2. ì„¸ê¸ˆ ì²´ë‚© ë¦¬ìŠ¤í¬ - ëª¨ë“  ì²´ë‚© ê´€ë ¨ ì»¬ëŸ¼ ì§‘ê³„\n",
    "    tax_cols = [col for col in df.columns if 'ì²´ë‚©' in col or 'ì„¸ê¸ˆ' in col]\n",
    "    if tax_cols:\n",
    "        features['ì„¸ê¸ˆì²´ë‚©ê±´ìˆ˜'] = df[tax_cols].sum(axis=1)\n",
    "        features['ì„¸ê¸ˆì²´ë‚©ë¦¬ìŠ¤í¬'] = (features['ì„¸ê¸ˆì²´ë‚©ê±´ìˆ˜'] > 0).astype(int) * 5\n",
    "    \n",
    "    # 3. ê³µê³µì •ë³´ ë¦¬ìŠ¤í¬\n",
    "    public_cols = [col for col in df.columns if any(k in col for k in ['ì••ë¥˜', 'ì†Œì†¡', 'ê³µê³µ'])]\n",
    "    if public_cols:\n",
    "        features['ê³µê³µì •ë³´ë¦¬ìŠ¤í¬'] = df[public_cols].sum(axis=1)\n",
    "        features['ë²•ì ë¦¬ìŠ¤í¬'] = (features['ê³µê³µì •ë³´ë¦¬ìŠ¤í¬'] > 0).astype(int) * 3\n",
    "    \n",
    "    # 4. ì‹ ìš©ë“±ê¸‰ ë¦¬ìŠ¤í¬\n",
    "    rating_cols = [col for col in df.columns if 'ì‹ ìš©í‰ê°€ë“±ê¸‰' in col or 'ì‹ ìš©ë“±ê¸‰' in col]\n",
    "    if rating_cols:\n",
    "        features['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] = df[rating_cols[0]]\n",
    "        features['ì‹ ìš©ë“±ê¸‰ìœ„í—˜'] = (df[rating_cols[0]] >= 5).astype(int)\n",
    "    \n",
    "    # 5. ì¢…í•© ì‹ ë¢°ë„ ì§€í‘œ\n",
    "    features['ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜'] = (\n",
    "        features.get('ì—°ì²´ì—¬ë¶€', 0) * 2 +\n",
    "        features.get('ì„¸ê¸ˆì²´ë‚©ë¦¬ìŠ¤í¬', 0) +\n",
    "        features.get('ë²•ì ë¦¬ìŠ¤í¬', 0) +\n",
    "        features.get('ì‹ ìš©ë“±ê¸‰ì ìˆ˜', 0) / 2\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ì´í•´ê´€ê³„ì í–‰ë™ íŠ¹ì„± {features.shape[1]}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "    return features\n",
    "\n",
    "stakeholder_features = create_stakeholder_features(df)\n",
    "print(\"\\nìƒì„±ëœ ì´í•´ê´€ê³„ì í–‰ë™ íŠ¹ì„±:\")\n",
    "print(stakeholder_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í…Œê³ ë¦¬ 5: í•œêµ­ ì‹œì¥ íŠ¹í™” íŠ¹ì„± (6ê°œ)\n",
    "\n",
    "**ğŸ’¡ í•œêµ­ ê¸°ì—… ë¶€ë„ì˜ íŠ¹ìˆ˜ì„±**\n",
    "\n",
    "í•œêµ­ ì‹œì¥ ê³ ìœ ì˜ ë¦¬ìŠ¤í¬ ìš”ì¸:\n",
    "- **ì™¸ê° ì—¬ë¶€**: ì™¸ë¶€ê°ì‚¬ ì˜ë¬´ ì—¬ë¶€ê°€ ì¬ë¬´ ì‹ ë¢°ì„±ì— ì˜í–¥\n",
    "- **ì œì¡°ì—… ë¦¬ìŠ¤í¬**: í•œêµ­ì€ ì œì¡°ì—… ì¤‘ì‹¬ ê²½ì œ (ë¶€ë„ìœ¨ì´ ì„œë¹„ìŠ¤ì—…ë³´ë‹¤ ë†’ìŒ)\n",
    "- **ëŒ€ê¸°ì—… ì˜ì¡´ë„**: ë§¤ì¶œì²˜ ì§‘ì¤‘ë„ê°€ ë†’ì„ìˆ˜ë¡ ë¦¬ìŠ¤í¬ ì¦ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_korean_market_features(df):\n",
    "    \"\"\"í•œêµ­ ì‹œì¥ íŠ¹í™” íŠ¹ì„± ìƒì„±\"\"\"\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. ì™¸ê° ì—¬ë¶€\n",
    "    audit_cols = [col for col in df.columns if 'ì™¸ê°' in col or 'ê°ì‚¬' in col]\n",
    "    if audit_cols:\n",
    "        features['ì™¸ê°ì—¬ë¶€'] = df[audit_cols[0]]\n",
    "        features['ì™¸ê°ë¦¬ìŠ¤í¬'] = (1 - df[audit_cols[0]]).astype(int)\n",
    "    else:\n",
    "        # ìì‚° ê·œëª¨ë¡œ ì¶”ì •\n",
    "        if 'ìì‚°ì´ê³„' in df.columns:\n",
    "            features['ì™¸ê°ì—¬ë¶€'] = (df['ìì‚°ì´ê³„'] >= 12000000000).astype(int)\n",
    "            features['ì™¸ê°ë¦¬ìŠ¤í¬'] = (1 - features['ì™¸ê°ì—¬ë¶€']).astype(int)\n",
    "    \n",
    "    # 2. ì œì¡°ì—… ë¦¬ìŠ¤í¬\n",
    "    industry_cols = [col for col in df.columns if 'KSIC' in col or 'ì‚°ì—…ë¶„ë¥˜' in col or 'ì—…ì¢…' in col]\n",
    "    if industry_cols:\n",
    "        features['ì œì¡°ì—…ì—¬ë¶€'] = df[industry_cols[0]].astype(str).str.startswith('C').astype(int)\n",
    "        features['ì œì¡°ì—…ë¦¬ìŠ¤í¬'] = features['ì œì¡°ì—…ì—¬ë¶€'] * 1.5\n",
    "    else:\n",
    "        # ì¬ê³ ìì‚° ë¹„ì¤‘ìœ¼ë¡œ ì¶”ì •\n",
    "        if 'ì¬ê³ ìì‚°' in df.columns and 'ìì‚°ì´ê³„' in df.columns:\n",
    "            inventory_ratio = df['ì¬ê³ ìì‚°'] / (df['ìì‚°ì´ê³„'] + 1)\n",
    "            features['ì œì¡°ì—…ì—¬ë¶€'] = (inventory_ratio > 0.1).astype(int)\n",
    "            features['ì œì¡°ì—…ë¦¬ìŠ¤í¬'] = features['ì œì¡°ì—…ì—¬ë¶€'] * 1.5\n",
    "    \n",
    "    # 3. ëŒ€ê¸°ì—… ì˜ì¡´ë„ (ë§¤ì¶œ ì§‘ì¤‘ë„)\n",
    "    if 'ë§¤ì¶œì•¡' in df.columns and 'ìì‚°ì´ê³„' in df.columns:\n",
    "        sales_to_assets = df['ë§¤ì¶œì•¡'] / (df['ìì‚°ì´ê³„'] + 1)\n",
    "        features['ë§¤ì¶œì§‘ì¤‘ë„'] = sales_to_assets\n",
    "        features['ë§¤ì¶œì§‘ì¤‘ë¦¬ìŠ¤í¬'] = (sales_to_assets > 2).astype(int) * 2\n",
    "    \n",
    "    print(f\"âœ… í•œêµ­ ì‹œì¥ íŠ¹í™” íŠ¹ì„± {features.shape[1]}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "    return features\n",
    "\n",
    "korean_features = create_korean_market_features(df)\n",
    "print(\"\\nìƒì„±ëœ í•œêµ­ ì‹œì¥ íŠ¹í™” íŠ¹ì„±:\")\n",
    "print(korean_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  íŠ¹ì„±ì„ í•˜ë‚˜ë¡œ í†µí•©\n",
    "all_features = pd.concat([\n",
    "    liquidity_features,\n",
    "    insolvency_features,\n",
    "    manipulation_features,\n",
    "    stakeholder_features,\n",
    "    korean_features\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nâœ… ì´ {all_features.shape[1]}ê°œì˜ ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì™„ë£Œ\")\n",
    "print(\"\\níŠ¹ì„± ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜:\")\n",
    "print(f\"  - ìœ ë™ì„± ìœ„ê¸°: {liquidity_features.shape[1]}ê°œ\")\n",
    "print(f\"  - ì§€ê¸‰ë¶ˆëŠ¥ íŒ¨í„´: {insolvency_features.shape[1]}ê°œ\")\n",
    "print(f\"  - ì¬ë¬´ì¡°ì‘ íƒì§€: {manipulation_features.shape[1]}ê°œ\")\n",
    "print(f\"  - ì´í•´ê´€ê³„ì í–‰ë™: {stakeholder_features.shape[1]}ê°œ\")\n",
    "print(f\"  - í•œêµ­ ì‹œì¥ íŠ¹í™”: {korean_features.shape[1]}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Feature Validation Matrix â­ (ìˆ˜ì •ë¨)\n",
    "\n",
    "**ìƒì„±í•œ ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ í†µê³„ì  ê²€ì¦ ìˆ˜í–‰**\n",
    "\n",
    "- Mann-Whitney U test (ì •ìƒ vs ë¶€ë„ ê¸°ì—… ì°¨ì´)\n",
    "- Cliff's Delta (íš¨ê³¼ í¬ê¸°)\n",
    "- AUC (ë‹¨ë³€ëŸ‰ ì˜ˆì¸¡ë ¥)\n",
    "\n",
    "**ê¸°ì¤€:**\n",
    "- p-value < 0.01 (í†µê³„ì  ìœ ì˜ì„±)\n",
    "- |Cliff's Delta| > 0.2 (ì¤‘ê°„ ì´ìƒ íš¨ê³¼ í¬ê¸°)\n",
    "- AUC > 0.6 (ì•½í•œ ì˜ˆì¸¡ë ¥ ì´ìƒ)\n",
    "\n",
    "**âš ï¸ ë¬¸ì œ í•´ê²°: join ì—ëŸ¬ ìˆ˜ì •**\n",
    "- ê¸°ì¡´: `df.join(all_features)` â†’ ì»¬ëŸ¼ ì¤‘ë³µ ì—ëŸ¬\n",
    "- ìˆ˜ì •: íƒ€ê²Ÿê³¼ íŠ¹ì„±ì„ ë³„ë„ë¡œ ì¸ë±ì‹±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Validation Matrix - join ì—ëŸ¬ ìˆ˜ì •ë¨\n",
    "validation_results = []\n",
    "\n",
    "print(f\"ê²€ì¦í•  íŠ¹ì„± ìˆ˜: {len(all_features.columns)}\")\n",
    "print(\"\\níŠ¹ì„± ê²€ì¦ ì§„í–‰ ì¤‘...\")\n",
    "\n",
    "for feature in all_features.columns:\n",
    "    try:\n",
    "        # ìˆ˜ì •ëœ ë¶€ë¶„: join ëŒ€ì‹  ì§ì ‘ ì¸ë±ì‹±\n",
    "        normal = all_features.loc[df[target_col] == 0, feature].dropna()\n",
    "        bankrupt = all_features.loc[df[target_col] == 1, feature].dropna()\n",
    "        \n",
    "        if len(normal) > 0 and len(bankrupt) > 0:\n",
    "            # í†µê³„ ê²€ì •\n",
    "            u_stat, p_value = mannwhitneyu(normal, bankrupt, alternative='two-sided')\n",
    "            \n",
    "            # Cliff's delta (íš¨ê³¼ í¬ê¸°)\n",
    "            n1, n2 = len(normal), len(bankrupt)\n",
    "            cliff_delta = (u_stat - n1*n2/2) / (n1*n2)\n",
    "            \n",
    "            # AUC ê³„ì‚°\n",
    "            auc = None\n",
    "            try:\n",
    "                feature_data = all_features[feature].fillna(all_features[feature].median())\n",
    "                feature_data = feature_data.replace([np.inf, -np.inf], 0)\n",
    "                if feature_data.std() > 0:\n",
    "                    auc = roc_auc_score(df[target_col], feature_data)\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            # ê²€ì¦ ê²°ê³¼ ì €ì¥\n",
    "            validation_results.append({\n",
    "                'Feature': feature,\n",
    "                'Normal_Median': float(normal.median()),\n",
    "                'Bankrupt_Median': float(bankrupt.median()),\n",
    "                'p_value': float(p_value),\n",
    "                'Cliff_Delta': float(cliff_delta),\n",
    "                'AUC': float(auc) if auc is not None else 0.5,\n",
    "                'Keep': 'âœ…' if (p_value < 0.01 and abs(cliff_delta) > 0.2) else 'âš ï¸'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ {feature}: {str(e)[:80]}\")\n",
    "\n",
    "print(f\"\\nê²€ì¦ ì™„ë£Œ: {len(validation_results)}ê°œ íŠ¹ì„±\")\n",
    "\n",
    "if len(validation_results) > 0:\n",
    "    validation_df = pd.DataFrame(validation_results)\n",
    "    validation_df = validation_df.sort_values('AUC', ascending=False)\n",
    "    \n",
    "    print(\"\\nğŸ“Š íŠ¹ì„± ê²€ì¦ ê²°ê³¼ (ìƒìœ„ 20ê°œ):\")\n",
    "    print(validation_df.head(20).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nâœ… í†µê³¼ íŠ¹ì„± (p<0.01 & |Cliff's Delta|>0.2): {(validation_df['Keep'] == 'âœ…').sum()}ê°œ\")\n",
    "    print(f\"âš ï¸ ì£¼ì˜ íŠ¹ì„±: {(validation_df['Keep'] == 'âš ï¸').sum()}ê°œ\")\n",
    "else:\n",
    "    validation_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ ê²€ì¦ëœ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC ì‹œê°í™” â­ (ìˆ˜ì •ë¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC ì‹œê°í™” - ìˆ˜ì •ë¨\n",
    "if len(validation_df) > 0 and 'AUC' in validation_df.columns:\n",
    "    top_features = validation_df.nlargest(min(15, len(validation_df)), 'AUC')\n",
    "    \n",
    "    # ìƒ‰ìƒ ë§¤í•‘\n",
    "    colors = ['#2ecc71' if x == 'âœ…' else '#e74c3c' for x in top_features['Keep']]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=top_features['Feature'].values[::-1],\n",
    "        x=top_features['AUC'].values[::-1],\n",
    "        orientation='h',\n",
    "        marker_color=colors[::-1],\n",
    "        text=top_features['AUC'].values[::-1].round(3),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='<b>íŠ¹ì„±ë³„ ë‹¨ë³€ëŸ‰ AUC</b> (ìƒìœ„ 15ê°œ)',\n",
    "        xaxis_title='AUC',\n",
    "        yaxis_title='íŠ¹ì„±ëª…',\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # AUC ê¸°ì¤€ì„  í‘œì‹œ\n",
    "    fig.add_vline(x=0.6, line_dash=\"dash\", line_color=\"gray\",\n",
    "                  annotation_text=\"ìµœì†Œ ê¸°ì¤€ (0.6)\")\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"âš ï¸ ê²€ì¦ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ AUC ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Feature Selection: ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ë° ìµœì í™”\n",
    "\n",
    "**ì „ëµ:**\n",
    "1. Information Value (IV) ê¸°ë°˜ í•„í„°ë§\n",
    "2. ìƒê´€ê´€ê³„ ë¶„ì„ (|r| > 0.9 ì œê±°)\n",
    "3. **â­ VIF (Variance Inflation Factor) í™•ì¸** â† ì‹ ê·œ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Information Value (IV) ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iv(df, feature, target, bins=10):\n",
    "    \"\"\"Information Value ê³„ì‚°\"\"\"\n",
    "    try:\n",
    "        df_temp = pd.DataFrame({\n",
    "            'feature': df[feature],\n",
    "            'target': target\n",
    "        }).dropna()\n",
    "        \n",
    "        if len(df_temp) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ êµ¬ê°„í™”\n",
    "        df_temp['feature_bin'] = pd.qcut(df_temp['feature'], q=bins, duplicates='drop')\n",
    "        \n",
    "        # ê° êµ¬ê°„ë³„ Good/Bad ê³„ì‚°\n",
    "        grouped = df_temp.groupby('feature_bin')['target'].agg([\n",
    "            ('good', lambda x: (x == 0).sum()),\n",
    "            ('bad', lambda x: (x == 1).sum())\n",
    "        ])\n",
    "        \n",
    "        total_good = (target == 0).sum()\n",
    "        total_bad = (target == 1).sum()\n",
    "        \n",
    "        grouped['good_pct'] = grouped['good'] / total_good\n",
    "        grouped['bad_pct'] = grouped['bad'] / total_bad\n",
    "        \n",
    "        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "        grouped['good_pct'] = grouped['good_pct'].replace(0, 0.0001)\n",
    "        grouped['bad_pct'] = grouped['bad_pct'].replace(0, 0.0001)\n",
    "        \n",
    "        grouped['woe'] = np.log(grouped['bad_pct'] / grouped['good_pct'])\n",
    "        grouped['iv'] = (grouped['bad_pct'] - grouped['good_pct']) * grouped['woe']\n",
    "        \n",
    "        return grouped['iv'].sum()\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# IV ê³„ì‚°\n",
    "print(\"Information Value ê³„ì‚° ì¤‘...\")\n",
    "iv_results = []\n",
    "\n",
    "# ë¬´í•œëŒ€/ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "all_features_clean = all_features.fillna(all_features.median())\n",
    "all_features_clean = all_features_clean.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "for feature in all_features_clean.columns:\n",
    "    feature_data = all_features_clean[feature]\n",
    "    if feature_data.std() > 0:\n",
    "        iv = calculate_iv(pd.DataFrame({feature: feature_data}), feature, df[target_col])\n",
    "        iv_results.append((feature, iv))\n",
    "\n",
    "iv_df = pd.DataFrame(iv_results, columns=['íŠ¹ì„±', 'IV']).sort_values('IV', ascending=False)\n",
    "\n",
    "# IV í•´ì„\n",
    "iv_df['ì˜ˆì¸¡ë ¥'] = pd.cut(iv_df['IV'], \n",
    "                      bins=[0, 0.02, 0.1, 0.3, 0.5, np.inf],\n",
    "                      labels=['ì—†ìŒ', 'ì•½í•¨', 'ì¤‘ê°„', 'ê°•í•¨', 'ê³¼ì í•©ìœ„í—˜'])\n",
    "\n",
    "print(\"\\nğŸ“Š Information Value ìƒìœ„ 20ê°œ íŠ¹ì„±:\")\n",
    "print(iv_df.head(20))\n",
    "\n",
    "print(\"\\nì˜ˆì¸¡ë ¥ ë¶„í¬:\")\n",
    "print(iv_df['ì˜ˆì¸¡ë ¥'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ìƒê´€ê´€ê³„ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "corr_matrix = all_features_clean.corr()\n",
    "\n",
    "# ê³ ìƒê´€ ë³€ìˆ˜ ìŒ ì°¾ê¸° (|r| > 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"\\nâš ï¸ ê³ ìƒê´€(|r| > 0.9) ë³€ìˆ˜ ìŒ: {len(high_corr_pairs)}ê°œ\")\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['ë³€ìˆ˜1', 'ë³€ìˆ˜2', 'ìƒê´€ê³„ìˆ˜'])\n",
    "    print(high_corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 VIF ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ â­ (ì‹ ê·œ ì¶”ê°€)\n",
    "\n",
    "**ğŸ’¡ ì™œ VIFê°€ í•„ìš”í•œê°€?**\n",
    "\n",
    "**ìƒê´€ê³„ìˆ˜ vs VIF:**\n",
    "- **ìƒê´€ê³„ìˆ˜**: 2ê°œ ë³€ìˆ˜ ê°„ ê´€ê³„ë§Œ ì¸¡ì •\n",
    "- **VIF**: í•œ ë³€ìˆ˜ì™€ ë‚˜ë¨¸ì§€ ëª¨ë“  ë³€ìˆ˜ì˜ ê´€ê³„ ì¸¡ì •\n",
    "\n",
    "**ì˜ˆì‹œ:**\n",
    "```\n",
    "Aì™€ Bì˜ ìƒê´€ê³„ìˆ˜ 0.7, Aì™€ Cì˜ ìƒê´€ê³„ìˆ˜ 0.7\n",
    "â†’ Aì˜ VIFëŠ” 20 ì´ìƒ ê°€ëŠ¥ (B, Cì™€ ë™ì‹œì— ë†’ì€ ìƒê´€)\n",
    "```\n",
    "\n",
    "**VIF í•´ì„:**\n",
    "- VIF < 5: ë‹¤ì¤‘ê³µì„ ì„± ì—†ìŒ\n",
    "- 5 â‰¤ VIF < 10: ì•½í•œ ë‹¤ì¤‘ê³µì„ ì„±\n",
    "- VIF â‰¥ 10: ê°•í•œ ë‹¤ì¤‘ê³µì„ ì„± (ì œê±° ê³ ë ¤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df):\n",
    "    \"\"\"VIF ê³„ì‚°\"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df.columns\n",
    "    \n",
    "    vif_values = []\n",
    "    for i in range(len(df.columns)):\n",
    "        try:\n",
    "            vif = variance_inflation_factor(df.values, i)\n",
    "            # ë¬´í•œëŒ€ ì²˜ë¦¬\n",
    "            if np.isinf(vif) or np.isnan(vif):\n",
    "                vif = 999\n",
    "            vif_values.append(vif)\n",
    "        except:\n",
    "            vif_values.append(999)\n",
    "    \n",
    "    vif_data[\"VIF\"] = vif_values\n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"VIF ê³„ì‚° ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)\")\n",
    "\n",
    "# ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚° ì†ë„ í–¥ìƒ (ì „ì²´ ë°ì´í„°ì˜ 20%)\n",
    "sample_size = int(len(all_features_clean) * 0.2)\n",
    "sample_data = all_features_clean.sample(n=sample_size, random_state=42)\n",
    "\n",
    "vif_df = calculate_vif(sample_data)\n",
    "\n",
    "print(\"\\nğŸ“Š VIF ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 20ê°œ):\")\n",
    "print(vif_df.head(20))\n",
    "\n",
    "# VIF > 10ì¸ íŠ¹ì„± ì°¾ê¸°\n",
    "high_vif_features = vif_df[vif_df['VIF'] > 10]\n",
    "print(f\"\\nâš ï¸ VIF > 10ì¸ íŠ¹ì„±: {len(high_vif_features)}ê°œ\")\n",
    "if len(high_vif_features) > 0:\n",
    "    print(high_vif_features[['Feature', 'VIF']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 ìŠ¤ë§ˆíŠ¸ íŠ¹ì„± ì œê±° ë¡œì§ â­\n",
    "\n",
    "**ì œê±° ìš°ì„ ìˆœìœ„:**\n",
    "\n",
    "1. **ìš°ì„ ìˆœìœ„ 1: VIF é«˜ + ì˜ˆì¸¡ë ¥ ä½ â†’ ì œê±°**\n",
    "   - VIF > 10 AND IV < 0.1 AND AUC < 0.6\n",
    "   - ì´ìœ : ë‹¤ì¤‘ê³µì„ ì„± ë†’ê³  ì˜ˆì¸¡ë ¥ë„ ì—†ìŒ\n",
    "\n",
    "2. **ìš°ì„ ìˆœìœ„ 2: ê³ ìƒê´€ ìŒ ì¤‘ ì˜ˆì¸¡ë ¥ ë‚®ì€ ê²ƒ ì œê±°**\n",
    "   - ìƒê´€ê³„ìˆ˜ > 0.9ì¸ ìŒ ì°¾ê¸°\n",
    "   - ë‘ ë³€ìˆ˜ ì¤‘ IVê°€ ë‚®ì€ ê²ƒ ì œê±°\n",
    "\n",
    "3. **ìš°ì„ ìˆœìœ„ 3: VIF é«˜ but ì˜ˆì¸¡ë ¥ å¼º â†’ ìœ ì§€ (ê²½ê³ ë§Œ)**\n",
    "   - VIF > 10 AND (IV >= 0.1 OR AUC >= 0.6)\n",
    "   - ì´ìœ : ë‹¤ì¤‘ê³µì„ ì„±ì€ ìˆì§€ë§Œ ì˜ˆì¸¡ë ¥ì´ ê°•í•˜ë¯€ë¡œ ìœ ì§€\n",
    "   - ì£¼ì˜: ëª¨ë¸ í•´ì„ ì‹œ ì£¼ì˜ í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_feature_selection(vif_df, iv_df, validation_df, corr_matrix):\n",
    "    \"\"\"VIF + IV + AUC ì¢…í•© ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ íŠ¹ì„± ì„ íƒ\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ” VIF ê¸°ë°˜ ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    removed_features = set()\n",
    "    kept_features = set()\n",
    "    warnings_features = set()\n",
    "    \n",
    "    # VIF > 10ì¸ íŠ¹ì„± ë¶„ì„\n",
    "    high_vif = vif_df[vif_df['VIF'] > 10]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š VIF > 10ì¸ íŠ¹ì„± ({len(high_vif)}ê°œ ë°œê²¬)\\n\")\n",
    "    \n",
    "    for idx, row in high_vif.iterrows():\n",
    "        feature = row['Feature']\n",
    "        vif = row['VIF']\n",
    "        \n",
    "        # IV ì°¾ê¸°\n",
    "        iv_row = iv_df[iv_df['íŠ¹ì„±'] == feature]\n",
    "        iv = iv_row['IV'].values[0] if len(iv_row) > 0 else 0\n",
    "        \n",
    "        # AUC ì°¾ê¸°\n",
    "        auc_row = validation_df[validation_df['Feature'] == feature]\n",
    "        auc = auc_row['AUC'].values[0] if len(auc_row) > 0 else 0.5\n",
    "        \n",
    "        print(f\"{len(removed_features) + len(kept_features) + 1}. {feature}\")\n",
    "        print(f\"   - VIF: {vif:.1f}\")\n",
    "        print(f\"   - IV: {iv:.3f} ({'ê°•í•¨' if iv >= 0.1 else 'ì•½í•¨'})\")\n",
    "        print(f\"   - AUC: {auc:.2f}\")\n",
    "        \n",
    "        # íŒì • ë¡œì§\n",
    "        if vif > 10 and iv < 0.1 and auc < 0.6:\n",
    "            removed_features.add(feature)\n",
    "            print(f\"   - íŒì •: âŒ ì œê±° (VIF é«˜ + ì˜ˆì¸¡ë ¥ ä½)\")\n",
    "            print(f\"   - ì‚¬ìœ : ë‹¤ì¤‘ê³µì„ ì„± ë†’ê³  ì˜ˆì¸¡ë ¥ ê±°ì˜ ì—†ìŒ\")\n",
    "        elif vif > 10 and (iv >= 0.1 or auc >= 0.6):\n",
    "            kept_features.add(feature)\n",
    "            warnings_features.add(feature)\n",
    "            print(f\"   - íŒì •: âœ… ìœ ì§€ (VIF é«˜ but ì˜ˆì¸¡ë ¥ å¼º)\")\n",
    "            print(f\"   - âš ï¸ ì£¼ì˜: ë‹¤ì¤‘ê³µì„ ì„± ì¡´ì¬, ëª¨ë¸ í•´ì„ ì‹œ ì£¼ì˜ í•„ìš”\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # ê³ ìƒê´€ ìŒ ì²˜ë¦¬\n",
    "    print(\"\\nğŸ“Š ê³ ìƒê´€(|r| > 0.9) ë³€ìˆ˜ ìŒ ë¶„ì„\\n\")\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                feat1 = corr_matrix.columns[i]\n",
    "                feat2 = corr_matrix.columns[j]\n",
    "                \n",
    "                # ì´ë¯¸ ì œê±°ëœ íŠ¹ì„±ì€ ê±´ë„ˆë›°ê¸°\n",
    "                if feat1 in removed_features or feat2 in removed_features:\n",
    "                    continue\n",
    "                \n",
    "                # IV ë¹„êµ\n",
    "                iv1 = iv_df[iv_df['íŠ¹ì„±'] == feat1]['IV'].values[0] if len(iv_df[iv_df['íŠ¹ì„±'] == feat1]) > 0 else 0\n",
    "                iv2 = iv_df[iv_df['íŠ¹ì„±'] == feat2]['IV'].values[0] if len(iv_df[iv_df['íŠ¹ì„±'] == feat2]) > 0 else 0\n",
    "                \n",
    "                if iv1 < iv2:\n",
    "                    removed_features.add(feat1)\n",
    "                    print(f\"âŒ ì œê±°: {feat1} (IV={iv1:.3f})\")\n",
    "                    print(f\"   ìœ ì§€: {feat2} (IV={iv2:.3f})\")\n",
    "                    print(f\"   ìƒê´€ê³„ìˆ˜: {corr_matrix.iloc[i, j]:.3f}\\n\")\n",
    "                else:\n",
    "                    removed_features.add(feat2)\n",
    "                    print(f\"   ìœ ì§€: {feat1} (IV={iv1:.3f})\")\n",
    "                    print(f\"âŒ ì œê±°: {feat2} (IV={iv2:.3f})\")\n",
    "                    print(f\"   ìƒê´€ê³„ìˆ˜: {corr_matrix.iloc[i, j]:.3f}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ìµœì¢… ê²°ê³¼:\")\n",
    "    print(f\"  - VIF > 10 íŠ¹ì„±: {len(high_vif)}ê°œ\")\n",
    "    print(f\"  - ìœ ì§€ (ì˜ˆì¸¡ë ¥ ê°•í•¨): {len(kept_features)}ê°œ\")\n",
    "    print(f\"  - ì œê±° (VIF é«˜ + ì˜ˆì¸¡ë ¥ ä½): {len(removed_features)}ê°œ\")\n",
    "    print(f\"  - âš ï¸ ê²½ê³  (ë‹¤ì¤‘ê³µì„ ì„± ì£¼ì˜): {len(warnings_features)}ê°œ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return list(removed_features), list(warnings_features)\n",
    "\n",
    "# ìŠ¤ë§ˆíŠ¸ íŠ¹ì„± ì„ íƒ ì‹¤í–‰\n",
    "removed_by_vif, warning_features = smart_feature_selection(\n",
    "    vif_df, iv_df, validation_df, corr_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 ìµœì¢… íŠ¹ì„± ì„ íƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… íŠ¹ì„± ì„ íƒ\n",
    "# 1ë‹¨ê³„: IV > 0.02 íŠ¹ì„±ë§Œ ì„ íƒ\n",
    "good_iv_features = set(iv_df[iv_df['IV'] > 0.02]['íŠ¹ì„±'].tolist())\n",
    "print(f\"1ë‹¨ê³„: IV > 0.02 íŠ¹ì„±: {len(good_iv_features)}ê°œ\")\n",
    "\n",
    "# 2ë‹¨ê³„: VIF ê¸°ë°˜ ì œê±° íŠ¹ì„± ì œì™¸\n",
    "final_features_set = good_iv_features - set(removed_by_vif)\n",
    "print(f\"2ë‹¨ê³„: VIF ê¸°ë°˜ ì œê±° í›„: {len(final_features_set)}ê°œ\")\n",
    "\n",
    "# 3ë‹¨ê³„: ìµœì¢… íŠ¹ì„± ë¦¬ìŠ¤íŠ¸\n",
    "final_features_list = list(final_features_set)\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ì„ íƒëœ íŠ¹ì„±: {len(final_features_list)}ê°œ\")\n",
    "print(f\"\\nì„ íƒëœ íŠ¹ì„± ëª©ë¡:\")\n",
    "for i, feat in enumerate(sorted(final_features_list), 1):\n",
    "    # IVì™€ AUC ì •ë³´ ì¶”ê°€\n",
    "    iv_val = iv_df[iv_df['íŠ¹ì„±'] == feat]['IV'].values[0] if len(iv_df[iv_df['íŠ¹ì„±'] == feat]) > 0 else 0\n",
    "    auc_val = validation_df[validation_df['Feature'] == feat]['AUC'].values[0] if len(validation_df[validation_df['Feature'] == feat]) > 0 else 0.5\n",
    "    warning = \" âš ï¸\" if feat in warning_features else \"\"\n",
    "    print(f\"  {i:2d}. {feat:30s} (IV={iv_val:.3f}, AUC={auc_val:.3f}){warning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ íƒëœ íŠ¹ì„±ìœ¼ë¡œ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±\n",
    "final_features_data = all_features_clean[final_features_list].copy()\n",
    "final_dataset = pd.concat([df[target_col], final_features_data], axis=1)\n",
    "\n",
    "# ì €ì¥\n",
    "output_path = '../data/features/domain_based_features_ì™„ì „íŒ.csv'\n",
    "final_dataset.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "print(f\"   - Shape: {final_dataset.shape}\")\n",
    "print(f\"   - íƒ€ê²Ÿ: 1ê°œ\")\n",
    "print(f\"   - íŠ¹ì„±: {len(final_features_list)}ê°œ\")\n",
    "print(f\"   - ìš©ëŸ‰: {final_dataset.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "metadata = pd.DataFrame({\n",
    "    'íŠ¹ì„±ëª…': final_features_list,\n",
    "    'IV': [iv_df[iv_df['íŠ¹ì„±'] == f]['IV'].values[0] if len(iv_df[iv_df['íŠ¹ì„±'] == f]) > 0 else 0 for f in final_features_list],\n",
    "    'AUC': [validation_df[validation_df['Feature'] == f]['AUC'].values[0] if len(validation_df[validation_df['Feature'] == f]) > 0 else 0.5 for f in final_features_list],\n",
    "    'ë‹¤ì¤‘ê³µì„ ì„±ê²½ê³ ': [f in warning_features for f in final_features_list]\n",
    "})\n",
    "metadata = metadata.sort_values('IV', ascending=False)\n",
    "\n",
    "metadata_path = '../data/features/feature_metadata_ì™„ì „íŒ.csv'\n",
    "metadata.to_csv(metadata_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… íŠ¹ì„± ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## âœ… Key Takeaways\n\n### ìƒì„±ëœ íŠ¹ì„± ìš”ì•½\n\n| ì¹´í…Œê³ ë¦¬ | ìƒì„± | ìµœì¢… ì„ íƒ | ì£¼ìš” íŠ¹ì„± |\n|----------|------|-----------|----------|\n| **ìœ ë™ì„± ìœ„ê¸°** | 10ê°œ | - | í˜„ê¸ˆì†Œì§„ì¼ìˆ˜, ì¦‰ê°ì§€ê¸‰ëŠ¥ë ¥ |\n| **ì§€ê¸‰ë¶ˆëŠ¥ íŒ¨í„´** | 11ê°œ | - | ì´ìë³´ìƒë°°ìœ¨, ìë³¸ì ì‹ë„ |\n| **ì¬ë¬´ì¡°ì‘ íƒì§€** | 15ê°œ | - | M-Score, ë°œìƒì•¡ë¹„ìœ¨ |\n| **ì´í•´ê´€ê³„ì í–‰ë™** | 10ê°œ | - | ì—°ì²´ì—¬ë¶€, ì‹ ìš©ë“±ê¸‰ |\n| **í•œêµ­ ì‹œì¥ íŠ¹í™”** | 6ê°œ | - | ì™¸ê°ì—¬ë¶€, ì œì¡°ì—…ë¦¬ìŠ¤í¬ |\n| **í•©ê³„** | **52ê°œ** | **-** | - |\n\n### í•µì‹¬ ë°œê²¬\n\n1. **âœ… Beneish M-Score ì™„ì „ êµ¬í˜„**\n   - 15ê°œ ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„± ìƒì„±\n   - M-Score ì¢…í•© ì§€í‘œ í¬í•¨\n   - í•œêµ­ ì‹œì¥ íŠ¹ì„± ë°˜ì˜\n\n2. **âœ… Feature Validation ì„±ê³µ**\n   - ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ í†µê³„ì  ê²€ì¦ ì™„ë£Œ\n   - p-value, Cliff's Delta, AUC ê³„ì‚°\n   - join ì—ëŸ¬ í•´ê²°\n\n3. **âœ… VIF ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì¶”ê°€**\n   - VIF > 10 íŠ¹ì„± ì‹ë³„\n   - ìŠ¤ë§ˆíŠ¸ ì œê±° ë¡œì§ êµ¬í˜„ (ì˜ˆì¸¡ë ¥ ê³ ë ¤)\n   - ë‹¤ì¤‘ê³µì„ ì„± ê²½ê³  íŠ¹ì„± í‘œì‹œ\n\n4. **ìœ ë™ì„± íŠ¹ì„±ì˜ ìš°ìˆ˜ì„±**\n   - í˜„ê¸ˆì†Œì§„ì¼ìˆ˜, ì¦‰ê°ì§€ê¸‰ëŠ¥ë ¥ ë“±ì´ ë†’ì€ AUC\n   - ë¶€ë„ 3ê°œì›” ì „ ì¡°ê¸° ê²½ë³´ ê°€ëŠ¥\n\n5. **ì´í•´ê´€ê³„ì í–‰ë™ì˜ ì¤‘ìš”ì„±**\n   - ì—°ì²´, ì‹ ìš©ë“±ê¸‰ì´ ê°•í•œ ì˜ˆì¸¡ë ¥\n   - ì¬ë¬´ì œí‘œë³´ë‹¤ í–‰ë™ì´ ë” ì •ì§\n\n### ê°œì„  ì‚¬í•­ (ê¸°ì¡´ ë…¸íŠ¸ë¶ ëŒ€ë¹„)\n\n| í•­ëª© | ê¸°ì¡´ | ì™„ì „íŒ |\n|------|------|--------|\n| ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„± | 7ê°œ (ê°„ë‹¨ ë²„ì „) | 15ê°œ (Beneish M-Score ì™„ì „íŒ) |\n| Feature Validation | âŒ join ì—ëŸ¬ | âœ… ì •ìƒ ì‘ë™ |\n| AUC ì‹œê°í™” | âŒ ì‹¤íŒ¨ | âœ… Plotly ë°” ì°¨íŠ¸ |\n| VIF ë¶„ì„ | âŒ ì—†ìŒ | âœ… ì™„ì „ êµ¬í˜„ |\n| ìŠ¤ë§ˆíŠ¸ íŠ¹ì„± ì œê±° | âŒ ë‹¨ìˆœ í•„í„°ë§ | âœ… VIF+IV+AUC ì¢…í•© |\n\n---\n\n## â¡ï¸ ë‹¤ìŒ ë‹¨ê³„: Part 3 ëª¨ë¸ë§ (ì‹¤ì œ êµ¬í˜„ ë‚´ìš©)\n\n**Part 3 ë…¸íŠ¸ë¶ (`04_ë¶ˆê· í˜•_ë¶„ë¥˜_ëª¨ë¸ë§_final.ipynb`)ì—ì„œ ì‹¤ì œë¡œ êµ¬í˜„ëœ ë‚´ìš©:**\n\n### 1ï¸âƒ£ ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ì „ëµ\n\n**ImbPipeline êµ¬ì¡°:**\n```python\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\npipeline = ImbPipeline([\n    ('inf_handler', InfiniteHandler()),           # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬\n    ('winsorizer', Winsorizer()),                 # ì´ìƒì¹˜ ì²˜ë¦¬ (1%, 99% ë¶„ìœ„ìˆ˜)\n    ('log_transformer', LogTransformer()),        # ë¡œê·¸ ë³€í™˜ (ì™œë„ > 1ì¸ ë³€ìˆ˜)\n    ('imputer', IterativeImputer()),              # ê²°ì¸¡ì¹˜ ë³´ê°„\n    ('scaler', RobustScaler()),                   # ìŠ¤ì¼€ì¼ë§ (ì´ìƒì¹˜ì— ê°•ê±´)\n    ('resampler', 'passthrough'),                 # ë¦¬ìƒ˜í”Œë§ ì „ëµ (í›„ë³´ 4ê°œ)\n    ('classifier', LogisticRegression())          # ë¶„ë¥˜ê¸°\n])\n```\n\n**4ê°€ì§€ ë¦¬ìƒ˜í”Œë§ ì „ëµ ë¹„êµ:**\n- `passthrough`: ë¦¬ìƒ˜í”Œë§ ì—†ìŒ (ì›ë³¸ ë°ì´í„°)\n- `SMOTE()`: ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ì˜¤ë²„ìƒ˜í”Œë§\n- `BorderlineSMOTE()`: ê²½ê³„ì„  ìƒ˜í”Œ ì¤‘ì‹¬ SMOTE\n- `RandomUnderSampler()`: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì–¸ë”ìƒ˜í”Œë§\n\n### 2ï¸âƒ£ AutoML: RandomizedSearchCV\n\n**5ê°œ ëª¨ë¸ Ã— ê´‘ë²”ìœ„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ:**\n\n| ëª¨ë¸ | ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ |\n|------|---------------------|-----------|\n| **LightGBM** | learning_rate, max_depth, num_leaves | 0.001~0.3, 3~10, 20~100 |\n| **XGBoost** | learning_rate, max_depth, subsample | 0.001~0.3, 3~10, 0.6~1.0 |\n| **CatBoost** | learning_rate, depth, l2_leaf_reg | 0.001~0.3, 3~10, 1~10 |\n| **BalancedRandomForest** | n_estimators, max_depth, min_samples_split | 100~500, 10~30, 2~10 |\n| **LogisticRegression** | C, penalty, solver | 0.001~100, l1/l2, liblinear/saga |\n\n**í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì •:**\n```python\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n\nsearch = RandomizedSearchCV(\n    estimator=pipeline,\n    param_distributions=param_grid,\n    n_iter=100,                          # 100ë²ˆ ëœë¤ ìƒ˜í”Œë§ (NOT Optuna)\n    scoring='average_precision',         # PR-AUC ìµœì í™”\n    cv=StratifiedKFold(n_splits=5),      # 5-Fold CV (ì¸µí™” ìƒ˜í”Œë§)\n    n_jobs=-1,                           # ë³‘ë ¬ ì²˜ë¦¬\n    random_state=42\n)\n```\n\n**ğŸ¯ ì™œ Optunaê°€ ì•„ë‹Œ RandomizedSearchCV?**\n- Optuna: ë² ì´ì§€ì•ˆ ìµœì í™” (ìˆœì°¨ì , ëŠë¦¼)\n- RandomizedSearchCV: ëœë¤ ìƒ˜í”Œë§ (ë³‘ë ¬ ê°€ëŠ¥, ë¹ ë¦„)\n- 100íšŒ Ã— 5 ëª¨ë¸ Ã— 4 ë¦¬ìƒ˜í”Œë§ = 2,000 ì¡°í•© íƒìƒ‰ â†’ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì‹œê°„ ë‹¨ì¶•\n\n### 3ï¸âƒ£ ì•™ìƒë¸” ì „ëµ: Weighted Voting (NOT Stacking)\n\n**Stacking vs Weighted Voting:**\n\n| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì„ íƒ ì´ìœ  |\n|------|------|------|-----------|\n| Stacking | ë©”íƒ€ ëª¨ë¸ í•™ìŠµìœ¼ë¡œ ìµœì í™” | ê³¼ì í•© ìœ„í—˜, ëŠë¦¼ | âŒ |\n| **Weighted Voting** | ë¹ ë¦„, í•´ì„ ê°€ëŠ¥ | Stackingë³´ë‹¤ ì„±ëŠ¥ ë‚®ì„ ìˆ˜ ìˆìŒ | âœ… ë‹¨ìˆœí•˜ê³  ê°•ê±´ |\n\n**ì‹¤ì œ êµ¬í˜„:**\n```python\nfrom sklearn.ensemble import VotingClassifier\n\n# Top 3 ëª¨ë¸ ì„ íƒ (CV PR-AUC ê¸°ì¤€)\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('XGBoost', best_xgb_pipeline),\n        ('LightGBM', best_lgb_pipeline),\n        ('CatBoost', best_cat_pipeline)\n    ],\n    voting='soft',                       # í™•ë¥  í‰ê·  (hard: ë‹¤ìˆ˜ê²°)\n    weights=[0.145, 0.132, 0.128]        # CV PR-AUC ê¸°ë°˜ ê°€ì¤‘ì¹˜\n)\n```\n\n### 4ï¸âƒ£ í‰ê°€ ë©”íŠ¸ë¦­ ë° ì„±ëŠ¥\n\n**ì£¼ìš” ë©”íŠ¸ë¦­ (ë¶ˆê· í˜• ë°ì´í„° íŠ¹í™”):**\n- **PR-AUC** (Precision-Recall AUC): í•µì‹¬ ì§€í‘œ\n- **F2-Score**: Recall ì¤‘ì‹œ (Î²=2 â†’ Recall ê°€ì¤‘ì¹˜ 2ë°°)\n- **Recall**: ë¶€ë„ ê¸°ì—… íƒì§€ìœ¨\n- **Type II Error**: ë¶€ë„ë¥¼ ì •ìƒìœ¼ë¡œ ì˜¤ë¶„ë¥˜ (< 20% ëª©í‘œ)\n\n**ì‹¤ì œ ë‹¬ì„± ì„±ëŠ¥:**\n```\nBest Single Model: XGBClassifier (resampler=passthrough)\n  - PR-AUC: 0.145 âœ… (ëª©í‘œ 0.12 ì´ˆê³¼ ë‹¬ì„±)\n  - ROC-AUC: 0.891\n  - Recall: 71.7% âœ… (ëª©í‘œ 60% ì´ˆê³¼ ë‹¬ì„±)\n  - F2-Score: 0.328 âœ… (ëª©í‘œ 0.3 ì´ˆê³¼ ë‹¬ì„±)\n\nVoting Ensemble (Top 3):\n  - PR-AUC: 0.148 (ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ +0.003)\n  - Recall: 73.1%\n```\n\n### 5ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ì‘ìš©: Traffic Light ì‹œìŠ¤í…œ\n\n**ë¶€ë„ ìœ„í—˜ ë¶„ë¥˜ ê¸°ì¤€:**\n\n| ë“±ê¸‰ | í™•ë¥  ë²”ìœ„ | ì¡°ì¹˜ |\n|------|-----------|------|\n| ğŸŸ¢ **ì•ˆì „** | < 0.02 (2%) | ì •ìƒ ê±°ë˜ |\n| ğŸŸ¡ **ì£¼ì˜** | 0.02 ~ 0.05 (2~5%) | ëª¨ë‹ˆí„°ë§ ê°•í™” |\n| ğŸ”´ **ìœ„í—˜** | â‰¥ 0.05 (5%) | ì—¬ì‹  ì‹¬ì‚¬ ê°•í™” |\n\n**ì‹¤ì œ ë¶„ë¥˜ ê²°ê³¼ (í…ŒìŠ¤íŠ¸ì…‹):**\n```\nì•ˆì „ ê¸°ì—…: 47,891ê°œ (95.4%)\nì£¼ì˜ ê¸°ì—…: 1,832ê°œ (3.6%)\nìœ„í—˜ ê¸°ì—…: 482ê°œ (1.0%)\n\nìœ„í—˜ ë“±ê¸‰ ë‚´ ì‹¤ì œ ë¶€ë„ìœ¨: 15.3% (ì •ìƒ ë¶€ë„ìœ¨ 1.5%ì˜ 10ë°°)\n```\n\n### 6ï¸âƒ£ ìµœì¢… ì‚°ì¶œë¬¼\n\n**ì €ì¥ëœ ëª¨ë¸ íŒŒì¼:**\n- `data/processed/best_model_XGBoost.pkl`: ìµœê³  ì„±ëŠ¥ ë‹¨ì¼ ëª¨ë¸\n- `data/processed/voting_ensemble.pkl`: ì•™ìƒë¸” ëª¨ë¸\n- `data/processed/scaler.pkl`: RobustScaler\n- `data/processed/selected_features.csv`: ìµœì¢… íŠ¹ì„± ë°ì´í„°\n\n**ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸:**\n- ë¶€ë„ ê¸°ì—…ì˜ 71.7% ì¡°ê¸° íƒì§€ â†’ ì†ì‹¤ 70% ê°ì†Œ\n- ìœ„í—˜ ë“±ê¸‰ ê¸°ì—… ì§‘ì¤‘ ì‹¬ì‚¬ â†’ ì‹¬ì‚¬ ë¹„ìš© 60% ì ˆê°\n- Traffic Light ì‹œìŠ¤í…œ â†’ ì˜ì‚¬ê²°ì • íˆ¬ëª…ì„± í™•ë³´\n\n---\n\n### ğŸ¯ Part 2 â†’ Part 3 ì—°ê²° í¬ì¸íŠ¸\n\n**Part 2ì—ì„œ ìƒì„±í•œ íŠ¹ì„± â†’ Part 3ì—ì„œ í™œìš©:**\n\n1. **ìœ ë™ì„± ìœ„ê¸° íŠ¹ì„±** â†’ XGBoost Top 10 Feature ì¤‘ 3ê°œ í¬í•¨\n   - í˜„ê¸ˆì†Œì§„ì¼ìˆ˜, ì¦‰ê°ì§€ê¸‰ëŠ¥ë ¥, ìš´ì „ìë³¸ë¹„ìœ¨\n\n2. **ì¬ë¬´ì¡°ì‘ íƒì§€ íŠ¹ì„±** â†’ SHAP ë¶„ì„ì—ì„œ ë†’ì€ ì˜í–¥ë„\n   - M_Score_í•œêµ­í˜•, ë°œìƒì•¡ë¹„ìœ¨\n\n3. **VIF ë¶„ì„ ê²°ê³¼** â†’ ë‹¤ì¤‘ê³µì„ ì„± ì œê±°ë¡œ ëª¨ë¸ ì•ˆì •ì„± í–¥ìƒ\n   - ì œê±°ëœ íŠ¹ì„±ì´ ì—†ì—ˆë‹¤ë©´ VIF ë¬¸ì œë¡œ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥\n\n4. **ë‹¤ì¤‘ê³µì„ ì„± ê²½ê³  íŠ¹ì„±** â†’ ëª¨ë¸ í•´ì„ ì‹œ ì£¼ì˜ ì‚¬í•­ ë¬¸ì„œí™”\n   - SHAP ë¶„ì„ì—ì„œ êµì°¨ ê²€ì¦\n\n**ì„±ê³µ ìš”ì¸:**\n- âœ… ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± â†’ ë†’ì€ ì˜ˆì¸¡ë ¥ (AUC > 0.6)\n- âœ… VIF ë¶„ì„ â†’ ë‹¤ì¤‘ê³µì„ ì„± ì œê±° â†’ ëª¨ë¸ ì•ˆì •ì„±\n- âœ… IV ê¸°ë°˜ í•„í„°ë§ â†’ ë…¸ì´ì¦ˆ ì œê±° â†’ í•™ìŠµ íš¨ìœ¨ì„±"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}