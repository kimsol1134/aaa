{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“— ë°œí‘œìš© Part 3: ëª¨ë¸ë§ ë° ìµœì í™”\n",
    "\n",
    "**í”„ë¡œì íŠ¸**: í•œêµ­ ê¸°ì—… ë¶€ë„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ  \n",
    "**ì‘ì„±ì¼**: 2025ë…„  \n",
    "**ëª©í‘œ**: ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„±ì„ í™œìš©í•œ ë¶ˆê· í˜• ë¶„ë¥˜ ëª¨ë¸ë§ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš©\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Part 3 ëª©í‘œ ë° ì´ì „ Part ìš”ì•½\n",
    "\n",
    "### Part 1-2 ì£¼ìš” ë°œê²¬\n",
    "\n",
    "**Part 1: ë¬¸ì œ ì •ì˜ ë° íƒìƒ‰ì  ë¶„ì„**\n",
    "- ìœ ë™ì„±ì´ ê°€ì¥ ê°•ë ¥í•œ ì˜ˆì¸¡ ë³€ìˆ˜ (ìœ ë™ë¹„ìœ¨, ë‹¹ì¢Œë¹„ìœ¨, í˜„ê¸ˆë¹„ìœ¨)\n",
    "- ì—…ì¢…ë³„ ë¶€ë„ìœ¨ 2ë°° ì°¨ì´ (ê±´ì„¤ì—… 2.8% vs ê¸ˆìœµì—… 0.9%)\n",
    "- ì™¸ê° ì—¬ë¶€ê°€ ë¶€ë„ìœ¨ì— ì˜í–¥\n",
    "\n",
    "**Part 2: ë„ë©”ì¸ íŠ¹ì„± ê³µí•™**\n",
    "- **ìƒì„±ëœ íŠ¹ì„±**: 52ê°œ â†’ **ìµœì¢… ì„ íƒ**: 27ê°œ (ë‹¤ì¤‘ê³µì„ ì„± ì œê±° + ì˜ˆì¸¡ë ¥ ê²€ì¦)\n",
    "- **Feature Validation**: Mann-Whitney U test, Cliff's Delta, AUC ê¸°ë°˜ ê²€ì¦\n",
    "- **VIF ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„**: ê³ VIF íŠ¹ì„± 14ê°œ ì œê±°\n",
    "- **IV ê¸°ë°˜ ì„ íƒ**: Information Value > 0.02 ê¸°ì¤€\n",
    "- **ì¶œë ¥ íŒŒì¼**: `domain_based_features_ì™„ì „íŒ.csv` (50,105 rows Ã— 27 features)\n",
    "\n",
    "### Part 3 ëª©í‘œ\n",
    "\n",
    "1. âœ… **ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬**: 5ê°€ì§€ ë¦¬ìƒ˜í”Œë§ ì „ëµ (SMOTE, BorderlineSMOTE, RandomUnderSampler, SMOTETomek)\n",
    "2. âœ… **AutoML ìµœì í™”**: RandomizedSearchCVë¡œ 5ê°œ ëª¨ë¸ Ã— 100íšŒ ëœë¤ ìƒ˜í”Œë§\n",
    "3. âœ… **ì•™ìƒë¸” ëª¨ë¸**: Weighted Voting (Top 3 ëª¨ë¸ ê¸°ë°˜)\n",
    "4. âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš©**: Traffic Light ì‹œìŠ¤í…œ (ë™ì  ì„ê³„ê°’ ê¸°ë°˜ 3ë“±ê¸‰ ë¶„ë¥˜)\n",
    "5. âœ… **ëª¨ë¸ ì €ì¥**: Part 4 SHAP ë¶„ì„ì„ ìœ„í•œ íŒŒì¼ ì¶œë ¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ë° í•œê¸€ í°íŠ¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (OSë³„ ìë™ ì„ íƒ)\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "else:  # Linux\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "\n",
    "plt.rc('axes', unicode_minus=False)  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n",
    "\n",
    "print(\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ:\", plt.rcParams['font.family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Gradient Boosting ëª¨ë¸\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")\n",
    "print(f\"Scikit-learn version: {__import__('sklearn').__version__}\")\n",
    "print(f\"Imbalanced-learn version: {__import__('imblearn').__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"CatBoost version: {__import__('catboost').__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë”© ë° ë¶„í• \n",
    "\n",
    "Part 2ì—ì„œ ìƒì„±í•œ ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„±ì„ ë¡œë”©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "DATA_PATH = '../data/features/domain_based_features_ì™„ì „íŒ.csv'\n",
    "SAVE_DIR = '../data/processed/'\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "print(\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "df = pd.read_csv(DATA_PATH, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nâœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ\")\n",
    "print(f\"   - Shape: {df.shape}\")\n",
    "print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íƒ€ê²Ÿ ë³€ìˆ˜ëª… ì„¤ì •\n",
    "TARGET_COL = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'\n",
    "\n",
    "# ë°ì´í„° ì •ë³´ í™•ì¸\n",
    "print(\"ğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì „ì²´ ê¸°ì—… ìˆ˜: {len(df):,}\")\n",
    "print(f\"íŠ¹ì„± ê°œìˆ˜: {df.shape[1] - 1}\")\n",
    "print(f\"\\në¶€ë„ìœ¨:\")\n",
    "print(df[TARGET_COL].value_counts())\n",
    "print(f\"\\në¶€ë„ìœ¨ ë¹„ìœ¨:\")\n",
    "print(df[TARGET_COL].value_counts(normalize=True) * 100)\n",
    "\n",
    "# ë¶ˆê· í˜• ë¹„ìœ¨\n",
    "imbalance_ratio = (df[TARGET_COL] == 0).sum() / (df[TARGET_COL] == 1).sum()\n",
    "print(f\"\\nâš ï¸  ë¶ˆê· í˜• ë¹„ìœ¨: 1:{imbalance_ratio:.1f} (ì •ìƒ:ë¶€ë„)\")\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"\\nê²°ì¸¡ì¹˜ ì´ ê°œìˆ˜: {missing_count:,}\")\n",
    "if missing_count > 0:\n",
    "    print(f\"ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜: {(df.isnull().sum() > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (80:20)\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# X, y ë¶„ë¦¬\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# Stratified split (ë¶€ë„ìœ¨ ìœ ì§€)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    stratify=y, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Train/Test Split ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train Set: {X_train.shape} (ë¶€ë„ìœ¨: {y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Test Set:  {X_test.shape} (ë¶€ë„ìœ¨: {y_test.mean()*100:.2f}%)\")\n",
    "print(f\"\\níŠ¹ì„± ëª©ë¡ (ì´ {X_train.shape[1]}ê°œ):\")\n",
    "for i, col in enumerate(X_train.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜\n",
    "\n",
    "ImbPipelineì—ì„œ ì‚¬ìš©í•  ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"ë¬´í•œëŒ€ ê°’(inf, -inf)ì„ NaNìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = X_copy.replace([np.inf, -np.inf], np.nan)\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"ì´ìƒì¹˜ë¥¼ íŠ¹ì • ë°±ë¶„ìœ„ìˆ˜ë¡œ í´ë¦¬í•‘ (Winsorizing)\"\"\"\n",
    "    \n",
    "    def __init__(self, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "        self.lower_bounds_ = None\n",
    "        self.upper_bounds_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        self.lower_bounds_ = X_df.quantile(self.lower_quantile)\n",
    "        self.upper_bounds_ = X_df.quantile(self.upper_quantile)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X.copy()\n",
    "        X_clipped = X_df.clip(lower=self.lower_bounds_, upper=self.upper_bounds_, axis=1)\n",
    "        return X_clipped.values if not isinstance(X, pd.DataFrame) else X_clipped\n",
    "\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"ì–‘ìˆ˜ ê°’ì—ë§Œ ë¡œê·¸ ë³€í™˜ ì ìš© (log1p)\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0):\n",
    "        self.threshold = threshold\n",
    "        self.positive_cols_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        # ì–‘ìˆ˜ë§Œ ìˆëŠ” ì»¬ëŸ¼ ì°¾ê¸°\n",
    "        self.positive_cols_ = [col for col in X_df.columns if (X_df[col] > self.threshold).all()]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X.copy()\n",
    "        for col in self.positive_cols_:\n",
    "            if col in X_df.columns:\n",
    "                X_df[col] = np.log1p(X_df[col])\n",
    "        return X_df.values if not isinstance(X, pd.DataFrame) else X_df\n",
    "\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   - InfiniteHandler: ë¬´í•œëŒ€ ê°’ â†’ NaN\")\n",
    "print(\"   - Winsorizer: ì´ìƒì¹˜ í´ë¦¬í•‘ (1%~99% ë°±ë¶„ìœ„ìˆ˜)\")\n",
    "print(\"   - LogTransformer: ì–‘ìˆ˜ ì»¬ëŸ¼ì— log1p ë³€í™˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ì „ëµ\n",
    "\n",
    "### ImbPipeline 6ë‹¨ê³„ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "1. InfiniteHandler    â†’ ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬\n",
    "2. Winsorizer         â†’ ì´ìƒì¹˜ ì œì–´ (1%~99% ë¶„ìœ„ìˆ˜)\n",
    "3. LogTransformer     â†’ ë¡œê·¸ ë³€í™˜ (ì–‘ìˆ˜ë§Œ)\n",
    "4. IterativeImputer   â†’ ê²°ì¸¡ì¹˜ ë³´ê°„\n",
    "5. RobustScaler       â†’ ìŠ¤ì¼€ì¼ë§ (ì¤‘ì•™ê°’ ê¸°ë°˜)\n",
    "6. Resampler          â†’ ë¦¬ìƒ˜í”Œë§ (5ê°€ì§€ ì „ëµ)\n",
    "7. Classifier         â†’ ë¶„ë¥˜ê¸° (5ê°œ ëª¨ë¸)\n",
    "```\n",
    "\n",
    "### 5ê°€ì§€ ë¦¬ìƒ˜í”Œë§ ì „ëµ\n",
    "\n",
    "| ì „ëµ | ì„¤ëª… | Sampling Ratio |\n",
    "|------|------|----------------|\n",
    "| **passthrough** | ë¦¬ìƒ˜í”Œë§ ì—†ìŒ (ë² ì´ìŠ¤ë¼ì¸) | - |\n",
    "| **SMOTE** | ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ìƒ˜í”Œ ìƒì„± | 0.2 (20%) |\n",
    "| **BorderlineSMOTE** | ê²½ê³„ì„  ìƒ˜í”Œ ì¤‘ì‹¬ ìƒì„± | 0.2 (20%) |\n",
    "| **RandomUnderSampler** | ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë¬´ì‘ìœ„ ì œê±° | 0.3 (30%) |\n",
    "| **SMOTETomek** | SMOTE + Tomek Links ì •ë¦¬ | 0.2 (20%) |\n",
    "\n",
    "**Sampling Ratio ê·¼ê±°**:\n",
    "- ì›ë³¸ ë¶€ë„ìœ¨: 1.51% (1:66)\n",
    "- ëª©í‘œ ë¶€ë„ìœ¨: 20% (1:5) â†’ ê³¼ìƒì„± ë°©ì§€í•˜ë©´ì„œë„ í•™ìŠµ íš¨ê³¼ ë³´ì¥\n",
    "- ì–¸ë”ìƒ˜í”Œë§: 30%ê¹Œì§€ë§Œ â†’ ì •ìƒ ê¸°ì—… ë°ì´í„° ì†ì‹¤ ìµœì†Œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5ê°€ì§€ ë¦¬ìƒ˜í”Œë§ ì „ëµ ì •ì˜\n",
    "SAMPLING_RATIO_OVER = 0.2   # SMOTE ê³„ì—´: 20%\n",
    "SAMPLING_RATIO_UNDER = 0.3  # ì–¸ë”ìƒ˜í”Œë§: 30%\n",
    "\n",
    "resampler_list = [\n",
    "    'passthrough',  # 1. ë¦¬ìƒ˜í”Œë§ ì—†ìŒ\n",
    "    SMOTE(sampling_strategy=SAMPLING_RATIO_OVER, random_state=RANDOM_STATE),  # 2. SMOTE\n",
    "    BorderlineSMOTE(sampling_strategy=SAMPLING_RATIO_OVER, random_state=RANDOM_STATE),  # 3. BorderlineSMOTE\n",
    "    RandomUnderSampler(sampling_strategy=SAMPLING_RATIO_UNDER, random_state=RANDOM_STATE),  # 4. ì–¸ë”ìƒ˜í”Œë§\n",
    "    SMOTETomek(sampling_strategy=SAMPLING_RATIO_OVER, random_state=RANDOM_STATE)  # 5. SMOTETomek\n",
    "]\n",
    "\n",
    "print(\"âœ… ë¦¬ìƒ˜í”Œë§ ì „ëµ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "for i, resampler in enumerate(resampler_list, 1):\n",
    "    if resampler == 'passthrough':\n",
    "        print(f\"{i}. passthrough (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)\")\n",
    "    else:\n",
    "        print(f\"{i}. {type(resampler).__name__} (ratio={resampler.sampling_strategy})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ì´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì •ì˜ (ë¦¬ìƒ˜í”Œë§ + ë¶„ë¥˜ê¸°ëŠ” AutoMLì—ì„œ ì„ íƒ)\n",
    "def create_pipeline():\n",
    "    \"\"\"ImbPipeline ìƒì„± (ì „ì²˜ë¦¬ 6ë‹¨ê³„ + ë¦¬ìƒ˜í”Œë§ + ë¶„ë¥˜ê¸°)\"\"\"\n",
    "    pipeline = ImbPipeline([\n",
    "        ('inf_handler', InfiniteHandler()),\n",
    "        ('winsorizer', Winsorizer(lower_quantile=0.01, upper_quantile=0.99)),\n",
    "        ('log_transformer', LogTransformer()),\n",
    "        ('imputer', IterativeImputer(max_iter=10, random_state=RANDOM_STATE)),\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('resampler', 'passthrough'),  # AutoMLì—ì„œ ì„ íƒ\n",
    "        ('classifier', LogisticRegression(random_state=RANDOM_STATE))  # AutoMLì—ì„œ ì„ íƒ\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\n",
    "test_pipeline = create_pipeline()\n",
    "print(\"\\nâœ… ImbPipeline ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"\\níŒŒì´í”„ë¼ì¸ ë‹¨ê³„:\")\n",
    "for step_name, step_obj in test_pipeline.steps:\n",
    "    print(f\"  - {step_name}: {type(step_obj).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AutoML: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "\n",
    "### RandomizedSearchCV ì„¤ì •\n",
    "\n",
    "- **íƒìƒ‰ ê³µê°„**: 5ê°œ ëª¨ë¸ Ã— 5ê°œ ë¦¬ìƒ˜í”Œë§ ì „ëµ Ã— ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "- **ìƒ˜í”Œë§ íšŸìˆ˜**: 100íšŒ ëœë¤ ìƒ˜í”Œë§\n",
    "- **êµì°¨ ê²€ì¦**: StratifiedKFold 5-Fold\n",
    "- **í‰ê°€ ë©”íŠ¸ë¦­**: PR-AUC (average_precision)\n",
    "- **ë³‘ë ¬ ì²˜ë¦¬**: n_jobs=-1 (ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©)\n",
    "\n",
    "### 5ê°œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° (scale_pos_weight ìš©)\n",
    "scale_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "sqrt_ratio = int(np.sqrt(scale_ratio))\n",
    "\n",
    "print(f\"ë¶ˆê· í˜• ë¹„ìœ¨: {scale_ratio:.1f}\")\n",
    "print(f\"sqrt_ratio: {sqrt_ratio}\")\n",
    "print(f\"scale_ratio: {int(scale_ratio)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜\n",
    "param_distributions = [\n",
    "    # 1. LightGBM (ìë™ ë¶ˆê· í˜• ì²˜ë¦¬)\n",
    "    {\n",
    "        'resampler': resampler_list,\n",
    "        'classifier': [lgb.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1)],\n",
    "        'classifier__n_estimators': [300, 500, 1000],\n",
    "        'classifier__learning_rate': [0.01, 0.02, 0.05],\n",
    "        'classifier__num_leaves': [31, 63, 127],\n",
    "        'classifier__max_depth': [-1, 10, 20],\n",
    "        'classifier__subsample': [0.7, 0.9],\n",
    "        'classifier__reg_alpha': [0.1, 0.5],\n",
    "        'classifier__reg_lambda': [0.1, 0.5],\n",
    "        'classifier__is_unbalance': [True]\n",
    "    },\n",
    "    \n",
    "    # 2. XGBoost\n",
    "    {\n",
    "        'resampler': resampler_list,\n",
    "        'classifier': [xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss')],\n",
    "        'classifier__n_estimators': [300, 500],\n",
    "        'classifier__max_depth': [4, 6, 8],\n",
    "        'classifier__learning_rate': [0.01, 0.05],\n",
    "        'classifier__gamma': [0, 0.1, 0.5],\n",
    "        'classifier__subsample': [0.7, 0.9],\n",
    "        'classifier__reg_alpha': [0.1, 1.0],\n",
    "        'classifier__scale_pos_weight': [1, sqrt_ratio, int(scale_ratio)]\n",
    "    },\n",
    "    \n",
    "    # 3. CatBoost\n",
    "    {\n",
    "        'resampler': resampler_list,\n",
    "        'classifier': [CatBoostClassifier(random_state=RANDOM_STATE, verbose=0)],\n",
    "        'classifier__iterations': [500, 1000],\n",
    "        'classifier__learning_rate': [0.01, 0.03, 0.1],\n",
    "        'classifier__depth': [4, 6, 8],\n",
    "        'classifier__l2_leaf_reg': [3, 5, 9],\n",
    "        'classifier__auto_class_weights': ['Balanced', 'SqrtBalanced']\n",
    "    },\n",
    "    \n",
    "    # 4. BalancedRandomForest\n",
    "    {\n",
    "        'resampler': resampler_list,\n",
    "        'classifier': [BalancedRandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)],\n",
    "        'classifier__n_estimators': [300, 500],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "        'classifier__max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    \n",
    "    # 5. LogisticRegression (ë² ì´ìŠ¤ë¼ì¸)\n",
    "    {\n",
    "        'resampler': resampler_list,\n",
    "        'classifier': [LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)],\n",
    "        'classifier__C': [0.1, 1.0, 10.0],\n",
    "        'classifier__class_weight': ['balanced', None]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"   - ëª¨ë¸ ìˆ˜: 5ê°œ\")\n",
    "print(f\"   - ë¦¬ìƒ˜í”Œë§ ì „ëµ: {len(resampler_list)}ê°œ\")\n",
    "print(f\"   - ì´ íƒìƒ‰ ê³µê°„: ë§¤ìš° í¼ (ëœë¤ ìƒ˜í”Œë§ í•„ìˆ˜)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV ì„¤ì •\n",
    "N_ITER = 100  # 100íšŒ ëœë¤ ìƒ˜í”Œë§\n",
    "CV_FOLDS = 5\n",
    "\n",
    "pipeline = create_pipeline()\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=N_ITER,\n",
    "    scoring='average_precision',  # PR-AUC ìµœì í™”\n",
    "    cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… RandomizedSearchCV ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ìƒ˜í”Œë§ íšŸìˆ˜: {N_ITER}íšŒ\")\n",
    "print(f\"êµì°¨ ê²€ì¦: {CV_FOLDS}-Fold StratifiedKFold\")\n",
    "print(f\"í‰ê°€ ë©”íŠ¸ë¦­: PR-AUC (average_precision)\")\n",
    "print(f\"ë³‘ë ¬ ì²˜ë¦¬: ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©\")\n",
    "print(\"\\nâ³ AutoML ì‹œì‘ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 20~40ë¶„)...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML ì‹¤í–‰\n",
    "start_time = datetime.now()\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(f\"\\nâœ… AutoML ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ë¶„)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "best_model = search.best_estimator_\n",
    "best_params = search.best_params_\n",
    "best_score = search.best_score_\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ† ìµœì  ëª¨ë¸ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\në¶„ë¥˜ê¸°: {type(best_model.named_steps['classifier']).__name__}\")\n",
    "print(f\"ë¦¬ìƒ˜í”Œë§ ì „ëµ: {type(best_model.named_steps['resampler']).__name__ if best_model.named_steps['resampler'] != 'passthrough' else 'passthrough'}\")\n",
    "print(f\"CV PR-AUC: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
    "for param, value in best_params.items():\n",
    "    if not param.startswith('classifier') or param == 'classifier':\n",
    "        continue\n",
    "    param_name = param.replace('classifier__', '')\n",
    "    print(f\"  - {param_name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìœ„ 10ê°œ ëª¨ë¸ ê²°ê³¼ í™•ì¸\n",
    "results_df = pd.DataFrame(search.cv_results_)\n",
    "\n",
    "# ì£¼ìš” ì»¬ëŸ¼ ì„ íƒ\n",
    "results_summary = results_df[[\n",
    "    'mean_test_score', 'std_test_score', 'rank_test_score',\n",
    "    'param_classifier', 'param_resampler'\n",
    "]].copy()\n",
    "\n",
    "# ë¶„ë¥˜ê¸° ì´ë¦„ ì¶”ì¶œ\n",
    "results_summary['classifier_name'] = results_summary['param_classifier'].apply(\n",
    "    lambda x: type(x).__name__ if hasattr(x, '__name__') else str(x)\n",
    ")\n",
    "results_summary['resampler_name'] = results_summary['param_resampler'].apply(\n",
    "    lambda x: type(x).__name__ if x != 'passthrough' else 'passthrough'\n",
    ")\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ì •ë ¬\n",
    "top10 = results_summary.nsmallest(10, 'rank_test_score')[[\n",
    "    'rank_test_score', 'mean_test_score', 'std_test_score', \n",
    "    'classifier_name', 'resampler_name'\n",
    "]]\n",
    "\n",
    "print(\"\\nğŸ“Š ìƒìœ„ 10ê°œ ëª¨ë¸ (PR-AUC ê¸°ì¤€)\")\n",
    "print(\"=\" * 80)\n",
    "print(top10.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weighted Voting ì•™ìƒë¸”\n",
    "\n",
    "ìƒìœ„ 3ê°œ ëª¨ë¸ì„ Soft Votingìœ¼ë¡œ ê²°í•©í•˜ì—¬ ì•™ìƒë¸” ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.  \n",
    "ê°€ì¤‘ì¹˜ëŠ” ê° ëª¨ë¸ì˜ CV PR-AUC ì ìˆ˜ì— ë¹„ë¡€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìœ„ 3ê°œ ëª¨ë¸ ì¶”ì¶œ\n",
    "top3_indices = results_df.nsmallest(3, 'rank_test_score').index\n",
    "\n",
    "top3_models = []\n",
    "top3_scores = []\n",
    "top3_names = []\n",
    "\n",
    "for i, idx in enumerate(top3_indices, 1):\n",
    "    # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ íŒŒë¼ë¯¸í„°ë¡œ íŒŒì´í”„ë¼ì¸ ì¬êµ¬ì„±\n",
    "    params = results_df.loc[idx, 'params']\n",
    "    pipeline_i = create_pipeline()\n",
    "    pipeline_i.set_params(**params)\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    pipeline_i.fit(X_train, y_train)\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ ì €ì¥\n",
    "    classifier_name = type(pipeline_i.named_steps['classifier']).__name__\n",
    "    resampler_name = type(pipeline_i.named_steps['resampler']).__name__ if pipeline_i.named_steps['resampler'] != 'passthrough' else 'passthrough'\n",
    "    score = results_df.loc[idx, 'mean_test_score']\n",
    "    \n",
    "    model_name = f\"Top{i}_{classifier_name}_{resampler_name}\"\n",
    "    \n",
    "    top3_models.append((model_name, pipeline_i))\n",
    "    top3_scores.append(score)\n",
    "    top3_names.append(classifier_name)\n",
    "    \n",
    "    print(f\"Top {i}: {classifier_name} + {resampler_name} (CV PR-AUC: {score:.4f})\")\n",
    "\n",
    "print(\"\\nâœ… ìƒìœ„ 3ê°œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Voting Classifier ìƒì„±\n",
    "# ê°€ì¤‘ì¹˜ = CV PR-AUC ì ìˆ˜ì— ë¹„ë¡€\n",
    "weights = top3_scores  # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=top3_models,\n",
    "    voting='soft',  # í™•ë¥  í‰ê· \n",
    "    weights=weights,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nâ³ Weighted Voting Ensemble í•™ìŠµ ì¤‘...\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… Weighted Voting Ensemble í•™ìŠµ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ì•™ìƒë¸” êµ¬ì„±:\")\n",
    "for i, (name, model) in enumerate(top3_models, 1):\n",
    "    print(f\"  {i}. {name} (ê°€ì¤‘ì¹˜: {weights[i-1]:.4f})\")\n",
    "print(f\"\\nì´ ê°€ì¤‘ì¹˜ í•©: {sum(weights):.4f}\")\n",
    "print(f\"ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜: {[w/sum(weights) for w in weights]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ìµœì¢… ëª¨ë¸ ì„ ì •\n",
    "\n",
    "Single Best Modelê³¼ Weighted Voting Ensembleì„ Test Setì—ì„œ ë¹„êµí•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ì„ ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì„ ì • ê¸°ì¤€**:\n",
    "- PR-AUCê°€ ë†’ì€ ìª½ ì„ íƒ\n",
    "- ë‹¨, ì•™ìƒë¸”ì´ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ 0.5% ì´ìƒ ìš°ìˆ˜í•´ì•¼ ì„ íƒ (ë³µì¡ë„ ê³ ë ¤)\n",
    "- ìœ ì§€ë³´ìˆ˜ ìš©ì´ì„±ë„ ê³ ë ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\"\"\"\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'PR-AUC': average_precision_score(y_test, y_prob),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_prob),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Single Best Model í‰ê°€\n",
    "single_best_metrics = evaluate_model(best_model, X_test, y_test, \"Single Best Model\")\n",
    "\n",
    "# Weighted Voting Ensemble í‰ê°€\n",
    "voting_metrics = evaluate_model(voting_clf, X_test, y_test, \"Weighted Voting Ensemble\")\n",
    "\n",
    "# ê²°ê³¼ ë¹„êµ\n",
    "comparison_df = pd.DataFrame([single_best_metrics, voting_metrics])\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "print(\"\\nğŸ“Š Test Set ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string())\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ì„ ì • ë¡œì§\n",
    "ENSEMBLE_THRESHOLD = 0.005  # 0.5% ì°¨ì´ ì„ê³„ê°’\n",
    "\n",
    "pr_auc_diff = voting_metrics['PR-AUC'] - single_best_metrics['PR-AUC']\n",
    "\n",
    "if pr_auc_diff > ENSEMBLE_THRESHOLD:\n",
    "    final_model = voting_clf\n",
    "    final_model_name = \"Weighted Voting Ensemble\"\n",
    "    decision_reason = f\"ì•™ìƒë¸”ì´ {pr_auc_diff:.4f} ({pr_auc_diff*100:.2f}%) ë” ìš°ìˆ˜\"\n",
    "elif pr_auc_diff < -ENSEMBLE_THRESHOLD:\n",
    "    final_model = best_model\n",
    "    final_model_name = \"Single Best Model\"\n",
    "    decision_reason = f\"ë‹¨ì¼ ëª¨ë¸ì´ {-pr_auc_diff:.4f} ({-pr_auc_diff*100:.2f}%) ë” ìš°ìˆ˜\"\n",
    "else:\n",
    "    # ì°¨ì´ê°€ ë¯¸ë¯¸í•˜ë©´ ë‹¨ìˆœí•œ ëª¨ë¸ ì„ íƒ (ìœ ì§€ë³´ìˆ˜ ìš©ì´ì„±)\n",
    "    final_model = best_model\n",
    "    final_model_name = \"Single Best Model\"\n",
    "    decision_reason = f\"ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸ ({abs(pr_auc_diff)*100:.2f}%) â†’ ë‹¨ìˆœ ëª¨ë¸ ì„ íƒ\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ† ìµœì¢… ëª¨ë¸ ì„ ì • ê²°ê³¼\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì„ ì • ëª¨ë¸: {final_model_name}\")\n",
    "print(f\"ì„ ì • ì´ìœ : {decision_reason}\")\n",
    "print(f\"\\nTest Set PR-AUC: {comparison_df.loc[final_model_name, 'PR-AUC']:.4f}\")\n",
    "print(f\"Test Set ROC-AUC: {comparison_df.loc[final_model_name, 'ROC-AUC']:.4f}\")\n",
    "print(f\"Test Set F1-Score: {comparison_df.loc[final_model_name, 'F1-Score']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "ìµœì¢… ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ì˜ˆì¸¡\n",
    "y_prob_test = final_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "# ìƒì„¸ í‰ê°€ ë©”íŠ¸ë¦­\n",
    "pr_auc = average_precision_score(y_test, y_prob_test)\n",
    "roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PR-AUC (í•µì‹¬ ë©”íŠ¸ë¦­):  {pr_auc:.4f}\")\n",
    "print(f\"ROC-AUC (ì°¸ê³ ):        {roc_auc:.4f}\")\n",
    "print(f\"F1-Score:              {f1:.4f}\")\n",
    "print(f\"Precision (ì •ë°€ë„):    {precision:.4f}\")\n",
    "print(f\"Recall (ì¬í˜„ìœ¨):       {recall:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\ní˜¼ë™ í–‰ë ¬ (Confusion Matrix):\")\n",
    "print(f\"  TN (ì •ìƒâ†’ì •ìƒ): {tn:,}\")\n",
    "print(f\"  FP (ì •ìƒâ†’ë¶€ë„):  {fp:,}\")\n",
    "print(f\"  FN (ë¶€ë„â†’ì •ìƒ):  {fn:,}  âš ï¸  Type II Error\")\n",
    "print(f\"  TP (ë¶€ë„â†’ë¶€ë„):  {tp:,}\")\n",
    "\n",
    "type2_error = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "print(f\"\\nType II Error (ë¶€ë„ ë¯¸íƒì§€ìœ¨): {type2_error:.2%}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred_test, target_names=['ì •ìƒ', 'ë¶€ë„'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 1: PR-AUC Curve\n",
    "precisions, recalls, thresholds_pr = precision_recall_curve(y_test, y_prob_test)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# PR Curve\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=recalls, y=precisions,\n",
    "    mode='lines',\n",
    "    name=f'PR Curve (AUC={pr_auc:.4f})',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# ë² ì´ìŠ¤ë¼ì¸ (ë¶€ë„ìœ¨)\n",
    "baseline = y_test.mean()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[baseline, baseline],\n",
    "    mode='lines',\n",
    "    name=f'Baseline (ë¶€ë„ìœ¨={baseline:.2%})',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Precision-Recall Curve (PR-AUC: {pr_auc:.4f})',\n",
    "    xaxis_title='Recall (ì¬í˜„ìœ¨)',\n",
    "    yaxis_title='Precision (ì •ë°€ë„)',\n",
    "    width=800, height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 2: Confusion Matrix\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=[[tn, fp], [fn, tp]],\n",
    "    x=['ì˜ˆì¸¡: ì •ìƒ', 'ì˜ˆì¸¡: ë¶€ë„'],\n",
    "    y=['ì‹¤ì œ: ì •ìƒ', 'ì‹¤ì œ: ë¶€ë„'],\n",
    "    text=[[f'TN<br>{tn:,}', f'FP<br>{fp:,}'], \n",
    "          [f'FN<br>{fn:,}<br>âš ï¸ Type II', f'TP<br>{tp:,}']],\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 14},\n",
    "    colorscale='Blues',\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Confusion Matrix (ê¸°ë³¸ ì„ê³„ê°’ 0.5)',\n",
    "    width=600, height=500,\n",
    "    xaxis=dict(side='bottom'),\n",
    "    yaxis=dict(side='left')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 3: ì˜ˆì¸¡ í™•ë¥  ë¶„í¬\n",
    "fig = go.Figure()\n",
    "\n",
    "# ì •ìƒ ê¸°ì—…\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=y_prob_test[y_test == 0],\n",
    "    name='ì •ìƒ ê¸°ì—…',\n",
    "    marker_color='green',\n",
    "    opacity=0.6,\n",
    "    nbinsx=50\n",
    "))\n",
    "\n",
    "# ë¶€ë„ ê¸°ì—…\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=y_prob_test[y_test == 1],\n",
    "    name='ë¶€ë„ ê¸°ì—…',\n",
    "    marker_color='red',\n",
    "    opacity=0.6,\n",
    "    nbinsx=50\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ (ì •ìƒ vs ë¶€ë„)',\n",
    "    xaxis_title='ë¶€ë„ í™•ë¥ ',\n",
    "    yaxis_title='ê¸°ì—… ìˆ˜',\n",
    "    yaxis_type='log',  # ë¡œê·¸ ìŠ¤ì¼€ì¼ (ë¶ˆê· í˜•)\n",
    "    barmode='overlay',\n",
    "    width=900, height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance ë¶„ì„\n",
    "\n",
    "ìµœì¢… ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance ì¶”ì¶œ\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"ëª¨ë¸ì—ì„œ Feature Importance ì¶”ì¶œ\"\"\"\n",
    "    \n",
    "    # Voting Classifierì¸ ê²½ìš° ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©\n",
    "    if isinstance(model, VotingClassifier):\n",
    "        model = model.estimators_[0]\n",
    "    \n",
    "    # Pipelineì¸ ê²½ìš° ë¶„ë¥˜ê¸° ì¶”ì¶œ\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        classifier = model.named_steps['classifier']\n",
    "    else:\n",
    "        classifier = model\n",
    "    \n",
    "    # ëª¨ë¸ë³„ Feature Importance ì¶”ì¶œ\n",
    "    if hasattr(classifier, 'feature_importances_'):\n",
    "        importances = classifier.feature_importances_\n",
    "    elif hasattr(classifier, 'coef_'):\n",
    "        importances = np.abs(classifier.coef_[0])\n",
    "    else:\n",
    "        raise ValueError(\"ëª¨ë¸ì— feature_importances_ ë˜ëŠ” coef_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # DataFrame ìƒì„±\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Feature Importance ê³„ì‚°\n",
    "feature_importance_df = get_feature_importance(final_model, X_train.columns)\n",
    "\n",
    "print(\"\\nğŸ“Š Feature Importance (Top 15)\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance_df.head(15).to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 4: Feature Importance (Top 15)\n",
    "top15_importance = feature_importance_df.head(15)\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=top15_importance['importance'][::-1],\n",
    "    y=top15_importance['feature'][::-1],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=top15_importance['importance'][::-1],\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text=top15_importance['importance'][::-1].round(4),\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Importance (Top 15)',\n",
    "    xaxis_title='Importance',\n",
    "    yaxis_title='Feature',\n",
    "    width=900, height=600,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Traffic Light ì‹œìŠ¤í…œ\n",
    "\n",
    "### ë™ì  ì„ê³„ê°’ ê²°ì •\n",
    "\n",
    "F1-Score ìµœì í™” ë° ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­(Recall â‰¥ 60%)ì„ ë°˜ì˜í•˜ì—¬ ì„ê³„ê°’ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "**3ë“±ê¸‰ ë¶„ë¥˜**:\n",
    "- ğŸ”´ **Red (ìœ„í—˜)**: ë¶€ë„ í™•ë¥  â‰¥ red_threshold â†’ ì¦‰ì‹œ ê´€ë¦¬ ëŒ€ìƒ\n",
    "- ğŸŸ¡ **Yellow (ì£¼ì˜)**: yellow_threshold â‰¤ ë¶€ë„ í™•ë¥  < red_threshold â†’ ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ\n",
    "- ğŸŸ¢ **Green (ì•ˆì „)**: ë¶€ë„ í™•ë¥  < yellow_threshold â†’ ì •ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: F1-Score ê¸°ë°˜ ìµœì  ì„ê³„ê°’ íƒìƒ‰\n",
    "precisions, recalls, thresholds_pr = precision_recall_curve(y_test, y_prob_test)\n",
    "\n",
    "# F1-Score ê³„ì‚°\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_pr[optimal_idx] if optimal_idx < len(thresholds_pr) else 0.5\n",
    "\n",
    "print(\"ğŸ“Š ì„ê³„ê°’ íƒìƒ‰ ê²°ê³¼\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"F1-Score ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}\")\n",
    "print(f\"  - F1-Score: {f1_scores[optimal_idx]:.4f}\")\n",
    "print(f\"  - Precision: {precisions[optimal_idx]:.4f}\")\n",
    "print(f\"  - Recall: {recalls[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ë°˜ì˜ (Recall >= 60%)\n",
    "target_recall = 0.6\n",
    "\n",
    "# Recall >= 60%ë¥¼ ë§Œì¡±í•˜ëŠ” ìµœëŒ€ Precision ì„ê³„ê°’ ì°¾ê¸°\n",
    "idx = np.where(recalls >= target_recall)[0]\n",
    "if len(idx) > 0:\n",
    "    best_idx = idx[np.argmax(precisions[idx])]\n",
    "    business_threshold = thresholds_pr[best_idx] if best_idx < len(thresholds_pr) else optimal_threshold\n",
    "    print(f\"\\nRecall 60% ë³´ì¥ ì„ê³„ê°’: {business_threshold:.4f}\")\n",
    "    print(f\"  - Precision: {precisions[best_idx]:.4f}\")\n",
    "    print(f\"  - Recall: {recalls[best_idx]:.4f}\")\n",
    "else:\n",
    "    business_threshold = optimal_threshold\n",
    "    print(f\"\\nâš ï¸  Recall 60% ë‹¬ì„± ë¶ˆê°€ â†’ F1 ìµœì  ì„ê³„ê°’ ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Traffic Light ì„ê³„ê°’ ì„¤ì •\n",
    "# Red: ë¹„ì¦ˆë‹ˆìŠ¤ ì„ê³„ê°’ ë˜ëŠ” ìƒìœ„ 5% ë°±ë¶„ìœ„ìˆ˜ ì¤‘ í° ê°’\n",
    "red_threshold = max(business_threshold, np.percentile(y_prob_test, 95))\n",
    "\n",
    "# Yellow: Redì˜ 40% ìˆ˜ì¤€ ë˜ëŠ” ìƒìœ„ 15% ë°±ë¶„ìœ„ìˆ˜ ì¤‘ í° ê°’\n",
    "yellow_threshold = max(red_threshold * 0.4, np.percentile(y_prob_test, 85))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸš¦ ìµœì¢… Traffic Light ì„ê³„ê°’\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸŸ¢ Green (ì•ˆì „):  < {yellow_threshold:.4f}\")\n",
    "print(f\"ğŸŸ¡ Yellow (ì£¼ì˜): {yellow_threshold:.4f} ~ {red_threshold:.4f}\")\n",
    "print(f\"ğŸ”´ Red (ìœ„í—˜):    â‰¥ {red_threshold:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic Light ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def traffic_light_classification(y_prob, yellow_th, red_th):\n",
    "    \"\"\"ë¶€ë„ í™•ë¥ ì„ 3ë“±ê¸‰ìœ¼ë¡œ ë¶„ë¥˜\"\"\"\n",
    "    conditions = [\n",
    "        (y_prob >= red_th),      # Red\n",
    "        (y_prob >= yellow_th)    # Yellow\n",
    "    ]\n",
    "    choices = ['Red', 'Yellow']\n",
    "    return np.select(conditions, choices, default='Green')\n",
    "\n",
    "# Test Set ë¶„ë¥˜\n",
    "grades_test = traffic_light_classification(y_prob_test, yellow_threshold, red_threshold)\n",
    "\n",
    "# í†µê³„ ê³„ì‚°\n",
    "traffic_stats = []\n",
    "\n",
    "for grade in ['Red', 'Yellow', 'Green']:\n",
    "    mask = (grades_test == grade)\n",
    "    n_companies = mask.sum()\n",
    "    n_defaults = (y_test[mask] == 1).sum()\n",
    "    \n",
    "    if n_companies > 0:\n",
    "        precision = n_defaults / n_companies\n",
    "        recall_contribution = n_defaults / (y_test == 1).sum()\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall_contribution = 0\n",
    "    \n",
    "    traffic_stats.append({\n",
    "        'ë“±ê¸‰': grade,\n",
    "        'ê¸°ì—… ìˆ˜': n_companies,\n",
    "        'ë¹„ìœ¨': f\"{n_companies / len(y_test) * 100:.1f}%\",\n",
    "        'ì‹¤ì œ ë¶€ë„ ìˆ˜': n_defaults,\n",
    "        'ì •ë°€ë„ (Precision)': f\"{precision * 100:.1f}%\",\n",
    "        'ë¶€ë„ í¬ì°©ë¥  ê¸°ì—¬': f\"{recall_contribution * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "traffic_df = pd.DataFrame(traffic_stats)\n",
    "\n",
    "# ì´í•© í–‰ ì¶”ê°€\n",
    "total_defaults = (y_test == 1).sum()\n",
    "red_yellow_defaults = ((grades_test == 'Red') | (grades_test == 'Yellow'))\n",
    "risk_defense_rate = (y_test[red_yellow_defaults] == 1).sum() / total_defaults * 100\n",
    "\n",
    "traffic_df.loc[len(traffic_df)] = [\n",
    "    '**í•©ê³„**',\n",
    "    len(y_test),\n",
    "    '100%',\n",
    "    total_defaults,\n",
    "    '-',\n",
    "    f'**{risk_defense_rate:.1f}%**'\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“Š Traffic Light ì‹œìŠ¤í…œ ì„±ëŠ¥ (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(traffic_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ… ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨: {risk_defense_rate:.1f}% (Red + Yellowì—ì„œ í¬ì°©í•œ ë¶€ë„ ë¹„ìœ¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 5: PR-AUC Curve + Traffic Light ì„ê³„ê°’\n",
    "fig = go.Figure()\n",
    "\n",
    "# PR Curve\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=recalls, y=precisions,\n",
    "    mode='lines',\n",
    "    name=f'PR Curve (AUC={pr_auc:.4f})',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# Yellow ì„ê³„ê°’ (Recall/Precision ì°¾ê¸°)\n",
    "yellow_idx = np.argmin(np.abs(thresholds_pr - yellow_threshold))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[recalls[yellow_idx]], y=[precisions[yellow_idx]],\n",
    "    mode='markers',\n",
    "    name=f'Yellow ì„ê³„ê°’ ({yellow_threshold:.4f})',\n",
    "    marker=dict(color='yellow', size=12, line=dict(color='black', width=2))\n",
    "))\n",
    "\n",
    "# Red ì„ê³„ê°’\n",
    "red_idx = np.argmin(np.abs(thresholds_pr - red_threshold))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[recalls[red_idx]], y=[precisions[red_idx]],\n",
    "    mode='markers',\n",
    "    name=f'Red ì„ê³„ê°’ ({red_threshold:.4f})',\n",
    "    marker=dict(color='red', size=12, line=dict(color='black', width=2))\n",
    "))\n",
    "\n",
    "# ë² ì´ìŠ¤ë¼ì¸\n",
    "baseline = y_test.mean()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[baseline, baseline],\n",
    "    mode='lines',\n",
    "    name=f'Baseline (ë¶€ë„ìœ¨={baseline:.2%})',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'PR Curve + Traffic Light ì„ê³„ê°’ (PR-AUC: {pr_auc:.4f})',\n",
    "    xaxis_title='Recall',\n",
    "    yaxis_title='Precision',\n",
    "    width=900, height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 6: Traffic Light ë¶„í¬ (ë„ë„› ì°¨íŠ¸)\n",
    "grade_counts = pd.Series(grades_test).value_counts()\n",
    "\n",
    "colors = {'Red': 'red', 'Yellow': 'yellow', 'Green': 'green'}\n",
    "color_list = [colors[grade] for grade in grade_counts.index]\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=[f\"ğŸ”´ {grade_counts.index[0]}\" if grade_counts.index[0] == 'Red' \n",
    "            else f\"ğŸŸ¡ {grade_counts.index[1]}\" if grade_counts.index[1] == 'Yellow'\n",
    "            else f\"ğŸŸ¢ {grade_counts.index[2]}\" for _ in range(len(grade_counts))],\n",
    "    values=grade_counts.values,\n",
    "    hole=0.4,\n",
    "    marker=dict(colors=color_list),\n",
    "    textinfo='label+percent',\n",
    "    textfont_size=14\n",
    ")])\n",
    "\n",
    "# ë¼ë²¨ ìˆ˜ì •\n",
    "labels_formatted = []\n",
    "for grade in grade_counts.index:\n",
    "    if grade == 'Red':\n",
    "        labels_formatted.append('ğŸ”´ Red (ìœ„í—˜)')\n",
    "    elif grade == 'Yellow':\n",
    "        labels_formatted.append('ğŸŸ¡ Yellow (ì£¼ì˜)')\n",
    "    else:\n",
    "        labels_formatted.append('ğŸŸ¢ Green (ì•ˆì „)')\n",
    "\n",
    "fig.data[0].labels = labels_formatted\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Traffic Light ë“±ê¸‰ë³„ ê¸°ì—… ë¶„í¬',\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” 7: ë“±ê¸‰ë³„ ì‹¤ì œ ë¶€ë„ìœ¨\n",
    "default_rates = []\n",
    "for grade in ['Red', 'Yellow', 'Green']:\n",
    "    mask = (grades_test == grade)\n",
    "    if mask.sum() > 0:\n",
    "        default_rate = (y_test[mask] == 1).mean() * 100\n",
    "    else:\n",
    "        default_rate = 0\n",
    "    default_rates.append(default_rate)\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=['ğŸ”´ Red (ìœ„í—˜)', 'ğŸŸ¡ Yellow (ì£¼ì˜)', 'ğŸŸ¢ Green (ì•ˆì „)'],\n",
    "    y=default_rates,\n",
    "    marker_color=['red', 'yellow', 'green'],\n",
    "    text=[f\"{rate:.1f}%\" for rate in default_rates],\n",
    "    textposition='outside'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Traffic Light ë“±ê¸‰ë³„ ì‹¤ì œ ë¶€ë„ìœ¨',\n",
    "    xaxis_title='ë“±ê¸‰',\n",
    "    yaxis_title='ë¶€ë„ìœ¨ (%)',\n",
    "    width=800, height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„\n",
    "\n",
    "### Cumulative Gains Curve\n",
    "\n",
    "ìƒìœ„ N% ê¸°ì—…ì„ ì‹¬ì‚¬í–ˆì„ ë•Œ ì „ì²´ ë¶€ë„ ê¸°ì—… ì¤‘ ëª‡ %ë¥¼ í¬ì°©í•  ìˆ˜ ìˆëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Gains ê³„ì‚°\n",
    "sorted_indices = np.argsort(y_prob_test)[::-1]  # í™•ë¥  ë‚´ë¦¼ì°¨ìˆœ\n",
    "y_test_sorted = y_test.iloc[sorted_indices].values\n",
    "\n",
    "cumulative_defaults = np.cumsum(y_test_sorted)\n",
    "total_defaults = y_test_sorted.sum()\n",
    "\n",
    "cumulative_gain = cumulative_defaults / total_defaults * 100\n",
    "population_pct = np.arange(1, len(y_test) + 1) / len(y_test) * 100\n",
    "\n",
    "# ëœë¤ ê¸°ì¤€ì„ \n",
    "random_baseline = population_pct\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig = go.Figure()\n",
    "\n",
    "# ëª¨ë¸ Gains\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=population_pct, y=cumulative_gain,\n",
    "    mode='lines',\n",
    "    name='ëª¨ë¸ (ìµœì  ì •ë ¬)',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# ëœë¤ ê¸°ì¤€ì„ \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=population_pct, y=random_baseline,\n",
    "    mode='lines',\n",
    "    name='ëœë¤ ì„ íƒ',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "# ì£¼ìš” í¬ì¸íŠ¸ í‘œì‹œ (ìƒìœ„ 5%, 10%, 20%)\n",
    "for pct in [5, 10, 20]:\n",
    "    idx = int(len(y_test) * pct / 100)\n",
    "    gain = cumulative_gain[idx]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[pct], y=[gain],\n",
    "        mode='markers+text',\n",
    "        name=f'ìƒìœ„ {pct}%',\n",
    "        marker=dict(size=10, color='red'),\n",
    "        text=[f'{gain:.1f}%'],\n",
    "        textposition='top center'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cumulative Gains Curve (ëª¨ë¸ íš¨ìœ¨ì„±)',\n",
    "    xaxis_title='ì‹¬ì‚¬ ëŒ€ìƒ ê¸°ì—… ë¹„ìœ¨ (%)',\n",
    "    yaxis_title='í¬ì°©í•œ ë¶€ë„ ê¸°ì—… ë¹„ìœ¨ (%)',\n",
    "    width=900, height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íš¨ìœ¨ì„± ë¶„ì„\n",
    "efficiency_analysis = []\n",
    "\n",
    "for pct in [5, 10, 15, 20, 30, 50]:\n",
    "    idx = int(len(y_test) * pct / 100)\n",
    "    gain = cumulative_gain[idx]\n",
    "    lift = gain / pct  # Lift = ëª¨ë¸ ì„±ëŠ¥ / ëœë¤ ì„±ëŠ¥\n",
    "    \n",
    "    efficiency_analysis.append({\n",
    "        'ì‹¬ì‚¬ ë¹„ìœ¨': f'{pct}%',\n",
    "        'í¬ì°© ë¶€ë„': f'{gain:.1f}%',\n",
    "        'Lift': f'{lift:.2f}x',\n",
    "        'ì‹¬ì‚¬ ê¸°ì—… ìˆ˜': f'{idx:,}',\n",
    "        'ì˜ˆìƒ ë¶€ë„ ìˆ˜': f'{int(cumulative_defaults[idx]):,}'\n",
    "    })\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_analysis)\n",
    "\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ íš¨ìœ¨ì„± ë¶„ì„\")\n",
    "print(\"=\" * 80)\n",
    "print(efficiency_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nğŸ’¡ í•´ì„: ìƒìœ„ 10% ê¸°ì—…ë§Œ ì‹¬ì‚¬í•´ë„ ì „ì²´ ë¶€ë„ì˜ XX%ë¥¼ í¬ì°© ê°€ëŠ¥\")\n",
    "print(\"         â†’ ëœë¤ ì„ íƒ ëŒ€ë¹„ X.XXë°° íš¨ìœ¨ì \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ ì €ì¥ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### Part 4 SHAP ë¶„ì„ì„ ìœ„í•œ íŒŒì¼ ì €ì¥\n",
    "\n",
    "SHAP ë¶„ì„ì— í•„ìš”í•œ íŒŒì¼:\n",
    "1. **ë¶„ë¥˜ê¸°ë§Œ** (ì „ì²˜ë¦¬ ì œì™¸) - SHAP explainer ìƒì„±ìš©\n",
    "2. **ì „ì²˜ë¦¬ëœ ë°ì´í„°** - Background ë°ì´í„° ë° ì„¤ëª… ëŒ€ìƒ\n",
    "3. **Traffic Light ì„ê³„ê°’** - ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš©ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ìµœì¢… ëª¨ë¸ (ì „ì²´ íŒŒì´í”„ë¼ì¸) ì €ì¥\n",
    "model_path = os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_ìµœì¢…ëª¨ë¸.pkl')\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì €ì¥: {model_path}\")\n",
    "\n",
    "# 2. ë¶„ë¥˜ê¸°ë§Œ ì €ì¥ (SHAP ë¶„ì„ìš©) â­ ì¤‘ìš”!\n",
    "if isinstance(final_model, VotingClassifier):\n",
    "    # Voting Classifierì¸ ê²½ìš° ì²« ë²ˆì§¸ ëª¨ë¸ì˜ ë¶„ë¥˜ê¸° ì‚¬ìš©\n",
    "    classifier_only = final_model.estimators_[0].named_steps['classifier']\n",
    "elif hasattr(final_model, 'named_steps'):\n",
    "    # Pipelineì¸ ê²½ìš°\n",
    "    classifier_only = final_model.named_steps['classifier']\n",
    "else:\n",
    "    classifier_only = final_model\n",
    "\n",
    "classifier_path = os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_ë¶„ë¥˜ê¸°.pkl')\n",
    "joblib.dump(classifier_only, classifier_path)\n",
    "print(f\"âœ… ë¶„ë¥˜ê¸°ë§Œ ì €ì¥ (SHAPìš©): {classifier_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (SHAP ë¶„ì„ìš©) â­ ì¤‘ìš”!\n",
    "\n",
    "# ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "if isinstance(final_model, VotingClassifier):\n",
    "    # Voting Classifierì¸ ê²½ìš° ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©\n",
    "    pipeline_for_preprocessing = final_model.estimators_[0]\n",
    "else:\n",
    "    pipeline_for_preprocessing = final_model\n",
    "\n",
    "preprocessing_steps = [\n",
    "    ('inf_handler', pipeline_for_preprocessing.named_steps['inf_handler']),\n",
    "    ('winsorizer', pipeline_for_preprocessing.named_steps['winsorizer']),\n",
    "    ('log_transformer', pipeline_for_preprocessing.named_steps['log_transformer']),\n",
    "    ('imputer', pipeline_for_preprocessing.named_steps['imputer']),\n",
    "    ('scaler', pipeline_for_preprocessing.named_steps['scaler'])\n",
    "]\n",
    "\n",
    "preprocessing_only = Pipeline(preprocessing_steps)\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì ìš©\n",
    "print(\"\\nâ³ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "X_train_processed = preprocessing_only.transform(X_train)\n",
    "X_test_processed = preprocessing_only.transform(X_test)\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ (ì»¬ëŸ¼ëª… ìœ ì§€)\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=X_train.columns)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=X_test.columns)\n",
    "\n",
    "# ì €ì¥\n",
    "X_train_processed_df.to_csv(os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_X_train_processed.csv'),\n",
    "                             index=False, encoding='utf-8-sig')\n",
    "X_test_processed_df.to_csv(os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_X_test_processed.csv'),\n",
    "                            index=False, encoding='utf-8-sig')\n",
    "y_train.to_csv(os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_y_train.csv'),\n",
    "               index=False, encoding='utf-8-sig', header=['target'])\n",
    "y_test.to_csv(os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_y_test.csv'),\n",
    "              index=False, encoding='utf-8-sig', header=['target'])\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ëœ Train ë°ì´í„° ì €ì¥: X_train_processed.csv ({X_train_processed_df.shape})\")\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ëœ Test ë°ì´í„° ì €ì¥: X_test_processed.csv ({X_test_processed_df.shape})\")\n",
    "print(f\"âœ… Train íƒ€ê²Ÿ ì €ì¥: y_train.csv\")\n",
    "print(f\"âœ… Test íƒ€ê²Ÿ ì €ì¥: y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Traffic Light ì„ê³„ê°’ ì €ì¥\n",
    "thresholds = {\n",
    "    'yellow_threshold': yellow_threshold,\n",
    "    'red_threshold': red_threshold,\n",
    "    'optimal_f1_threshold': optimal_threshold\n",
    "}\n",
    "\n",
    "thresholds_path = os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_ì„ê³„ê°’.pkl')\n",
    "joblib.dump(thresholds, thresholds_path)\n",
    "print(f\"\\nâœ… Traffic Light ì„ê³„ê°’ ì €ì¥: {thresholds_path}\")\n",
    "print(f\"   - Yellow: {yellow_threshold:.4f}\")\n",
    "print(f\"   - Red: {red_threshold:.4f}\")\n",
    "print(f\"   - F1 ìµœì : {optimal_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ìµœì¢… ê²°ê³¼ ìš”ì•½ ì €ì¥\n",
    "summary = {\n",
    "    'final_model_name': final_model_name,\n",
    "    'test_pr_auc': pr_auc,\n",
    "    'test_roc_auc': roc_auc,\n",
    "    'test_f1_score': f1,\n",
    "    'test_precision': precision,\n",
    "    'test_recall': recall,\n",
    "    'confusion_matrix': {'TN': int(tn), 'FP': int(fp), 'FN': int(fn), 'TP': int(tp)},\n",
    "    'type2_error': type2_error,\n",
    "    'risk_defense_rate': risk_defense_rate,\n",
    "    'traffic_light_thresholds': thresholds,\n",
    "    'n_features': X_train.shape[1],\n",
    "    'n_train': len(X_train),\n",
    "    'n_test': len(X_test),\n",
    "    'train_default_rate': y_train.mean(),\n",
    "    'test_default_rate': y_test.mean()\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(SAVE_DIR, 'ë°œí‘œ_Part3_ê²°ê³¼ìš”ì•½.pkl')\n",
    "joblib.dump(summary, summary_path)\n",
    "print(f\"\\nâœ… ê²°ê³¼ ìš”ì•½ ì €ì¥: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ íŒŒì¼ ëª©ë¡ í™•ì¸\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ ì €ì¥ëœ íŒŒì¼ ëª©ë¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "saved_files = [\n",
    "    'ë°œí‘œ_Part3_ìµœì¢…ëª¨ë¸.pkl',\n",
    "    'ë°œí‘œ_Part3_ë¶„ë¥˜ê¸°.pkl',\n",
    "    'ë°œí‘œ_Part3_X_train_processed.csv',\n",
    "    'ë°œí‘œ_Part3_X_test_processed.csv',\n",
    "    'ë°œí‘œ_Part3_y_train.csv',\n",
    "    'ë°œí‘œ_Part3_y_test.csv',\n",
    "    'ë°œí‘œ_Part3_ì„ê³„ê°’.pkl',\n",
    "    'ë°œí‘œ_Part3_ê²°ê³¼ìš”ì•½.pkl'\n",
    "]\n",
    "\n",
    "for i, filename in enumerate(saved_files, 1):\n",
    "    filepath = os.path.join(SAVE_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / 1024**2\n",
    "        print(f\"{i}. âœ… {filename} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"{i}. âŒ {filename} (íŒŒì¼ ì—†ìŒ)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ Part 3 ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPart 4 SHAP ë¶„ì„ì„ ìœ„í•œ í•„ìˆ˜ íŒŒì¼:\")\n",
    "print(\"  1. ë°œí‘œ_Part3_ë¶„ë¥˜ê¸°.pkl - SHAP explainer ìƒì„±ìš©\")\n",
    "print(\"  2. ë°œí‘œ_Part3_X_train_processed.csv - Background ë°ì´í„°\")\n",
    "print(\"  3. ë°œí‘œ_Part3_X_test_processed.csv - ì„¤ëª… ëŒ€ìƒ ë°ì´í„°\")\n",
    "print(\"  4. ë°œí‘œ_Part3_ì„ê³„ê°’.pkl - Traffic Light ì‹œìŠ¤í…œìš©\")\n",
    "print(\"\\në‹¤ìŒ ë‹¨ê³„: Part 4 - SHAP ê¸°ë°˜ ëª¨ë¸ í•´ì„ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Part 3 ìš”ì•½\n",
    "\n",
    "### âœ… ì™„ë£Œ í•­ëª©\n",
    "\n",
    "1. **ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬**: 5ê°€ì§€ ë¦¬ìƒ˜í”Œë§ ì „ëµ í…ŒìŠ¤íŠ¸ (SMOTE, BorderlineSMOTE, RandomUnderSampler, SMOTETomek)\n",
    "2. **AutoML ìµœì í™”**: RandomizedSearchCVë¡œ 5ê°œ ëª¨ë¸ Ã— 100íšŒ ìƒ˜í”Œë§\n",
    "3. **ì•™ìƒë¸” ëª¨ë¸**: Weighted Voting (Top 3 ëª¨ë¸ ê¸°ë°˜)\n",
    "4. **Traffic Light ì‹œìŠ¤í…œ**: ë™ì  ì„ê³„ê°’ ê¸°ë°˜ 3ë“±ê¸‰ ë¶„ë¥˜ (Red/Yellow/Green)\n",
    "5. **ëª¨ë¸ ì €ì¥**: Part 4 SHAP ë¶„ì„ì„ ìœ„í•œ ëª¨ë“  í•„ìˆ˜ íŒŒì¼ ì¶œë ¥\n",
    "\n",
    "### ğŸ“Š í•µì‹¬ ê²°ê³¼\n",
    "\n",
    "- **ìµœì¢… ëª¨ë¸**: [Single Best / Weighted Voting]\n",
    "- **Test PR-AUC**: X.XXXX\n",
    "- **ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨**: XX.X% (Red + Yellowì—ì„œ ë¶€ë„ í¬ì°©)\n",
    "- **ëª¨ë¸ íš¨ìœ¨ì„±**: ìƒìœ„ 10% ì‹¬ì‚¬ ì‹œ ì „ì²´ ë¶€ë„ì˜ XX% í¬ì°© (Lift X.XXx)\n",
    "\n",
    "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (Part 4)\n",
    "\n",
    "- SHAP ê°’ ê³„ì‚° ë° Feature Importance ì¬ë¶„ì„\n",
    "- ê°œë³„ ê¸°ì—… ë¶€ë„ ì›ì¸ í•´ì„ (Waterfall Plot)\n",
    "- ì—…ì¢…ë³„/ë“±ê¸‰ë³„ SHAP íŒ¨í„´ ë¶„ì„\n",
    "- ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ë° ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ\n",
    "\n",
    "---\n",
    "\n",
    "**ë…¸íŠ¸ë¶ ì‘ì„±ì¼**: 2025ë…„  \n",
    "**í”„ë¡œì íŠ¸**: í•œêµ­ ê¸°ì—… ë¶€ë„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ  \n",
    "**ì €ì**: Claude Code\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
