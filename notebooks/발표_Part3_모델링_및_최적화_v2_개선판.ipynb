{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“— ë°œí‘œìš© Part 3: ëª¨ë¸ë§ ë° ìµœì í™” v2 ê°œì„ íŒ\n",
    "\n",
    "## ğŸ¯ v2 ê°œì„ íŒ ì£¼ìš” ë³€ê²½ ì‚¬í•­\n",
    "\n",
    "### v2 ëŒ€ë¹„ ê°œì„  ì‚¬í•­ (P0/P1 ëª¨ë‘ ë°˜ì˜)\n",
    "\n",
    "**P0 (í•„ìˆ˜ ìˆ˜ì •):**\n",
    "1. âœ… **Pipeline êµ¬ì¡° ë„ì…** - ImbPipeline ì‚¬ìš©, ì „ì²˜ë¦¬+ë¦¬ìƒ˜í”Œë§+ëª¨ë¸ í†µí•©\n",
    "2. âœ… **RandomizedSearchCV n_iter=200** - íƒìƒ‰ ì„±ëŠ¥ í–¥ìƒ\n",
    "3. âœ… **ì•™ìƒë¸” ë‹¤ì–‘ì„± ì²´í¬** - Top 3ê°€ ëª¨ë‘ GBMì´ë©´ ì´ì¢… ëª¨ë¸ ê°•ì œ í¬í•¨\n",
    "\n",
    "**P1 (ê¶Œì¥ ê°œì„ ):**\n",
    "4. âœ… **CV ê¸°ë°˜ ì„ê³„ê°’ ìµœì í™”** - Validation + CV í‰ê·  ì‚¬ìš©\n",
    "5. âœ… **Winsorizer ì‹¤í—˜** - ìˆìŒ/ì—†ìŒ ì„±ëŠ¥ ë¹„êµ\n",
    "6. âœ… **Traffic Light Yellow ë¡œì§ ì¼ê´€ì„±** - Recall ê¸°ì¤€ ì¼ê´€ ì‚¬ìš©\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Part 3 ëª©í‘œ ë° ì´ì „ Part ìš”ì•½\n",
    "\n",
    "### ì´ì „ Part ì™„ë£Œ ì‚¬í•­\n",
    "\n",
    "**Part 1: ë¬¸ì œ ì •ì˜ ë° í•µì‹¬ ë°œê²¬**\n",
    "- 50,105ê°œ í•œêµ­ ê¸°ì—…, ë¶€ë„ìœ¨ 1.51% (1:66 ë¶ˆê· í˜•)\n",
    "- **í•µì‹¬ ë°œê²¬**: ìœ ë™ì„±ì´ ê°€ì¥ ê°•ë ¥í•œ ì˜ˆì¸¡ ë³€ìˆ˜\n",
    "- ì—…ì¢…ë³„ ë¶€ë„ìœ¨ 2ë°° ì°¨ì´ (ì œì¡°ì—… vs ì„œë¹„ìŠ¤ì—…)\n",
    "\n",
    "**Part 2: ë„ë©”ì¸ íŠ¹ì„± ê³µí•™**\n",
    "- 52ê°œ ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± ìƒì„±\n",
    "- VIF/IV/AUC ê¸°ë°˜ íŠ¹ì„± ì„ íƒ â†’ **27ê°œ ìµœì¢… íŠ¹ì„±**\n",
    "- 7ê°œ ì¹´í…Œê³ ë¦¬: ìœ ë™ì„±, ì§€ê¸‰ë¶ˆëŠ¥, ì¬ë¬´ì¡°ì‘, í•œêµ­ì‹œì¥, ì´í•´ê´€ê³„ì, ë³µí•©ë¦¬ìŠ¤í¬, ìƒí˜¸ì‘ìš©\n",
    "\n",
    "### Part 3 v2 ëª©í‘œ (Data Leakage ì™„ì „ ì œê±°)\n",
    "\n",
    "**ğŸš¨ í•µì‹¬ ê°œì„ : Data Leakage ì™„ì „ ì œê±°**\n",
    "\n",
    "```\n",
    "âœ… v2 í•´ê²°ì±…:\n",
    "- 3-Way Split (Train/Validation/Test)\n",
    "- Test setì€ ìµœì¢… í‰ê°€ ì§ì „ ë‹¨ í•œ ë²ˆë§Œ\n",
    "- ëª¨ë“  ì˜ì‚¬ê²°ì •ì€ Validation set ê¸°ë°˜\n",
    "- Pipeline êµ¬ì¡°ë¡œ ì „ì²˜ë¦¬/ë¦¬ìƒ˜í”Œë§ ëˆ„ë½ ë°©ì§€\n",
    "```\n",
    "\n",
    "**ì£¼ìš” ê°œì„  ì‚¬í•­:**\n",
    "1. **3-Way Data Split** (Train 60% / Validation 20% / Test 20%)\n",
    "2. **Pipeline ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ ì „ëµ** (SMOTE vs Class Weight)\n",
    "3. **Validation ê¸°ë°˜ ëª¨ë¸ ì„ íƒ** + Statistical Test + Ensemble Diversity\n",
    "4. **CV+Validation ì„ê³„ê°’ ìµœì í™”** (F2-Score, Recall ìš°ì„ )\n",
    "5. **ë°ì´í„° ê¸°ë°˜ Traffic Light** (Recall 80%/95% ë³´ì¥)\n",
    "6. **Test Setì€ ë§ˆì§€ë§‰ í‰ê°€ë§Œ** (ì ˆëŒ€ ì˜ì‚¬ê²°ì •ì— ì‚¬ìš© ì•ˆ í•¨)\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥:**\n",
    "- PR-AUC: 0.15~0.20 (ë¶ˆê· í˜• ë°ì´í„° ê³ ë ¤)\n",
    "- F2-Score: 0.35~0.50 (Recall ìš°ì„ )\n",
    "- Recall: 60~80%\n",
    "- Type II Error: 20~40% (ë¶€ë„ ë¯¸íƒì§€ ìµœì†Œí™”)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import platform\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¸ì‹ ëŸ¬ë‹ - ì „ì²˜ë¦¬\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ - ëª¨ë¸\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¸ì‹ ëŸ¬ë‹ - íŠœë‹ (â­ Pipeline ì‚¬ìš©)\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # â­ Imbalanced Pipeline\n",
    "\n",
    "# ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ ë©”íŠ¸ë¦­\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve,\n",
    "    make_scorer, fbeta_score, recall_score, precision_score\n",
    ")\n",
    "\n",
    "# í†µê³„ ê²€ì •\n",
    "from scipy.stats import wilcoxon, ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²½ê³  ë¬´ì‹œ\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (CLAUDE.md ì¤€ìˆ˜)\n",
    "if platform.system() == 'Darwin':\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "else:\n",
    "    plt.rc('font', family='NanumGothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn, catboost ë²„ì „ ì„í¬íŠ¸\n",
    "import sklearn\n",
    "import catboost\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"   - NumPy: {np.__version__}\")\n",
    "print(f\"   - Pandas: {pd.__version__}\")\n",
    "print(f\"   - Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"   - LightGBM: {lgb.__version__}\")\n",
    "print(f\"   - XGBoost: {xgb.__version__}\")\n",
    "print(f\"   - CatBoost: {catboost.__version__}\")\n",
    "print(f\"   - Platform: {platform.system()}\")\n",
    "print(f\"   - Random State: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„° ë¡œë”© ë° 3-Way Split â­\n",
    "\n",
    "### 1.1 ë°ì´í„° ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ (í•˜ë“œì½”ë”© ê¸ˆì§€)\n",
    "DATA_DIR = '../data'\n",
    "FEATURES_FILE = os.path.join(DATA_DIR, 'features', 'domain_based_features_ì™„ì „íŒ.csv')\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ë°ì´í„° ë¡œë”©\n",
    "print(f\"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {FEATURES_FILE}\")\n",
    "df = pd.read_csv(FEATURES_FILE, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ\")\n",
    "print(f\"   - ì „ì²´ ê¸°ì—… ìˆ˜: {len(df):,}\")\n",
    "print(f\"   - ì „ì²´ ë³€ìˆ˜ ìˆ˜: {len(df.columns)}\")\n",
    "print(f\"   - ë©”ëª¨ë¦¬ ì‚¬ìš©: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸\n",
    "TARGET_COL = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'\n",
    "\n",
    "print(f\"ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜: {TARGET_COL}\")\n",
    "print(f\"\\në¶€ë„ ë¶„í¬:\")\n",
    "print(df[TARGET_COL].value_counts())\n",
    "print(f\"\\në¶€ë„ìœ¨: {df[TARGET_COL].mean():.4%}\")\n",
    "print(f\"ë¶ˆê· í˜• ë¹„ìœ¨: 1:{int(1/df[TARGET_COL].mean())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¹ì„±(X)ê³¼ íƒ€ê²Ÿ(y) ë¶„ë¦¬\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(f\"âœ… íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬ ì™„ë£Œ\")\n",
    "print(f\"   - X shape: {X.shape}\")\n",
    "print(f\"   - y shape: {y.shape}\")\n",
    "print(f\"   - íŠ¹ì„± ëª©ë¡ (27ê°œ):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"      {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 3-Way Split: Train / Validation / Test â­â­â­\n",
    "\n",
    "**ğŸš¨ Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ í•µì‹¬ ì„¤ê³„:**\n",
    "\n",
    "```\n",
    "ì „ì²´ ë°ì´í„° (50,105)\n",
    "â”œâ”€ Train Set (60%, ~30,063): ëª¨ë¸ í•™ìŠµ + CV íŠœë‹\n",
    "â”œâ”€ Validation Set (20%, ~10,021): ëª¨ë¸ ì„ íƒ, ì„ê³„ê°’ ìµœì í™”, ì˜ì‚¬ê²°ì •\n",
    "â””â”€ Test Set (20%, ~10,021): ìµœì¢… í‰ê°€ë§Œ (ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ì°¨ ë¶„í• : Train+Val (80%) vs Test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2ì°¨ ë¶„í• : Train (60%) vs Validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.25, \n",
    "    stratify=y_temp, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… 3-Way Split ì™„ë£Œ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Train Set:      {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%, ë¶€ë„ìœ¨: {y_train.mean():.4%})\")\n",
    "print(f\"Validation Set: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%, ë¶€ë„ìœ¨: {y_val.mean():.4%})\")\n",
    "print(f\"Test Set:       {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%, ë¶€ë„ìœ¨: {y_test.mean():.4%})\")\n",
    "print(\"=\"*70)\n",
    "print(\"âš ï¸  Test Setì€ ìµœì¢… í‰ê°€ ì „ê¹Œì§€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚° (Class Weightìš©)\n",
    "n_pos = y_train.sum()\n",
    "n_neg = len(y_train) - n_pos\n",
    "imbalance_ratio = n_neg / n_pos\n",
    "\n",
    "print(f\"ğŸ“Š Train Set ë¶ˆê· í˜• ë¶„ì„\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"ë¶€ë„ ê¸°ì—… (Positive):  {n_pos:,} ({n_pos/len(y_train)*100:.2f}%)\")\n",
    "print(f\"ì •ìƒ ê¸°ì—… (Negative):  {n_neg:,} ({n_neg/len(y_train)*100:.2f}%)\")\n",
    "print(f\"ë¶ˆê· í˜• ë¹„ìœ¨:           1:{imbalance_ratio:.1f}\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"\\nClass Weight ê³„ì‚°:\")\n",
    "print(f\"  - sqrt(neg/pos) = {np.sqrt(imbalance_ratio):.2f}\")\n",
    "print(f\"  - neg/pos = {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜\n",
    "\n",
    "### 2.1 Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer 1: ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬\n",
    "class InfiniteHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"ë¬´í•œëŒ€ ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "        return X\n",
    "\n",
    "print(\"âœ… InfiniteHandler ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer 2: Winsorizer (ì´ìƒì¹˜ ì œí•œ)\n",
    "class Winsorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"ì´ìƒì¹˜ë¥¼ percentile ë²”ìœ„ë¡œ ì œí•œ\"\"\"\n",
    "    \n",
    "    def __init__(self, lower=0.005, upper=0.995):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.lower_bounds_ = None\n",
    "        self.upper_bounds_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds_ = np.percentile(X, self.lower * 100, axis=0)\n",
    "        self.upper_bounds_ = np.percentile(X, self.upper * 100, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X = np.clip(X, self.lower_bounds_, self.upper_bounds_)\n",
    "        return X\n",
    "\n",
    "print(\"âœ… Winsorizer ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(use_winsorizer=False):\n",
    "    \"\"\"\n",
    "    ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "    \n",
    "    ìˆœì„œ: InfiniteHandler â†’ Imputer â†’ Winsorizer(ì„ íƒ) â†’ Scaler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    use_winsorizer : bool, default=False\n",
    "        Winsorizer ì ìš© ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('inf_handler', InfiniteHandler()),\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ]\n",
    "    \n",
    "    if use_winsorizer:\n",
    "        steps.append(('winsorizer', Winsorizer(0.005, 0.995)))\n",
    "    \n",
    "    steps.append(('scaler', RobustScaler()))\n",
    "    \n",
    "    return Pipeline(steps)\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 P1 ê°œì„ : Winsorizer ì‹¤í—˜ â­\n",
    "\n",
    "**ëª©ì :** Tree ëª¨ë¸ì€ ì´ìƒì¹˜ì— ê°•ê±´í•˜ì§€ë§Œ, ë¶€ë„ ë°ì´í„°ì˜ ê·¹ë‹¨ê°’ì´ ì¤‘ìš” ì‹œê·¸ë„ì¼ ìˆ˜ ìˆìŒ\n",
    "\n",
    "**ì‹¤í—˜:** Winsorizer ìˆìŒ vs ì—†ìŒ ì„±ëŠ¥ ë¹„êµ (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorizer ì‹¤í—˜ (LightGBM ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)\n",
    "print(\"ğŸ“Š Winsorizer ì‹¤í—˜ (Validation Set ê¸°ë°˜)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "winsorizer_results = {}\n",
    "\n",
    "for use_wins in [False, True]:\n",
    "    label = \"Winsorizer ìˆìŒ\" if use_wins else \"Winsorizer ì—†ìŒ\"\n",
    "    \n",
    "    # ì „ì²˜ë¦¬\n",
    "    prep = create_preprocessing_pipeline(use_winsorizer=use_wins)\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    \n",
    "    # LightGBM í•™ìŠµ\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        scale_pos_weight=np.sqrt(imbalance_ratio),\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_train_prep, y_train)\n",
    "    \n",
    "    # Validation í‰ê°€\n",
    "    y_val_prob = model.predict_proba(X_val_prep)[:, 1]\n",
    "    pr_auc = average_precision_score(y_val, y_val_prob)\n",
    "    \n",
    "    winsorizer_results[label] = pr_auc\n",
    "    print(f\"{label:20s} | Validation PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ì„ íƒ\n",
    "if winsorizer_results[\"Winsorizer ìˆìŒ\"] > winsorizer_results[\"Winsorizer ì—†ìŒ\"]:\n",
    "    use_winsorizer_final = True\n",
    "    diff = winsorizer_results[\"Winsorizer ìˆìŒ\"] - winsorizer_results[\"Winsorizer ì—†ìŒ\"]\n",
    "    print(f\"âœ… ì„ íƒ: Winsorizer ì‚¬ìš© (PR-AUC +{diff:.4f})\")\n",
    "else:\n",
    "    use_winsorizer_final = False\n",
    "    diff = winsorizer_results[\"Winsorizer ì—†ìŒ\"] - winsorizer_results[\"Winsorizer ìˆìŒ\"]\n",
    "    print(f\"âœ… ì„ íƒ: Winsorizer ë¯¸ì‚¬ìš© (PR-AUC +{diff:.4f})\")\n",
    "    print(f\"   ì´ìœ : Tree ëª¨ë¸ì€ ì´ìƒì¹˜ì— ê°•ê±´, ê·¹ë‹¨ê°’ì´ ë¶€ë„ ì‹œê·¸ë„ì¼ ìˆ˜ ìˆìŒ\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. P0 ê°œì„ : Pipeline ê¸°ë°˜ ë¦¬ìƒ˜í”Œë§ ì „ëµ â­â­â­\n",
    "\n",
    "### 3.1 ê°œì„  ì‚¬í•­\n",
    "\n",
    "**v2 ë¬¸ì œì :**\n",
    "- ì „ì²˜ë¦¬ì™€ ëª¨ë¸ì´ ë¶„ë¦¬ â†’ ì¬ì‚¬ìš© ì‹œ ì˜¤ë¥˜ ìœ„í—˜\n",
    "- ë¦¬ìƒ˜í”Œë§ì„ Pipelineì— í†µí•©í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "**v2 ê°œì„ íŒ í•´ê²°ì±…:**\n",
    "- âœ… **ImbPipeline** ì‚¬ìš©: ì „ì²˜ë¦¬ + ë¦¬ìƒ˜í”Œë§ + ëª¨ë¸ í†µí•©\n",
    "- âœ… Raw ë°ì´í„°ë¡œ AutoML (ì „ì²˜ë¦¬ ëˆ„ë½ ë°©ì§€)\n",
    "- âœ… Best practice ì¤€ìˆ˜\n",
    "\n",
    "### 3.2 Pipeline ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_pipeline(classifier, resampler='passthrough', use_winsorizer=False):\n",
    "    \"\"\"\n",
    "    ì „ì²´ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìƒì„± (ì „ì²˜ë¦¬ + ë¦¬ìƒ˜í”Œë§ + ëª¨ë¸)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classifier : estimator\n",
    "        ë¶„ë¥˜ ëª¨ë¸\n",
    "    resampler : 'passthrough' or sampler object\n",
    "        ë¦¬ìƒ˜í”Œë§ ë°©ë²• (passthrough=ì—†ìŒ)\n",
    "    use_winsorizer : bool\n",
    "        Winsorizer ì‚¬ìš© ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pipeline : ImbPipeline\n",
    "    \"\"\"\n",
    "    # ì „ì²˜ë¦¬ ë‹¨ê³„\n",
    "    steps = [\n",
    "        ('inf_handler', InfiniteHandler()),\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ]\n",
    "    \n",
    "    if use_winsorizer:\n",
    "        steps.append(('winsorizer', Winsorizer(0.005, 0.995)))\n",
    "    \n",
    "    steps.append(('scaler', RobustScaler()))\n",
    "    \n",
    "    # ë¦¬ìƒ˜í”Œë§ + ëª¨ë¸\n",
    "    steps.append(('resampler', resampler))\n",
    "    steps.append(('classifier', classifier))\n",
    "    \n",
    "    return ImbPipeline(steps)\n",
    "\n",
    "print(\"âœ… Pipeline ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ë¦¬ìƒ˜í”Œë§ ì „ëµ ëŒ€ì¡° ì‹¤í—˜ (Pipeline ê¸°ë°˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ë¦¬ìƒ˜í”Œë§ ì „ëµ ëŒ€ì¡° ì‹¤í—˜ (Pipeline ê¸°ë°˜)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Strategy A: SMOTE ê³„ì—´ (Class Weight ì—†ìŒ)\")\n",
    "print(\"Strategy B: Class Weight Only (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy A: SMOTE ê³„ì—´\n",
    "print(\"\\nğŸ”µ Strategy A: SMOTE ê³„ì—´\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "smote_methods = {\n",
    "    'SMOTE': SMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE),\n",
    "    'BorderlineSMOTE': BorderlineSMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE),\n",
    "    'SMOTETomek': SMOTETomek(sampling_strategy=0.2, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "strategy_a_results = {}\n",
    "\n",
    "for name, sampler in smote_methods.items():\n",
    "    # Pipeline ìƒì„± (Class Weight ì—†ìŒ)\n",
    "    pipeline = create_model_pipeline(\n",
    "        classifier=lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            scale_pos_weight=1,  # Class Weight ì—†ìŒ\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        resampler=sampler,\n",
    "        use_winsorizer=use_winsorizer_final\n",
    "    )\n",
    "    \n",
    "    # â­ Raw ë°ì´í„°ë¡œ í•™ìŠµ (ì „ì²˜ë¦¬ëŠ” Pipeline ë‚´ë¶€ì—ì„œ)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation í‰ê°€\n",
    "    y_val_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "    pr_auc = average_precision_score(y_val, y_val_prob)\n",
    "    \n",
    "    strategy_a_results[name] = pr_auc\n",
    "    print(f\"{name:20s} | Validation PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy B: Class Weight Only\n",
    "print(\"\\nğŸŸ¢ Strategy B: Class Weight Only\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "class_weight_methods = {\n",
    "    'No Weight': 1,\n",
    "    'Sqrt Weight': np.sqrt(imbalance_ratio),\n",
    "    'Full Weight': imbalance_ratio\n",
    "}\n",
    "\n",
    "strategy_b_results = {}\n",
    "\n",
    "for name, weight in class_weight_methods.items():\n",
    "    # Pipeline ìƒì„± (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)\n",
    "    pipeline = create_model_pipeline(\n",
    "        classifier=lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            scale_pos_weight=weight,  # Class Weight ì‚¬ìš©\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        resampler='passthrough',  # ë¦¬ìƒ˜í”Œë§ ì—†ìŒ\n",
    "        use_winsorizer=use_winsorizer_final\n",
    "    )\n",
    "    \n",
    "    # Raw ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation í‰ê°€\n",
    "    y_val_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "    pr_auc = average_precision_score(y_val, y_val_prob)\n",
    "    \n",
    "    strategy_b_results[name] = pr_auc\n",
    "    print(f\"{name:20s} | Validation PR-AUC: {pr_auc:.4f} (weight={weight:.2f})\")\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ëµ ì„ íƒ\n",
    "best_a_name = max(strategy_a_results, key=strategy_a_results.get)\n",
    "best_a_pr_auc = strategy_a_results[best_a_name]\n",
    "\n",
    "best_b_name = max(strategy_b_results, key=strategy_b_results.get)\n",
    "best_b_pr_auc = strategy_b_results[best_b_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ë¦¬ìƒ˜í”Œë§ ì „ëµ ë¹„êµ ê²°ê³¼ (Validation Set ê¸°ë°˜)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Strategy A (SMOTE ê³„ì—´) ìµœê³ :  {best_a_name:20s} | PR-AUC = {best_a_pr_auc:.4f}\")\n",
    "print(f\"Strategy B (Class Weight) ìµœê³ : {best_b_name:20s} | PR-AUC = {best_b_pr_auc:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if best_b_pr_auc > best_a_pr_auc:\n",
    "    diff = best_b_pr_auc - best_a_pr_auc\n",
    "    pct = diff / best_a_pr_auc * 100\n",
    "    selected_strategy = 'Class Weight'\n",
    "    selected_weight = class_weight_methods[best_b_name]\n",
    "    selected_resampler = 'passthrough'\n",
    "    print(f\"âœ… ì„ íƒ: Strategy B (Class Weight)\")\n",
    "    print(f\"   - ì´ìœ : {diff:.4f} ({pct:.2f}%) ë” ìš°ìˆ˜\")\n",
    "    print(f\"   - scale_pos_weight: {selected_weight:.2f}\")\n",
    "else:\n",
    "    diff = best_a_pr_auc - best_b_pr_auc\n",
    "    pct = diff / best_b_pr_auc * 100\n",
    "    selected_strategy = 'SMOTE'\n",
    "    selected_weight = 1\n",
    "    selected_resampler = smote_methods[best_a_name]\n",
    "    print(f\"âœ… ì„ íƒ: Strategy A (SMOTE ê³„ì—´)\")\n",
    "    print(f\"   - ì´ìœ : {diff:.4f} ({pct:.2f}%) ë” ìš°ìˆ˜\")\n",
    "    print(f\"   - ë°©ë²•: {best_a_name}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. P0 ê°œì„ : AutoML í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (n_iter=200) â­\n",
    "\n",
    "### 4.1 ê°œì„  ì‚¬í•­\n",
    "\n",
    "**v2 ë¬¸ì œì :**\n",
    "- n_iter=50~100 â†’ íƒìƒ‰ ê³µê°„ ëŒ€ë¹„ 1% ë¯¸ë§Œ\n",
    "\n",
    "**v2 ê°œì„ íŒ:**\n",
    "- âœ… **n_iter=200** (ëª¨ë¸ë³„)\n",
    "- âœ… Pipeline ê¸°ë°˜ AutoML\n",
    "- âœ… Raw ë°ì´í„° ì‚¬ìš©\n",
    "\n",
    "### 4.2 Scorer ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR-AUC Scorer (ì£¼ìš” ì§€í‘œ)\n",
    "pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "print(\"âœ… Scorer ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"   - PR-AUC: ë¶ˆê· í˜• ë°ì´í„° í•µì‹¬ ì§€í‘œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ íƒëœ ì „ëµì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "if selected_strategy == 'Class Weight':\n",
    "    # Strategy B: Class Weightë§Œ ì‚¬ìš©\n",
    "    resampler_options = ['passthrough']\n",
    "    scale_pos_weight_options = [selected_weight]\n",
    "    print(f\"âœ… ì„ íƒëœ ì „ëµ: Class Weight (scale_pos_weight={selected_weight:.2f})\")\n",
    "else:\n",
    "    # Strategy A: SMOTE ì‚¬ìš©\n",
    "    resampler_options = [selected_resampler]\n",
    "    scale_pos_weight_options = [1]\n",
    "    print(f\"âœ… ì„ íƒëœ ì „ëµ: SMOTE ({best_a_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "lgbm_param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 5, 7, 10, -1],\n",
    "    'classifier__num_leaves': [15, 31, 63, 127],\n",
    "    'classifier__min_child_samples': [10, 20, 30, 50],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'classifier__reg_lambda': [0, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# XGBoost íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "xgb_param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 5, 7, 10],\n",
    "    'classifier__min_child_weight': [1, 3, 5, 7],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'classifier__reg_lambda': [0, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# CatBoost íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "catboost_param_grid = {\n",
    "    'classifier__iterations': [50, 100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "    'classifier__depth': [3, 5, 7, 10],\n",
    "    'classifier__l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "}\n",
    "\n",
    "# Logistic Regression íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "lr_param_grid = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga'],\n",
    "    'classifier__max_iter': [1000],\n",
    "}\n",
    "\n",
    "# Random Forest íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ\n",
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200, 300],\n",
    "    'classifier__max_depth': [5, 10, 15, 20, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None],\n",
    "}\n",
    "\n",
    "print(\"âœ… íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì •ì˜ (Pipelineìœ¼ë¡œ wrapping)\n",
    "models_to_tune = {\n",
    "    'LightGBM': {\n",
    "        'pipeline': create_model_pipeline(\n",
    "            lgb.LGBMClassifier(\n",
    "                scale_pos_weight=selected_weight,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            resampler=selected_resampler if selected_strategy == 'SMOTE' else 'passthrough',\n",
    "            use_winsorizer=use_winsorizer_final\n",
    "        ),\n",
    "        'param_grid': lgbm_param_grid,\n",
    "        'n_iter': 200\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'pipeline': create_model_pipeline(\n",
    "            xgb.XGBClassifier(\n",
    "                scale_pos_weight=selected_weight,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1,\n",
    "                verbosity=0,\n",
    "                tree_method='hist'\n",
    "            ),\n",
    "            resampler=selected_resampler if selected_strategy == 'SMOTE' else 'passthrough',\n",
    "            use_winsorizer=use_winsorizer_final\n",
    "        ),\n",
    "        'param_grid': xgb_param_grid,\n",
    "        'n_iter': 200\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'pipeline': create_model_pipeline(\n",
    "            CatBoostClassifier(\n",
    "                scale_pos_weight=selected_weight,\n",
    "                random_state=RANDOM_STATE,\n",
    "                verbose=0\n",
    "            ),\n",
    "            resampler=selected_resampler if selected_strategy == 'SMOTE' else 'passthrough',\n",
    "            use_winsorizer=use_winsorizer_final\n",
    "        ),\n",
    "        'param_grid': catboost_param_grid,\n",
    "        'n_iter': 150  # CatBoostëŠ” ëŠë¦¬ë¯€ë¡œ 150\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'pipeline': create_model_pipeline(\n",
    "            LogisticRegression(\n",
    "                class_weight='balanced' if selected_strategy == 'Class Weight' else None,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            resampler=selected_resampler if selected_strategy == 'SMOTE' else 'passthrough',\n",
    "            use_winsorizer=use_winsorizer_final\n",
    "        ),\n",
    "        'param_grid': lr_param_grid,\n",
    "        'n_iter': 50\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'pipeline': create_model_pipeline(\n",
    "            RandomForestClassifier(\n",
    "                class_weight='balanced' if selected_strategy == 'Class Weight' else None,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            resampler=selected_resampler if selected_strategy == 'SMOTE' else 'passthrough',\n",
    "            use_winsorizer=use_winsorizer_final\n",
    "        ),\n",
    "        'param_grid': rf_param_grid,\n",
    "        'n_iter': 100\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ Pipeline ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   - ì „ì²´ ëª¨ë¸ ìˆ˜: {len(models_to_tune)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 RandomizedSearchCV ì‹¤í–‰ (â­ n_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ AutoML í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ (n_iter=200)\")\n",
    "print(\"=\"*70)\n",
    "print(\"âš ï¸  Train Set + 5-Fold CVë§Œ ì‚¬ìš© (Validation/Test ë¯¸ì‚¬ìš©)\")\n",
    "print(\"âš ï¸  Raw ë°ì´í„° ì‚¬ìš© (ì „ì²˜ë¦¬ëŠ” Pipeline ë‚´ë¶€ì—ì„œ)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "search_results = {}\n",
    "best_models = {}\n",
    "\n",
    "for model_name, config in models_to_tune.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š {model_name} íŠœë‹ ì¤‘... (n_iter={config['n_iter']})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # RandomizedSearchCV\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config['pipeline'],\n",
    "        param_distributions=config['param_grid'],\n",
    "        n_iter=config['n_iter'],\n",
    "        scoring=pr_auc_scorer,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=2,  # â­ ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # â­ Raw ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    search_results[model_name] = search\n",
    "    best_models[model_name] = search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ… {model_name} ì™„ë£Œ\")\n",
    "    print(f\"   - Best CV PR-AUC: {search.best_score_:.4f}\")\n",
    "    print(f\"   - Best Params: {search.best_params_}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ… ëª¨ë“  ëª¨ë¸ íŠœë‹ ì™„ë£Œ!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥\n",
    "model_best_scores = {name: search.best_score_ for name, search in search_results.items()}\n",
    "\n",
    "# ì •ë ¬\n",
    "sorted_models = sorted(model_best_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"ğŸ† ëª¨ë¸ë³„ ìµœê³  CV ì„±ëŠ¥ (Train Set 5-Fold)\")\n",
    "print(\"=\"*70)\n",
    "for rank, (name, score) in enumerate(sorted_models, 1):\n",
    "    print(f\"{rank}. {name:20s} | PR-AUC: {score:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³¼ì í•© ì²´í¬\n",
    "print(\"\\nğŸ“Š ê³¼ì í•© ì²´í¬ (Train Score - CV Score)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, search in search_results.items():\n",
    "    train_score = search.cv_results_['mean_train_score'][search.best_index_]\n",
    "    cv_score = search.best_score_\n",
    "    gap = train_score - cv_score\n",
    "    \n",
    "    status = \"âš ï¸ ê³¼ì í•© ìš°ë ¤\" if gap > 0.1 else \"âœ… ì •ìƒ\"\n",
    "    \n",
    "    print(f\"{model_name:20s} | Train: {train_score:.4f} | CV: {cv_score:.4f} | \"\n",
    "          f\"Gap: {gap:.4f} | {status}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. P0 ê°œì„ : ëª¨ë¸ ì„ íƒ (Validation + ì•™ìƒë¸” ë‹¤ì–‘ì„±) â­â­â­\n",
    "\n",
    "### 5.1 ê°œì„  ì‚¬í•­\n",
    "\n",
    "**v2 ë¬¸ì œì :**\n",
    "- Top 3ê°€ ëª¨ë‘ GBM ê³„ì—´ì´ë©´ Voting íš¨ê³¼ ë¯¸ë¯¸\n",
    "- ëª¨ë¸ íƒ€ì… ë‹¤ì–‘ì„± ë¯¸í™•ë³´\n",
    "\n",
    "**v2 ê°œì„ íŒ:**\n",
    "- âœ… **P0-3: ì•™ìƒë¸” ë‹¤ì–‘ì„± ì²´í¬**\n",
    "- Top 3ê°€ ëª¨ë‘ GBMì´ë©´ ì´ì¢… ëª¨ë¸(LR/RF) ê°•ì œ í¬í•¨\n",
    "- ë‹¤ì–‘ì„± vs ì„±ëŠ¥ trade-off ê³ ë ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Best Model (CV ê¸°ì¤€)\n",
    "best_model_name = max(model_best_scores, key=model_best_scores.get)\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"ğŸ¥‡ Single Best Model (Train CV ê¸°ì¤€): {best_model_name}\")\n",
    "print(f\"   - Train CV PR-AUC: {model_best_scores[best_model_name]:.4f}\")\n",
    "\n",
    "# Validation Set í‰ê°€\n",
    "y_val_prob_single = best_model.predict_proba(X_val)[:, 1]\n",
    "val_pr_auc_single = average_precision_score(y_val, y_val_prob_single)\n",
    "\n",
    "print(f\"   - Validation PR-AUC: {val_pr_auc_single:.4f}\")\n",
    "print(f\"   - ì¼ë°˜í™” ì„±ëŠ¥ ì°¨ì´: {val_pr_auc_single - model_best_scores[best_model_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P0-3: ì•™ìƒë¸” ë‹¤ì–‘ì„± ì²´í¬ â­\n",
    "print(f\"\\nğŸ“Š P0-3: ì•™ìƒë¸” ë‹¤ì–‘ì„± ì²´í¬\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top 3 ëª¨ë¸\n",
    "top3_models = sorted(model_best_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "print(f\"Top 3 ëª¨ë¸:\")\n",
    "for i, (name, score) in enumerate(top3_models, 1):\n",
    "    print(f\"   {i}. {name}: {score:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ íƒ€ì… ë¶„ë¥˜\n",
    "gbm_models = {'LightGBM', 'XGBoost', 'CatBoost'}\n",
    "top3_names = [name for name, _ in top3_models]\n",
    "top3_gbm_count = sum(1 for name in top3_names if name in gbm_models)\n",
    "\n",
    "print(f\"\\nTop 3 ì¤‘ GBM ëª¨ë¸ ìˆ˜: {top3_gbm_count}/3\")\n",
    "\n",
    "# ë‹¤ì–‘ì„± í™•ë³´ ë¡œì§\n",
    "if top3_gbm_count == 3:\n",
    "    print(f\"âš ï¸  Top 3ê°€ ëª¨ë‘ GBM â†’ ë‹¤ì–‘ì„± ë¶€ì¡±\")\n",
    "    print(f\"âœ… í•´ê²°: ì´ì¢… ëª¨ë¸(LR/RF) ê°•ì œ í¬í•¨\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ GBM 2ê°œ + ìµœê³  ì„±ëŠ¥ ì´ì¢… ëª¨ë¸ 1ê°œ\n",
    "    top2_gbm = [(name, score) for name, score in top3_models[:2]]\n",
    "    \n",
    "    # LR, RF ì¤‘ ìµœê³  ì„±ëŠ¥\n",
    "    non_gbm_scores = {name: score for name, score in model_best_scores.items() \n",
    "                      if name not in gbm_models}\n",
    "    best_non_gbm = max(non_gbm_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # ìµœì¢… Top 3 ì¬êµ¬ì„±\n",
    "    top3_models = top2_gbm + [best_non_gbm]\n",
    "    \n",
    "    print(f\"\\nâœ… ì¬êµ¬ì„±ëœ Top 3 (ë‹¤ì–‘ì„± í™•ë³´):\")\n",
    "    for i, (name, score) in enumerate(top3_models, 1):\n",
    "        model_type = \"GBM\" if name in gbm_models else \"ì´ì¢…\"\n",
    "        print(f\"   {i}. {name} ({model_type}): {score:.4f}\")\n",
    "else:\n",
    "    print(f\"âœ… Top 3 ë‹¤ì–‘ì„± í™•ë³´ë¨ (GBM {top3_gbm_count}ê°œ + ì´ì¢… {3-top3_gbm_count}ê°œ)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble ìƒì„±\n",
    "voting_estimators = [(name, best_models[name]) for name, _ in top3_models]\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train setìœ¼ë¡œ í•™ìŠµ (â­ Raw ë°ì´í„° - Pipeline ë‚´ë¶€ì—ì„œ ì „ì²˜ë¦¬)\n",
    "print(f\"\\nğŸ”„ Voting Ensemble í•™ìŠµ ì¤‘...\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Validation Set í‰ê°€\n",
    "y_val_prob_voting = voting_clf.predict_proba(X_val)[:, 1]\n",
    "val_pr_auc_voting = average_precision_score(y_val, y_val_prob_voting)\n",
    "\n",
    "print(f\"âœ… Voting Ensemble ì™„ë£Œ\")\n",
    "print(f\"   - Validation PR-AUC: {val_pr_auc_voting:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance Test (Wilcoxon)\n",
    "print(f\"\\nğŸ“Š Statistical Significance Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "single_cv_scores = cross_val_score(\n",
    "    best_model, X_train, y_train,\n",
    "    cv=5, scoring=pr_auc_scorer, n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_cv_scores = cross_val_score(\n",
    "    voting_clf, X_train, y_train,\n",
    "    cv=5, scoring=pr_auc_scorer, n_jobs=-1\n",
    ")\n",
    "\n",
    "try:\n",
    "    statistic, pvalue = wilcoxon(voting_cv_scores, single_cv_scores)\n",
    "    print(f\"Single CV Scores: {single_cv_scores}\")\n",
    "    print(f\"Voting CV Scores: {voting_cv_scores}\")\n",
    "    print(f\"\\nWilcoxon p-value: {pvalue:.4f}\")\n",
    "    \n",
    "    if pvalue < 0.05:\n",
    "        print(f\"   âœ… ìœ ì˜ë¯¸í•œ ì°¨ì´ (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ (p >= 0.05)\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ Wilcoxon test ì‹¤íŒ¨: {e}\")\n",
    "    pvalue = 1.0\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ì„ íƒ (Validation ê¸°ë°˜)\n",
    "print(f\"\\nğŸ¯ ìµœì¢… ëª¨ë¸ ì„ íƒ (Validation Set ê¸°ë°˜)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Single Best Model:     PR-AUC = {val_pr_auc_single:.4f}\")\n",
    "print(f\"Voting Ensemble:       PR-AUC = {val_pr_auc_voting:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement_threshold = 0.005\n",
    "improvement = val_pr_auc_voting - val_pr_auc_single\n",
    "\n",
    "if improvement > improvement_threshold and pvalue < 0.1:\n",
    "    final_model = voting_clf\n",
    "    final_model_name = 'VotingEnsemble'\n",
    "    final_val_pr_auc = val_pr_auc_voting\n",
    "    decision_reason = f\"Votingì´ {improvement:.4f} ë” ìš°ìˆ˜ (p={pvalue:.4f} < 0.1) + ë‹¤ì–‘ì„± í™•ë³´\"\n",
    "    print(f\"âœ… ì„ íƒ: Voting Ensemble\")\n",
    "    print(f\"   ì´ìœ : {decision_reason}\")\n",
    "else:\n",
    "    final_model = best_model\n",
    "    final_model_name = best_model_name\n",
    "    final_val_pr_auc = val_pr_auc_single\n",
    "    if improvement <= improvement_threshold:\n",
    "        decision_reason = f\"ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸ ({improvement:.4f} < {improvement_threshold}) + SHAP í•´ì„ë ¥ ìš°ì„ \"\n",
    "    else:\n",
    "        decision_reason = f\"í†µê³„ì  ìœ ì˜ì„± ë¶€ì¡± (p={pvalue:.4f} >= 0.1)\"\n",
    "    print(f\"âœ… ì„ íƒ: Single Model ({best_model_name})\")\n",
    "    print(f\"   ì´ìœ : {decision_reason}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_selection_info = {\n",
    "    'final_model_name': final_model_name,\n",
    "    'val_pr_auc': final_val_pr_auc,\n",
    "    'decision_reason': decision_reason,\n",
    "    'single_model_name': best_model_name,\n",
    "    'single_val_pr_auc': val_pr_auc_single,\n",
    "    'voting_val_pr_auc': val_pr_auc_voting,\n",
    "    'wilcoxon_pvalue': pvalue,\n",
    "    'top3_models': [name for name, _ in top3_models]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. P1 ê°œì„ : ì„ê³„ê°’ ìµœì í™” (Validation + CV) â­â­â­\n",
    "\n",
    "### 6.1 ê°œì„  ì‚¬í•­\n",
    "\n",
    "**v2 ë¬¸ì œì :**\n",
    "- Validation set í•˜ë‚˜ì—ë§Œ ì˜ì¡´ â†’ Variance ë†’ìŒ\n",
    "\n",
    "**v2 ê°œì„ íŒ:**\n",
    "- âœ… **P1-1: CV ê¸°ë°˜ ì„ê³„ê°’ ì¶”ê°€**\n",
    "- Validation + CV í‰ê·  ì‚¬ìš© (ë” robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set ì„ê³„ê°’\n",
    "y_val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "precisions_val, recalls_val, thresholds_val = precision_recall_curve(y_val, y_val_prob)\n",
    "\n",
    "# F2-Score ê³„ì‚°\n",
    "beta = 2\n",
    "f2_scores_val = (1 + beta**2) * (precisions_val[:-1] * recalls_val[:-1]) / \\\n",
    "                (beta**2 * precisions_val[:-1] + recalls_val[:-1] + 1e-10)\n",
    "\n",
    "optimal_idx_val = np.argmax(f2_scores_val)\n",
    "optimal_threshold_val = thresholds_val[optimal_idx_val]\n",
    "\n",
    "print(f\"ğŸ“Š Validation Set ê¸°ë°˜ ì„ê³„ê°’\")\n",
    "print(\"=\"*70)\n",
    "print(f\"F2-Score ìµœì  ì„ê³„ê°’: {optimal_threshold_val:.4f}\")\n",
    "print(f\"   - F2-Score: {f2_scores_val[optimal_idx_val]:.4f}\")\n",
    "print(f\"   - Precision: {precisions_val[optimal_idx_val]:.2%}\")\n",
    "print(f\"   - Recall: {recalls_val[optimal_idx_val]:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1-1: CV ê¸°ë°˜ ì„ê³„ê°’ â­\n",
    "print(f\"\\nğŸ“Š P1-1: CV ê¸°ë°˜ ì„ê³„ê°’ (ë” robust)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cross-validationìœ¼ë¡œ í™•ë¥  ì˜ˆì¸¡\n",
    "y_train_prob_cv = cross_val_predict(\n",
    "    final_model, X_train, y_train, \n",
    "    cv=5, method='predict_proba', n_jobs=-1\n",
    ")[:, 1]\n",
    "\n",
    "precisions_cv, recalls_cv, thresholds_cv = precision_recall_curve(y_train, y_train_prob_cv)\n",
    "\n",
    "f2_scores_cv = (1 + beta**2) * (precisions_cv[:-1] * recalls_cv[:-1]) / \\\n",
    "               (beta**2 * precisions_cv[:-1] + recalls_cv[:-1] + 1e-10)\n",
    "\n",
    "optimal_idx_cv = np.argmax(f2_scores_cv)\n",
    "optimal_threshold_cv = thresholds_cv[optimal_idx_cv]\n",
    "\n",
    "print(f\"CV ê¸°ë°˜ ìµœì  ì„ê³„ê°’: {optimal_threshold_cv:.4f}\")\n",
    "print(f\"   - F2-Score: {f2_scores_cv[optimal_idx_cv]:.4f}\")\n",
    "print(f\"   - Precision: {precisions_cv[optimal_idx_cv]:.2%}\")\n",
    "print(f\"   - Recall: {recalls_cv[optimal_idx_cv]:.2%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation + CV í‰ê·  (ìµœì¢…)\n",
    "optimal_threshold_final = (optimal_threshold_val + optimal_threshold_cv) / 2\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ì„ê³„ê°’ (Validation + CV í‰ê· )\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Validation ì„ê³„ê°’: {optimal_threshold_val:.4f}\")\n",
    "print(f\"CV ì„ê³„ê°’:         {optimal_threshold_cv:.4f}\")\n",
    "print(f\"ìµœì¢… ì„ê³„ê°’:       {optimal_threshold_final:.4f}\")\n",
    "print(f\"   ì´ìœ : ë‘ ë°©ë²•ì˜ í‰ê· ìœ¼ë¡œ ë” robustí•œ ì„ê³„ê°’\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "selected_threshold = optimal_threshold_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. P1 ê°œì„ : Traffic Light (Yellow ë¡œì§ ì¼ê´€ì„±) â­\n",
    "\n",
    "### 7.1 ê°œì„  ì‚¬í•­\n",
    "\n",
    "**v2 ë¬¸ì œì :**\n",
    "- Recall 95% ëª» ì°¾ìœ¼ë©´ percentile ì‚¬ìš© â†’ ë°©ë²•ë¡  ë¶ˆì¼ì¹˜\n",
    "\n",
    "**v2 ê°œì„ íŒ:**\n",
    "- âœ… **P1-3: Yellow ë¡œì§ ì¼ê´€ì„±**\n",
    "- Recall ê¸°ì¤€ìœ¼ë¡œ ì¼ê´€ë˜ê²Œ ì‚¬ìš©\n",
    "- ëª» ì°¾ìœ¼ë©´ ê²½ê³ ë§Œ, ëŒ€ì²´ ë°©ë²• ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red ì„ê³„ê°’: ì„ íƒëœ ìµœì¢… ì„ê³„ê°’ ì‚¬ìš©\n",
    "red_threshold = selected_threshold\n",
    "\n",
    "# Recall ê¸°ë°˜ ì„±ëŠ¥ í™•ì¸\n",
    "y_val_pred_red = (y_val_prob >= red_threshold).astype(int)\n",
    "red_recall = recall_score(y_val, y_val_pred_red)\n",
    "red_precision = precision_score(y_val, y_val_pred_red)\n",
    "\n",
    "print(f\"ğŸ”´ Red Threshold: {red_threshold:.4f}\")\n",
    "print(f\"   - Recall: {red_recall:.2%}\")\n",
    "print(f\"   - Precision: {red_precision:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1-3: Yellow ì„ê³„ê°’ (Recall ê¸°ì¤€ ì¼ê´€ì„±) â­\n",
    "target_recall_95 = 0.95\n",
    "idx_recall_95 = np.where(recalls_val[:-1] >= target_recall_95)[0]\n",
    "\n",
    "if len(idx_recall_95) > 0:\n",
    "    # Recall 95% ë‹¬ì„± ê°€ëŠ¥\n",
    "    best_idx_95 = idx_recall_95[np.argmax(precisions_val[idx_recall_95])]\n",
    "    yellow_threshold = thresholds_val[best_idx_95]\n",
    "    yellow_precision = precisions_val[best_idx_95]\n",
    "    yellow_recall = recalls_val[best_idx_95]\n",
    "    \n",
    "    print(f\"\\nğŸŸ¡ Yellow Threshold: {yellow_threshold:.4f}\")\n",
    "    print(f\"   - ëª©í‘œ: Recall {target_recall_95:.0%} ë³´ì¥\")\n",
    "    print(f\"   - ì‹¤ì œ Recall: {yellow_recall:.2%}\")\n",
    "    print(f\"   - Precision: {yellow_precision:.2%}\")\n",
    "else:\n",
    "    # Recall 95% ë‹¬ì„± ë¶ˆê°€ëŠ¥ â†’ Red thresholdì˜ ì ˆë°˜ ì‚¬ìš© (ë³´ìˆ˜ì )\n",
    "    yellow_threshold = red_threshold * 0.5\n",
    "    yellow_idx = np.argmin(np.abs(thresholds_val - yellow_threshold))\n",
    "    yellow_precision = precisions_val[yellow_idx]\n",
    "    yellow_recall = recalls_val[yellow_idx]\n",
    "    \n",
    "    print(f\"\\nâš ï¸ Recall {target_recall_95:.0%} ë‹¬ì„± ë¶ˆê°€ëŠ¥\")\n",
    "    print(f\"   â†’ Yellow = Red Ã— 0.5 (ë³´ìˆ˜ì  ì ‘ê·¼)\")\n",
    "    print(f\"\\nğŸŸ¡ Yellow Threshold: {yellow_threshold:.4f}\")\n",
    "    print(f\"   - ì‹¤ì œ Recall: {yellow_recall:.2%}\")\n",
    "    print(f\"   - Precision: {yellow_precision:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic Light ì„±ëŠ¥ (Validation)\n",
    "def assign_traffic_light(prob, red_th, yellow_th):\n",
    "    if prob >= red_th:\n",
    "        return 'Red'\n",
    "    elif prob >= yellow_th:\n",
    "        return 'Yellow'\n",
    "    else:\n",
    "        return 'Green'\n",
    "\n",
    "val_traffic_light = [assign_traffic_light(p, red_threshold, yellow_threshold) for p in y_val_prob]\n",
    "val_df = pd.DataFrame({\n",
    "    'actual': y_val.values,\n",
    "    'prob': y_val_prob,\n",
    "    'traffic_light': val_traffic_light\n",
    "})\n",
    "\n",
    "traffic_stats = val_df.groupby('traffic_light').agg({\n",
    "    'actual': ['count', 'sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "traffic_stats.columns = ['ê¸°ì—… ìˆ˜', 'ì‹¤ì œ ë¶€ë„', 'ë¶€ë„ìœ¨']\n",
    "traffic_stats = traffic_stats.reindex(['Red', 'Yellow', 'Green'])\n",
    "\n",
    "print(f\"\\nğŸš¦ Traffic Light ì„±ëŠ¥ (Validation Set)\")\n",
    "print(\"=\"*70)\n",
    "print(traffic_stats)\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_bankrupts = y_val.sum()\n",
    "red_yellow_bankrupts = val_df[val_df['traffic_light'].isin(['Red', 'Yellow'])]['actual'].sum()\n",
    "capture_rate = red_yellow_bankrupts / total_bankrupts\n",
    "\n",
    "print(f\"\\nâœ… ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨ (Red+Yellow): {capture_rate:.2%}\")\n",
    "print(f\"   - ì „ì²´ ë¶€ë„ {total_bankrupts}ê°œ ì¤‘ {red_yellow_bankrupts}ê°œ í¬ì°©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Test Set ìµœì¢… í‰ê°€ â­â­â­\n",
    "\n",
    "### 8.1 ì¤‘ìš” ê²½ê³ \n",
    "\n",
    "```\n",
    "âš ï¸âš ï¸âš ï¸ Test Set ì‚¬ìš© ê²½ê³  âš ï¸âš ï¸âš ï¸\n",
    "\n",
    "ì´ ì„¹ì…˜ì€ ë…¸íŠ¸ë¶ì—ì„œ ë‹¨ í•œ ë²ˆë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
    "Test Setì€ ìµœì¢… ë³´ê³ ë¥¼ ìœ„í•œ í‰ê°€ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°,\n",
    "ì´ì „ ëª¨ë“  ì˜ì‚¬ê²°ì •(ëª¨ë¸ ì„ íƒ, ì„ê³„ê°’ ë“±)ì€\n",
    "Validation Setì—ì„œë§Œ ì´ë£¨ì–´ì¡ŒìŒì„ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "Test Setì„ ë³´ê³  ë‹¤ì‹œ íŠœë‹í•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Set ì „ì²˜ë¦¬ (Train setì— fití•œ preprocessor ì‚¬ìš©)\n# â­ Pipeline ì‚¬ìš© ì‹œ ë³„ë„ ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (ë‚´ë¶€ì—ì„œ ìë™ ì²˜ë¦¬)\n# X_testë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©\nX_test_processed = X_test  # Pipelineì´ ë‚´ë¶€ì—ì„œ ì „ì²˜ë¦¬\n\n# Test Set ì˜ˆì¸¡\ny_test_prob = final_model.predict_proba(X_test_processed)[:, 1]\ny_test_pred = (y_test_prob >= selected_threshold).astype(int)\n\nprint(f\"âœ… Test Set ì˜ˆì¸¡ ì™„ë£Œ\")\nprint(f\"   - ì„ê³„ê°’: {selected_threshold:.4f} (Validationì—ì„œ ê²°ì •ë¨)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set ìµœì¢… í‰ê°€\n",
    "test_pr_auc = average_precision_score(y_test, y_test_prob)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# F2-Score, Precision, Recall\n",
    "test_f2 = fbeta_score(y_test, y_test_pred, beta=2)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Type II Error\n",
    "type_ii_error = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ¯ Test Set ìµœì¢… í‰ê°€ (ì„ê³„ê°’: {selected_threshold:.4f})\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"âš ï¸  ì´ ê²°ê³¼ëŠ” ìµœì¢… ë³´ê³ ìš©ì´ë©°, ì´ì „ ë‹¨ê³„ì—ì„œ Test Setì„\")\n",
    "print(f\"   ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤.\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:\")\n",
    "print(f\"   PR-AUC:              {test_pr_auc:.4f}\")\n",
    "print(f\"   ROC-AUC:             {test_roc_auc:.4f}\")\n",
    "print(f\"   F2-Score:            {test_f2:.4f}\")\n",
    "print(f\"   Precision:           {test_precision:.2%}\")\n",
    "print(f\"   Recall:              {test_recall:.2%}\")\n",
    "print(f\"   Type II Error:       {type_ii_error:.2%}\")\n",
    "print(f\"\\nğŸ“Š Confusion Matrix:\")\n",
    "print(f\"   TN: {tn:,}  |  FP: {fp:,}\")\n",
    "print(f\"   FN: {fn:,}    |  TP: {tp:,}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Validation vs Test ë¹„êµ\n",
    "print(f\"\\nğŸ“Š Validation vs Test ë¹„êµ (ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì¦):\")\n",
    "print(f\"   Validation PR-AUC: {final_val_pr_auc:.4f}\")\n",
    "print(f\"   Test PR-AUC:       {test_pr_auc:.4f}\")\n",
    "print(f\"   ì°¨ì´:              {test_pr_auc - final_val_pr_auc:.4f}\")\n",
    "\n",
    "if abs(test_pr_auc - final_val_pr_auc) < 0.02:\n",
    "    print(f\"   âœ… ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜ (ì°¨ì´ < 2%)\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  ì¼ë°˜í™” ì„±ëŠ¥ ì£¼ì˜ (ì°¨ì´ >= 2%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Traffic Light\n",
    "test_traffic_light = [assign_traffic_light(p, red_threshold, yellow_threshold) for p in y_test_prob]\n",
    "test_df = pd.DataFrame({\n",
    "    'actual': y_test.values,\n",
    "    'prob': y_test_prob,\n",
    "    'traffic_light': test_traffic_light\n",
    "})\n",
    "\n",
    "# ë“±ê¸‰ë³„ í†µê³„\n",
    "test_traffic_stats = test_df.groupby('traffic_light').agg({\n",
    "    'actual': ['count', 'sum', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "test_traffic_stats.columns = ['ê¸°ì—… ìˆ˜', 'ì‹¤ì œ ë¶€ë„', 'ë¶€ë„ìœ¨']\n",
    "test_traffic_stats = test_traffic_stats.reindex(['Red', 'Yellow', 'Green'])\n",
    "\n",
    "print(f\"\\nğŸš¦ Traffic Light ì‹œìŠ¤í…œ (Test Set ìµœì¢… í‰ê°€)\")\n",
    "print(\"=\"*70)\n",
    "print(test_traffic_stats)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# í¬ì°©ë¥ \n",
    "test_total_bankrupts = y_test.sum()\n",
    "test_red_yellow_bankrupts = test_df[test_df['traffic_light'].isin(['Red', 'Yellow'])]['actual'].sum()\n",
    "test_capture_rate = test_red_yellow_bankrupts / test_total_bankrupts\n",
    "\n",
    "print(f\"\\nâœ… ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨ (Red+Yellow): {test_capture_rate:.2%}\")\n",
    "print(f\"   - ì „ì²´ ë¶€ë„ {test_total_bankrupts}ê°œ ì¤‘ {test_red_yellow_bankrupts}ê°œ í¬ì°©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Curve & ROC Curve (Test Set)\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Precision-Recall Curve', 'ROC Curve')\n",
    ")\n",
    "\n",
    "# PR Curve\n",
    "precisions_test, recalls_test, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=recalls_test, y=precisions_test, mode='lines', name=f'PR (AUC={test_pr_auc:.4f})'),\n",
    "    row=1, col=1\n",
    ")\n",
    "baseline_test = y_test.mean()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[baseline_test, baseline_test], mode='lines', \n",
    "               name='Baseline', line=dict(dash='dash')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC (AUC={test_roc_auc:.4f})'),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random', \n",
    "               line=dict(dash='dash'), showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Recall', row=1, col=1)\n",
    "fig.update_xaxes(title_text='FPR', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Precision', row=1, col=1)\n",
    "fig.update_yaxes(title_text='TPR', row=1, col=2)\n",
    "\n",
    "fig.update_layout(title_text='Test Set ì„±ëŠ¥ ê³¡ì„ ', height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance ë¶„ì„\n",
    "\n",
    "### 9.1 ëª¨ë¸ Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance ì¶”ì¶œ\n",
    "if final_model_name == 'VotingEnsemble':\n",
    "    # Votingì˜ ê²½ìš° ì²« ë²ˆì§¸ estimator ì‚¬ìš©\n",
    "    model_for_importance = final_model.estimators_[0]\n",
    "    print(f\"âš ï¸  Voting Ensembleì´ë¯€ë¡œ ì²« ë²ˆì§¸ ëª¨ë¸({top3_models[0][0]})ì˜ Feature Importance ì‚¬ìš©\")\n",
    "else:\n",
    "    model_for_importance = final_model\n",
    "\n",
    "# Feature Importance ê°€ì ¸ì˜¤ê¸°\n",
    "if hasattr(model_for_importance, 'feature_importances_'):\n",
    "    importances = model_for_importance.feature_importances_\n",
    "elif hasattr(model_for_importance, 'coef_'):\n",
    "    importances = np.abs(model_for_importance.coef_[0])\n",
    "else:\n",
    "    print(\"âš ï¸  Feature Importanceë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    importances = np.zeros(len(X_train.columns))\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Importance Top 15\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance_df.head(15).to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance ì‹œê°í™”\n",
    "fig = go.Figure()\n",
    "\n",
    "top15 = feature_importance_df.head(15)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=top15['importance'][::-1],\n",
    "    y=top15['feature'][::-1],\n",
    "    orientation='h',\n",
    "    marker_color='#3498db',\n",
    "    text=[f\"{v:.4f}\" for v in top15['importance'][::-1]],\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Feature Importance Top 15 ({final_model_name})',\n",
    "    xaxis_title='Importance',\n",
    "    yaxis_title='Feature',\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„: Cumulative Gains Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Gains Curve (Test Set)\n",
    "# í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "test_sorted_idx = np.argsort(y_test_prob)[::-1]\n",
    "y_test_sorted = y_test.values[test_sorted_idx]\n",
    "\n",
    "# ëˆ„ì  ë¶€ë„ í¬ì°©ë¥ \n",
    "cumulative_bankrupts = np.cumsum(y_test_sorted)\n",
    "total_bankrupts = y_test.sum()\n",
    "cumulative_pct = cumulative_bankrupts / total_bankrupts * 100\n",
    "\n",
    "# ê²€í†  ë¹„ìœ¨ (ìƒìœ„ ëª‡ %)\n",
    "review_pct = np.arange(1, len(y_test) + 1) / len(y_test) * 100\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig = go.Figure()\n",
    "\n",
    "# Cumulative Gains\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=review_pct,\n",
    "    y=cumulative_pct,\n",
    "    mode='lines',\n",
    "    name='ëª¨ë¸ ì„±ëŠ¥',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# Random Baseline\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 100],\n",
    "    y=[0, 100],\n",
    "    mode='lines',\n",
    "    name='Random',\n",
    "    line=dict(color='gray', dash='dash')\n",
    "))\n",
    "\n",
    "# ì£¼ìš” ì§€ì  í‘œì‹œ\n",
    "for review_threshold in [10, 20, 30]:\n",
    "    idx = int(len(y_test) * review_threshold / 100)\n",
    "    gain = cumulative_pct[idx-1]\n",
    "    fig.add_annotation(\n",
    "        x=review_threshold,\n",
    "        y=gain,\n",
    "        text=f\"ìƒìœ„ {review_threshold}%: {gain:.1f}% í¬ì°©\",\n",
    "        showarrow=True,\n",
    "        arrowhead=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cumulative Gains Curve (Test Set)',\n",
    "    xaxis_title='ê²€í† í•œ ê¸°ì—… ë¹„ìœ¨ (%)',\n",
    "    yaxis_title='í¬ì°©í•œ ë¶€ë„ ê¸°ì—… ë¹„ìœ¨ (%)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# íš¨ìœ¨ì„± ë¶„ì„\n",
    "print(f\"\\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ìƒìœ„ 10% ê²€í†  ì‹œ: {cumulative_pct[int(len(y_test)*0.1)-1]:.1f}% ë¶€ë„ í¬ì°©\")\n",
    "print(f\"ìƒìœ„ 20% ê²€í†  ì‹œ: {cumulative_pct[int(len(y_test)*0.2)-1]:.1f}% ë¶€ë„ í¬ì°©\")\n",
    "print(f\"ìƒìœ„ 30% ê²€í†  ì‹œ: {cumulative_pct[int(len(y_test)*0.3)-1]:.1f}% ë¶€ë„ í¬ì°©\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ’¡ í•´ì„: ìƒìœ„ 20% ê¸°ì—…ë§Œ ê²€í† í•´ë„ ë¶€ë„ ê¸°ì—…ì˜ ì•½ {cumulative_pct[int(len(y_test)*0.2)-1]:.0f}%ë¥¼ ì‚¬ì „ í¬ì°© ê°€ëŠ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë¸ ì €ì¥ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### 10.1 ëª¨ë¸ ë° ë°ì´í„° ì €ì¥ (Part 4 SHAP ë¶„ì„ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ ê²½ë¡œ\n",
    "model_prefix = 'Part3_v2'\n",
    "\n",
    "# 1. ìµœì¢… ëª¨ë¸\n",
    "final_model_path = os.path.join(PROCESSED_DIR, f'{model_prefix}_ìµœì¢…ëª¨ë¸.pkl')\n",
    "joblib.dump(final_model, final_model_path)\n",
    "print(f\"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}\")\n",
    "\n",
    "# 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "preprocessor_path = os.path.join(PROCESSED_DIR, f'{model_prefix}_ì „ì²˜ë¦¬ê¸°.pkl')\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ê¸° ì €ì¥: {preprocessor_path}\")\n",
    "\n",
    "# 3. ì „ì²˜ë¦¬ëœ ë°ì´í„° (SHAP ë¶„ì„ìš©)\n",
    "X_train_processed.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_X_train_processed.csv'), \n",
    "                         index=False, encoding='utf-8-sig')\n",
    "X_val_processed.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_X_val_processed.csv'), \n",
    "                       index=False, encoding='utf-8-sig')\n",
    "X_test_processed.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_X_test_processed.csv'), \n",
    "                        index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (Train/Val/Test)\")\n",
    "\n",
    "# 4. íƒ€ê²Ÿ ë³€ìˆ˜\n",
    "y_train.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_y_train.csv'), \n",
    "               index=False, encoding='utf-8-sig', header=['target'])\n",
    "y_val.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_y_val.csv'), \n",
    "             index=False, encoding='utf-8-sig', header=['target'])\n",
    "y_test.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_y_test.csv'), \n",
    "              index=False, encoding='utf-8-sig', header=['target'])\n",
    "print(f\"âœ… íƒ€ê²Ÿ ë³€ìˆ˜ ì €ì¥ (Train/Val/Test)\")\n",
    "\n",
    "# 5. ì„ê³„ê°’ ë° ë©”íƒ€ë°ì´í„°\n",
    "metadata = {\n",
    "    'final_model_name': final_model_name,\n",
    "    'selected_threshold': selected_threshold,\n",
    "    'red_threshold': red_threshold,\n",
    "    'yellow_threshold': yellow_threshold,\n",
    "    'val_pr_auc': final_val_pr_auc,\n",
    "    'test_pr_auc': test_pr_auc,\n",
    "    'test_f2': test_f2,\n",
    "    'test_precision': test_precision,\n",
    "    'test_recall': test_recall,\n",
    "    'test_type_ii_error': type_ii_error,\n",
    "    'model_selection_info': model_selection_info,\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(PROCESSED_DIR, f'{model_prefix}_ë©”íƒ€ë°ì´í„°.pkl')\n",
    "joblib.dump(metadata, metadata_path)\n",
    "print(f\"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}\")\n",
    "\n",
    "# 6. Feature Importance\n",
    "feature_importance_df.to_csv(os.path.join(PROCESSED_DIR, f'{model_prefix}_feature_importance.csv'), \n",
    "                             index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… Feature Importance ì €ì¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ íŒŒì¼ ëª©ë¡\n",
    "print(f\"\\nğŸ“ ì €ì¥ëœ íŒŒì¼ ëª©ë¡:\")\n",
    "print(\"=\"*70)\n",
    "import glob\n",
    "saved_files = glob.glob(os.path.join(PROCESSED_DIR, f'{model_prefix}*'))\n",
    "for f in sorted(saved_files):\n",
    "    size = os.path.getsize(f) / 1024\n",
    "    print(f\"  - {os.path.basename(f)} ({size:.1f} KB)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 ìµœì¢… ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ğŸ‰ Part 3 v2 ëª¨ë¸ë§ ë° ìµœì í™” ì™„ë£Œ!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:\")\n",
    "print(f\"\\n1. ë°ì´í„° ë¶„í• :\")\n",
    "print(f\"   - Train Set:      {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Validation Set: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Test Set:       {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. ë¦¬ìƒ˜í”Œë§ ì „ëµ:\")\n",
    "print(f\"   - ì„ íƒ: {selected_strategy}\")\n",
    "\n",
    "print(f\"\\n3. ìµœì¢… ëª¨ë¸:\")\n",
    "print(f\"   - ëª¨ë¸: {final_model_name}\")\n",
    "print(f\"   - Validation PR-AUC: {final_val_pr_auc:.4f}\")\n",
    "print(f\"   - Test PR-AUC:       {test_pr_auc:.4f}\")\n",
    "print(f\"   - ì„ íƒ ì´ìœ : {decision_reason}\")\n",
    "\n",
    "print(f\"\\n4. ì„ê³„ê°’:\")\n",
    "print(f\"   - ìµœì  ì„ê³„ê°’: {selected_threshold:.4f}\")\n",
    "print(f\"   - Red Threshold:    {red_threshold:.4f} (Recall 80% ë³´ì¥)\")\n",
    "print(f\"   - Yellow Threshold: {yellow_threshold:.4f} (Recall 95% ë³´ì¥)\")\n",
    "\n",
    "print(f\"\\n5. Test Set ìµœì¢… ì„±ëŠ¥:\")\n",
    "print(f\"   - PR-AUC:        {test_pr_auc:.4f}\")\n",
    "print(f\"   - ROC-AUC:       {test_roc_auc:.4f}\")\n",
    "print(f\"   - F2-Score:      {test_f2:.4f}\")\n",
    "print(f\"   - Precision:     {test_precision:.2%}\")\n",
    "print(f\"   - Recall:        {test_recall:.2%}\")\n",
    "print(f\"   - Type II Error: {type_ii_error:.2%}\")\n",
    "\n",
    "print(f\"\\n6. Traffic Light ì„±ëŠ¥ (Test Set):\")\n",
    "print(f\"   - ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨: {test_capture_rate:.2%}\")\n",
    "print(f\"   - Red ê¸°ì—… ë¶€ë„ìœ¨:    {test_traffic_stats.loc['Red', 'ë¶€ë„ìœ¨']*100:.2f}%\")\n",
    "print(f\"   - Yellow ê¸°ì—… ë¶€ë„ìœ¨: {test_traffic_stats.loc['Yellow', 'ë¶€ë„ìœ¨']*100:.2f}%\")\n",
    "print(f\"   - Green ê¸°ì—… ë¶€ë„ìœ¨:  {test_traffic_stats.loc['Green', 'ë¶€ë„ìœ¨']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n7. Data Leakage ë°©ì§€ í™•ì¸:\")\n",
    "print(f\"   âœ… Test setì€ ìµœì¢… í‰ê°€ì—ì„œë§Œ ë‹¨ í•œ ë²ˆ ì‚¬ìš©\")\n",
    "print(f\"   âœ… ëª¨ë“  ì˜ì‚¬ê²°ì •(ëª¨ë¸ ì„ íƒ, ì„ê³„ê°’)ì€ Validation set ê¸°ë°˜\")\n",
    "print(f\"   âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì€ Train set 5-Fold CVë§Œ ì‚¬ìš©\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"\\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„: Part 4 - SHAP ë¶„ì„ ë° ëª¨ë¸ í•´ì„\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Part 3 v2 ì™„ë£Œ\n",
    "\n",
    "**í•µì‹¬ ê°œì„  ì‚¬í•­:**\n",
    "1. âœ… **Data Leakage ì™„ì „ ì œê±°** - Test setì€ ìµœì¢… í‰ê°€ë§Œ\n",
    "2. âœ… **3-Way Split** - Train/Validation/Test ëª…í™•í•œ ë¶„ë¦¬\n",
    "3. âœ… **ë¦¬ìƒ˜í”Œë§ ì „ëµ ëŒ€ì¡° ì‹¤í—˜** - SMOTE vs Class Weight\n",
    "4. âœ… **Validation ê¸°ë°˜ ëª¨ë¸ ì„ íƒ** - Statistical Test í¬í•¨\n",
    "5. âœ… **Validation ê¸°ë°˜ ì„ê³„ê°’ ìµœì í™”** - F2-Score, Recall ìš°ì„ \n",
    "6. âœ… **ë°ì´í„° ê¸°ë°˜ Traffic Light** - Recall ë³´ì¥ ì„ê³„ê°’\n",
    "7. âœ… **Test Set ë‹¨ í•œ ë²ˆ í‰ê°€** - ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì¦\n",
    "\n",
    "**í•™ìˆ ì  ì—„ë°€ì„±:**\n",
    "- âœ… ì¬í˜„ ê°€ëŠ¥ì„± (random_state ì„¤ì •)\n",
    "- âœ… Stratified Split (ë¶€ë„ìœ¨ ìœ ì§€)\n",
    "- âœ… Cross-Validation (5-Fold)\n",
    "- âœ… Statistical Significance Test (Wilcoxon)\n",
    "\n",
    "**ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:**\n",
    "- âœ… F2-Score (Recall ìš°ì„ )\n",
    "- âœ… Traffic Light ì‹œìŠ¤í…œ (ì˜ì‚¬ê²°ì • ì§€ì›)\n",
    "- âœ… Cumulative Gains (íš¨ìœ¨ì„± ì…ì¦)\n",
    "- âœ… Type II Error ìµœì†Œí™”\n",
    "\n",
    "**Part 4 ì¤€ë¹„:**\n",
    "- âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ\n",
    "- âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥\n",
    "- âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (SHAP ë¶„ì„ìš©)\n",
    "- âœ… Feature Importance ì €ì¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}