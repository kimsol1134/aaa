"""
Part3 ë…¸íŠ¸ë¶ì˜ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ - ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Part3 ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import os
import joblib
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import average_precision_score, recall_score, fbeta_score

import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier

from imblearn.over_sampling import SMOTE, BorderlineSMOTE
from imblearn.under_sampling import TomekLinks
from imblearn.combine import SMOTETomek

# ì„¤ì •
RANDOM_STATE = 42
DATA_DIR = 'data'
PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')
os.makedirs(PROCESSED_DIR, exist_ok=True)

INPUT_FILE = os.path.join(DATA_DIR, 'features', 'domain_based_features_ì™„ì „íŒ.csv')
OUTPUT_PREFIX = 'ë°œí‘œ_Part3_v3'

print("=" * 80)
print("ğŸš€ Part3 ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("=" * 80)

# 1. ë°ì´í„° ë¡œë“œ
print("\nğŸ“‚ 1. ë°ì´í„° ë¡œë“œ ì¤‘...")
# íŠ¹ì„± ë°ì´í„°
features_df = pd.read_csv(INPUT_FILE, index_col=0, encoding='utf-8-sig')
print(f"   íŠ¹ì„± ë°ì´í„° shape: {features_df.shape}")

# ì›ë³¸ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œë“œ
print("   ì›ë³¸ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œë“œ...")
original_file = os.path.join(DATA_DIR, 'ê¸°ì—…ì‹ ìš©í‰ê°€ì •ë³´_210801.csv')
original_df = pd.read_csv(original_file, index_col=0, encoding='cp949')

target_col = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'
if target_col not in original_df.columns:
    # ì»¬ëŸ¼ëª… í™•ì¸
    print("   ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ê²Ÿ ë³€ìˆ˜:")
    target_candidates = [col for col in original_df.columns if 'Performance' in col or 'ë¶€ë„' in col]
    print(f"   {target_candidates}")
    if target_candidates:
        target_col = target_candidates[0]
        print(f"   â†’ '{target_col}' ì‚¬ìš©")
    else:
        raise ValueError("íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# íƒ€ê²Ÿ ë³€ìˆ˜ ì¶”ì¶œ (ì¸ë±ìŠ¤ ê¸°ì¤€ ë§¤ì¹­)
y = original_df.loc[features_df.index, target_col]
print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ shape: {y.shape}")

# 2. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
print("\nğŸ¯ 2. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ í™•ì¸...")
X = features_df
print(f"   X shape: {X.shape}")
print(f"   y ë¶„í¬: ì •ìƒ={np.sum(y==0)}, ë¶€ë„={np.sum(y==1)}, ë¹„ìœ¨={np.mean(y):.4f}")

# 3. Train/Val/Test ë¶„í• 
print("\nâœ‚ï¸  3. ë°ì´í„° ë¶„í•  (Train/Val/Test)...")
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE, stratify=y_temp
)

print(f"   Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

# 4. ì»¤ìŠ¤í…€ Transformer ì •ì˜
print("\nğŸ”§ 4. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±...")

from sklearn.base import BaseEstimator, TransformerMixin

class InfiniteHandler(BaseEstimator, TransformerMixin):
    """ë¬´í•œëŒ€ ê°’ì„ 0ìœ¼ë¡œ ë³€í™˜"""
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X = X.copy()
        X = X.replace([np.inf, -np.inf], 0)
        return X

class LogTransformer(BaseEstimator, TransformerMixin):
    """ë¡œê·¸ ë³€í™˜ (ì–‘ìˆ˜ë§Œ)"""
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X = X.copy()
        for col in X.columns:
            if (X[col] > 0).all():
                X[col] = np.log1p(X[col])
        return X

def create_pipeline(clf, use_smote=True):
    """
    Part3 ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸ ìƒì„±

    Args:
        clf: ë¶„ë¥˜ê¸°
        use_smote: SMOTE ì‚¬ìš© ì—¬ë¶€

    Returns:
        Pipeline
    """
    steps = [
        ('inf_handler', InfiniteHandler()),
        ('imputer', SimpleImputer(strategy='median')),
        ('log_transform', LogTransformer()),
        ('scaler', RobustScaler()),
    ]

    if use_smote:
        steps.append(('smote', SMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE)))

    steps.append(('classifier', clf))

    return Pipeline(steps)

# 5. ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ë¹ ë¥¸ í•™ìŠµ (Logistic Regression)
print("\nğŸ“ 5. Logistic Regression (L1) ëª¨ë¸ í•™ìŠµ...")
print("   (ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ê°„ë‹¨í•œ ëª¨ë¸ ì‚¬ìš©)")

lr_clf = LogisticRegression(
    penalty='l1',
    C=1.0,
    solver='liblinear',
    class_weight='balanced',
    random_state=RANDOM_STATE,
    max_iter=1000
)

pipeline = create_pipeline(lr_clf, use_smote=True)
print("   í•™ìŠµ ì¤‘...")
pipeline.fit(X_train, y_train)
print("   âœ… í•™ìŠµ ì™„ë£Œ!")

# 6. ê²€ì¦
print("\nğŸ“Š 6. ëª¨ë¸ ê²€ì¦...")
y_val_pred_proba = pipeline.predict_proba(X_val)[:, 1]
val_pr_auc = average_precision_score(y_val, y_val_pred_proba)
print(f"   Validation PR-AUC: {val_pr_auc:.4f}")

# 7. í…ŒìŠ¤íŠ¸
print("\nğŸ§ª 7. ìµœì¢… í…ŒìŠ¤íŠ¸...")
y_test_pred_proba = pipeline.predict_proba(X_test)[:, 1]
test_pr_auc = average_precision_score(y_test, y_test_pred_proba)

# ì„ê³„ê°’ ì„¤ì • (ê°„ë‹¨íˆ ì¤‘ì•™ê°’ ì‚¬ìš©)
threshold = 0.05
y_test_pred = (y_test_pred_proba >= threshold).astype(int)
test_recall = recall_score(y_test, y_test_pred)
test_f2 = fbeta_score(y_test, y_test_pred, beta=2)

print(f"   Test PR-AUC: {test_pr_auc:.4f}")
print(f"   Test Recall: {test_recall:.4f}")
print(f"   Test F2-Score: {test_f2:.4f}")

# 8. ëª¨ë¸ ì €ì¥
print("\nğŸ’¾ 8. ëª¨ë¸ ë° íŒŒì´í”„ë¼ì¸ ì €ì¥...")

# íŒŒì´í”„ë¼ì¸ ì „ì²´ ì €ì¥ (ì „ì²˜ë¦¬ + ëª¨ë¸)
model_path = os.path.join(PROCESSED_DIR, f'{OUTPUT_PREFIX}_ìµœì¢…ëª¨ë¸.pkl')
joblib.dump(pipeline, model_path)
print(f"   âœ… {OUTPUT_PREFIX}_ìµœì¢…ëª¨ë¸.pkl")

# ì„ê³„ê°’ ì €ì¥
thresholds = {
    'selected': threshold,
    'red': 0.05,
    'yellow': 0.02,
    'green': 0.01
}
threshold_path = os.path.join(PROCESSED_DIR, f'{OUTPUT_PREFIX}_ì„ê³„ê°’.pkl')
joblib.dump(thresholds, threshold_path)
print(f"   âœ… {OUTPUT_PREFIX}_ì„ê³„ê°’.pkl")

# ê²°ê³¼ ì €ì¥
results = {
    'model_name': 'LogisticRegression_L1_SMOTE',
    'test_pr_auc': test_pr_auc,
    'test_recall': test_recall,
    'test_f2': test_f2,
    'val_pr_auc': val_pr_auc
}
results_path = os.path.join(PROCESSED_DIR, f'{OUTPUT_PREFIX}_ê²°ê³¼.pkl')
joblib.dump(results, results_path)
print(f"   âœ… {OUTPUT_PREFIX}_ê²°ê³¼.pkl")

# ì „ì²˜ë¦¬ë§Œ ë¶„ë¦¬í•´ì„œ ì €ì¥ (Streamlitìš©)
preprocessing_steps = pipeline.steps[:-1]  # ë§ˆì§€ë§‰ classifier ì œì™¸
preprocessing_pipeline = Pipeline(preprocessing_steps)
preprocess_path = os.path.join(PROCESSED_DIR, 'preprocessing_pipeline.pkl')
joblib.dump(preprocessing_pipeline, preprocess_path)
print(f"   âœ… preprocessing_pipeline.pkl")

print(f"\nì €ì¥ ìœ„ì¹˜: {PROCESSED_DIR}")
print("=" * 80)
print("âœ… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
print("=" * 80)

# deploymentë¡œ ë³µì‚¬
print("\nğŸ“¦ deployment í´ë”ë¡œ ë³µì‚¬ ì¤‘...")
import shutil

deploy_model_dir = 'deployment/data/processed'
os.makedirs(deploy_model_dir, exist_ok=True)

files_to_copy = [
    f'{OUTPUT_PREFIX}_ìµœì¢…ëª¨ë¸.pkl',
    f'{OUTPUT_PREFIX}_ì„ê³„ê°’.pkl',
    'preprocessing_pipeline.pkl'
]

for fname in files_to_copy:
    src = os.path.join(PROCESSED_DIR, fname)
    dst = os.path.join(deploy_model_dir, fname)
    if os.path.exists(src):
        shutil.copy(src, dst)
        print(f"   âœ… {fname} â†’ deployment/")

print("\nğŸ‰ ì™„ë£Œ! deployment í´ë”ì— ìµœì¢… ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
