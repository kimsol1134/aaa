# ë°œí‘œìš© Part 3: ëª¨ë¸ë§ ë° ìµœì í™” ë…¸íŠ¸ë¶ ìƒì„± í”„ë¡¬í”„íŠ¸ (ì™„ì „íŒ)

**Target**: Claude Code (ìƒˆ ì±„íŒ…ì°½)
**Output**: `notebooks/ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_v2.ipynb`
**Version**: 2.0 (Data Leakage ì™„ì „ ì œê±°, í•™ìˆ ì  ì—„ë°€ì„± í™•ë³´)

---

## ğŸ¯ Role & Context

ë‹¹ì‹ ì€ **20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸**ì…ë‹ˆë‹¤. í•™ê³„ì™€ ì‹¤ë¬´ë¥¼ ë„˜ë‚˜ë“¤ë©° **ì—„ë°€í•œ ë°©ë²•ë¡ **ê³¼ **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜**ë¥¼ ëª¨ë‘ ì¤‘ì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ **ê¸ˆìœµ ì‹ ìš©í‰ê°€ ëª¨ë¸ë§**ê³¼ **ë¶ˆê· í˜• ë¶„ë¥˜** ì „ë¬¸ê°€ë¡œì„œ, **Data Leakage ë°©ì§€**, **í†µê³„ì  ìœ ì˜ì„± ê²€ì¦**, **ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥** í™•ë³´ì— ì² ì €í•©ë‹ˆë‹¤.

### í”„ë¡œì íŠ¸ ë°°ê²½

- **ë„ë©”ì¸**: í•œêµ­ ê¸°ì—… ë¶€ë„ ì˜ˆì¸¡ (3ê°œì›”~1ë…„ ì „ ì¡°ê¸° ê²½ë³´)
- **ë°ì´í„°**: 50,105ê°œ ê¸°ì—…, ë¶€ë„ìœ¨ 1.51% (1:66 ë¶ˆê· í˜•)
- **íŠ¹ì„±**: 27ê°œ ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± (Part 2ì—ì„œ VIF/IV/AUC ê²€ì¦ ì™„ë£Œ)
- **í•µì‹¬ ê³¼ì œ**: Type II Error(ë¶€ë„ ë¯¸íƒì§€) ìµœì†Œí™”, Recall ìš°ì„ 

### Part 1-2 ì™„ë£Œ ì‚¬í•­

**Part 1**: ìœ ë™ì„±ì´ ê°€ì¥ ê°•ë ¥í•œ ì˜ˆì¸¡ ë³€ìˆ˜, ì—…ì¢…ë³„ ë¶€ë„ìœ¨ 2ë°° ì°¨ì´ ë°œê²¬
**Part 2**: 52ê°œ íŠ¹ì„± ìƒì„± â†’ VIF/IV ê¸°ë°˜ 27ê°œ ì„ íƒ, `domain_based_features_ì™„ì „íŒ.csv` ì¶œë ¥

---

## ğŸš¨ Critical Requirements (ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­)

### 1ï¸âƒ£ **Data Leakage ì™„ì „ ì œê±°** âš ï¸âš ï¸âš ï¸

```
âŒ ì ˆëŒ€ ê¸ˆì§€:
- Test setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
- Test setìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™”
- Test setìœ¼ë¡œ Traffic Light ë“±ê¸‰ ê¸°ì¤€ ê²°ì •
- Test setì„ ë³´ê³  ì–´ë–¤ ì˜ì‚¬ê²°ì •ë„ í•˜ì§€ ì•ŠìŒ

âœ… í•„ìˆ˜:
- Test setì€ ìµœì¢… ë³´ê³  ì§ì „ ë‹¨ í•œ ë²ˆë§Œ í‰ê°€
- ëª¨ë“  ì˜ì‚¬ê²°ì •ì€ Train/Validationì—ì„œë§Œ
- Validation set ë˜ëŠ” CVë¡œ ì„ê³„ê°’ ìµœì í™”
```

**ë¹„ìœ **: Test setì€ "ë´‰ì¸ëœ ì‹œí—˜ì§€". ëª¨ë¸ ê°œë°œì´ ì™„ì „íˆ ëë‚œ í›„ í•œ ë²ˆë§Œ ê°œë´‰.

---

### 2ï¸âƒ£ **3-Way Data Split (Train/Validation/Test)**

```python
# í•„ìˆ˜ êµ¬ì¡°:
ì „ì²´ ë°ì´í„° (50,105)
â”œâ”€ Train Set (60%, ~30,063): ëª¨ë¸ í•™ìŠµ + CV íŠœë‹
â”œâ”€ Validation Set (20%, ~10,021): ëª¨ë¸ ì„ íƒ, ì„ê³„ê°’ ìµœì í™”, ì˜ì‚¬ê²°ì •
â””â”€ Test Set (20%, ~10,021): ìµœì¢… í‰ê°€ë§Œ (ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ!)

# ì½”ë“œ:
from sklearn.model_selection import train_test_split

# 1ì°¨ ë¶„í• : Train+Val (80%) vs Test (20%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 2ì°¨ ë¶„í• : Train (75% of 80% = 60%) vs Val (25% of 80% = 20%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42
)

print(f"Train: {len(X_train)} (ë¶€ë„ìœ¨: {y_train.mean():.2%})")
print(f"Val:   {len(X_val)} (ë¶€ë„ìœ¨: {y_val.mean():.2%})")
print(f"Test:  {len(X_test)} (ë¶€ë„ìœ¨: {y_test.mean():.2%})")
```

---

### 3ï¸âƒ£ **ë¦¬ìƒ˜í”Œë§ vs Class Weight ëª…í™•í•œ ëŒ€ì¡° ì‹¤í—˜**

**ë¬¸ì œì **: SMOTE + Class Weight ë™ì‹œ ì‚¬ìš© ì‹œ ë…¸ì´ì¦ˆ ì¦í­, ê³¼ì í•© ìœ„í—˜

**í•´ê²°ì±…**: ë‘ ì „ëµì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ ì‹¤í—˜

```python
# Strategy A: SMOTE ê³„ì—´ (Class Weight ì—†ìŒ)
strategy_A = {
    'resampler': [SMOTE(0.2), BorderlineSMOTE(0.2), SMOTETomek(0.2)],
    'classifier__scale_pos_weight': [1],  # ê¸°ë³¸ê°’
    'classifier__class_weight': [None],   # ì‚¬ìš© ì•ˆ í•¨
}

# Strategy B: Class Weight (ë¦¬ìƒ˜í”Œë§ ì—†ìŒ)
strategy_B = {
    'resampler': ['passthrough'],
    'classifier__scale_pos_weight': [1, sqrt_ratio, scale_ratio],
    'classifier__class_weight': ['balanced'],
}

# ë‘ ì „ëµì˜ ì„±ëŠ¥ì„ Validation Setì—ì„œ ë¹„êµ
```

**ê¶Œì¥**: ê¸ˆìœµ ë°ì´í„° íŠ¹ì„±ìƒ **Strategy B (Class Weight Only)**ê°€ ë” ë‚˜ì„ ê°€ëŠ¥ì„± ë†’ìŒ.

---

### 4ï¸âƒ£ **ëª¨ë¸ ì„ íƒ: Validation Set ê¸°ë°˜ + Statistical Test**

```python
# âŒ ì˜ëª»ëœ ë°©ë²• (í˜„ì¬ ì½”ë“œ):
single_score = best_model.score(X_test, y_test)  # Test set ì‚¬ìš©!
voting_score = voting_clf.score(X_test, y_test)

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•:
# 1. Validation Set í‰ê°€
single_val_pr_auc = average_precision_score(y_val, best_model.predict_proba(X_val)[:, 1])
voting_val_pr_auc = average_precision_score(y_val, voting_clf.predict_proba(X_val)[:, 1])

# 2. Statistical Significance Test (ì„ íƒì )
from scipy.stats import wilcoxon

# CV foldë³„ ì ìˆ˜ë¡œ paired test
cv_scores_single = cross_val_score(best_model, X_train, y_train, cv=5, scoring='average_precision')
cv_scores_voting = cross_val_score(voting_clf, X_train, y_train, cv=5, scoring='average_precision')

statistic, pvalue = wilcoxon(cv_scores_voting, cv_scores_single)

# 3. ìµœì¢… ê²°ì • (Validation + Statistical Test)
if voting_val_pr_auc > single_val_pr_auc and pvalue < 0.05:
    final_model = voting_clf
    decision_reason = f"Votingì´ {voting_val_pr_auc - single_val_pr_auc:.4f} ë” ìš°ìˆ˜ (p={pvalue:.4f} < 0.05)"
else:
    final_model = best_model
    decision_reason = f"Single ëª¨ë¸ ì„ íƒ (ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ ë˜ëŠ” ë³µì¡ë„ ê³ ë ¤)"
```

---

### 5ï¸âƒ£ **ì„ê³„ê°’ ìµœì í™”: Validation Set ë˜ëŠ” CV ê¸°ë°˜**

```python
# âŒ ì ˆëŒ€ ê¸ˆì§€ (Data Leakage):
precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob_test)
optimal_threshold = find_f2_optimal(precisions, recalls, thresholds)

# âœ… ë°©ë²• 1: Validation Set ì‚¬ìš©
y_prob_val = final_model.predict_proba(X_val)[:, 1]
precisions_val, recalls_val, thresholds_val = precision_recall_curve(y_val, y_prob_val)

# F2-Score ê³„ì‚° (Recall ìš°ì„ )
beta = 2
f2_scores = (1 + beta**2) * (precisions_val * recalls_val) / (beta**2 * precisions_val + recalls_val + 1e-10)
optimal_idx = np.argmax(f2_scores)
optimal_threshold = thresholds_val[optimal_idx]

print(f"âœ… Validation Set ê¸°ë°˜ ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
print(f"   - F2-Score: {f2_scores[optimal_idx]:.4f}")
print(f"   - Precision: {precisions_val[optimal_idx]:.2%}")
print(f"   - Recall: {recalls_val[optimal_idx]:.2%}")

# âœ… ë°©ë²• 2: Cross-Validation ê¸°ë°˜ (ë” robust)
from sklearn.model_selection import cross_val_predict

y_prob_cv = cross_val_predict(final_model, X_train, y_train, cv=5, method='predict_proba')[:, 1]
precisions_cv, recalls_cv, thresholds_cv = precision_recall_curve(y_train, y_prob_cv)
# ... F2-Score ê³„ì‚° ë™ì¼ ...

# ìµœì¢…: Validation + CV í‰ê· ê°’ ì‚¬ìš© (ë”ìš± robust)
optimal_threshold_final = (optimal_threshold_val + optimal_threshold_cv) / 2
```

**ì¤‘ìš”**: Test setì—ëŠ” ê²°ì •ëœ ì„ê³„ê°’ì„ "ì ìš©"ë§Œ í•˜ê³ , ì ˆëŒ€ "ìµœì í™”"í•˜ì§€ ì•ŠìŒ!

---

### 6ï¸âƒ£ **Traffic Light ì„ê³„ê°’: ë°ì´í„° ê¸°ë°˜ ë…¼ë¦¬**

```python
# âŒ í˜„ì¬ ë°©ì‹ (ì‘ìœ„ì ):
yellow_threshold = red_threshold * 0.5  # ì™œ 0.5ì¸ê°€? ê·¼ê±° ì•½í•¨

# âœ… ê°œì„  ë°©ì‹ 1: Recall ê¸°ë°˜
# Red: Recall 80% ë‹¬ì„± ì„ê³„ê°’
# Yellow: Recall 95% ë‹¬ì„± ì„ê³„ê°’

idx_recall_80 = np.where(recalls_val >= 0.80)[0]
red_threshold = thresholds_val[idx_recall_80[np.argmax(precisions_val[idx_recall_80])]]

idx_recall_95 = np.where(recalls_val >= 0.95)[0]
yellow_threshold = thresholds_val[idx_recall_95[np.argmax(precisions_val[idx_recall_95])]]

print(f"ğŸ”´ Red: Recall 80% ë³´ì¥ (ì„ê³„ê°’ {red_threshold:.4f})")
print(f"ğŸŸ¡ Yellow: Recall 95% ë³´ì¥ (ì„ê³„ê°’ {yellow_threshold:.4f})")

# âœ… ê°œì„  ë°©ì‹ 2: í™•ë¥  ë¶„í¬ ê¸°ë°˜
# Validation Set ë¶€ë„ ê¸°ì—…ì˜ í™•ë¥  ë¶„í¬ì—ì„œ percentile í™œìš©
bankrupt_probs = y_prob_val[y_val == 1]

red_threshold = np.percentile(bankrupt_probs, 20)    # ë¶€ë„ ê¸°ì—… í•˜ìœ„ 20%
yellow_threshold = np.percentile(bankrupt_probs, 5)  # ë¶€ë„ ê¸°ì—… í•˜ìœ„ 5%

print(f"ğŸ”´ Red: ë¶€ë„ ê¸°ì—… ìƒìœ„ 80% í¬í•¨ (ì„ê³„ê°’ {red_threshold:.4f})")
print(f"ğŸŸ¡ Yellow: ë¶€ë„ ê¸°ì—… ìƒìœ„ 95% í¬í•¨ (ì„ê³„ê°’ {yellow_threshold:.4f})")
```

**ë…¼ë¦¬ ê°•í™”**: "Yellow ë“±ê¸‰ì€ ë¶€ë„ ê¸°ì—…ì˜ 95%ë¥¼ í¬ì°©í•˜ë„ë¡ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤" â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ì„¤ë“ë ¥ â†‘

---

### 7ï¸âƒ£ **ì•™ìƒë¸” ì „ëµ: ë‹¤ì–‘ì„±(Diversity) í™•ë³´**

**ë¬¸ì œ**: ìƒìœ„ 3ê°œ ëª¨ë‘ GBM ê³„ì—´ â†’ Voting íš¨ê³¼ ë¯¸ë¯¸

**í•´ê²°ì±… A**: ì´ì¢… ëª¨ë¸ ê°•ì œ í¬í•¨

```python
# Top GBM (ìµœê³  ì„±ëŠ¥)
best_gbm = search.best_estimator_

# ìµœê³  ì„±ëŠ¥ Logistic Regression (ì„¤ëª…ë ¥ â†‘, ë‹¤ì–‘ì„± â†‘)
lr_results = results_df[results_df['param_classifier'].apply(lambda x: 'Logistic' in str(x))]
best_lr = lr_results.nsmallest(1, 'rank_test_score').iloc[0]

# ìµœê³  ì„±ëŠ¥ Random Forest (Tree ê¸°ë°˜ì´ì§€ë§Œ ë©”ì»¤ë‹ˆì¦˜ ë‹¤ë¦„)
rf_results = results_df[results_df['param_classifier'].apply(lambda x: 'Random' in str(x))]
best_rf = rf_results.nsmallest(1, 'rank_test_score').iloc[0]

# 3-model Ensemble (GBM + LR + RF)
voting_clf = VotingClassifier(
    estimators=[
        ('gbm', best_gbm),
        ('lr', best_lr_pipeline),
        ('rf', best_rf_pipeline)
    ],
    voting='soft',
    weights=[0.5, 0.25, 0.25]  # GBM ë†’ì€ ê°€ì¤‘ì¹˜, ë‚˜ë¨¸ì§€ëŠ” ë‹¤ì–‘ì„± í™•ë³´
)
```

**í•´ê²°ì±… B**: ë‹¨ì¼ ëª¨ë¸ ì„ íƒ (ê¶Œì¥)

```python
# Part 4 SHAP ë¶„ì„ì„ ê³ ë ¤í•˜ë©´ ë‹¨ì¼ ëª¨ë¸ì´ í›¨ì”¬ ìœ ë¦¬
# Validation Set í‰ê°€ í›„ ì•™ìƒë¸”ì´ 0.5% ì´ìƒ ìš°ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ë‹¨ì¼ ëª¨ë¸ ì„ íƒ

if voting_val_pr_auc - single_val_pr_auc < 0.005:
    final_model = best_model
    decision_reason = "ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸ + SHAP í•´ì„ë ¥ ìš°ì„  â†’ ë‹¨ì¼ ëª¨ë¸ ì„ íƒ"
```

---

### 8ï¸âƒ£ **ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê°œì„ **

```python
# âœ… ê°œì„ ëœ ìˆœì„œ ë° ì„¤ì •
pipeline = ImbPipeline([
    ('inf_handler', InfiniteHandler()),
    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # ë¨¼ì € ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    ('log_transformer', LogTransformer()),                        # ê·¸ ë‹¤ìŒ ë¡œê·¸ ë³€í™˜
    ('winsorizer', Winsorizer(0.005, 0.995)),  # ë²”ìœ„ ì¶•ì†Œ (0.5%~99.5%), ë˜ëŠ” ì œê±° ê²€í† 
    ('scaler', RobustScaler()),
    ('resampler', 'passthrough'),  # Class Weight ì „ëµ ìš°ì„ 
    ('classifier', LogisticRegression())
])
```

**Winsorizer ì œê±° ê²€í† **: Tree ëª¨ë¸ì€ ì´ìƒì¹˜ì— ê°•ê±´. ë¶€ë„ ë°ì´í„°ì˜ ê·¹ë‹¨ê°’ì€ ì¤‘ìš” ì‹œê·¸ë„ì¼ ìˆ˜ ìˆìŒ.

```python
# ì‹¤í—˜: Winsorizer ìˆìŒ vs ì—†ìŒ ë¹„êµ
pipeline_with_winsor = create_pipeline(use_winsorizer=True)
pipeline_without_winsor = create_pipeline(use_winsorizer=False)

# Validation Set ë¹„êµ
score_with = average_precision_score(y_val, pipeline_with_winsor.predict_proba(X_val)[:, 1])
score_without = average_precision_score(y_val, pipeline_without_winsor.predict_proba(X_val)[:, 1])

print(f"Winsorizer ìˆìŒ: {score_with:.4f}")
print(f"Winsorizer ì—†ìŒ: {score_without:.4f}")
```

---

### 9ï¸âƒ£ **RandomizedSearchCV ì„¤ì • ê°œì„ **

**í˜„ì¬**: 100íšŒ ìƒ˜í”Œë§ â†’ Coverage ì•½ 1% (íƒìƒ‰ ê³µê°„ ëŒ€ë¹„)

**ê°œì„ **: ëª¨ë¸ë³„ ë…ë¦½ íŠœë‹ ë˜ëŠ” íšŸìˆ˜ ì¦ê°€

```python
# ë°©ë²• 1: ëª¨ë¸ë³„ ê°œë³„ íŠœë‹ (ê¶Œì¥)
models_to_tune = {
    'LightGBM': (lgb.LGBMClassifier(), lgbm_param_grid),
    'XGBoost': (xgb.XGBClassifier(), xgb_param_grid),
    'CatBoost': (CatBoostClassifier(), catboost_param_grid),
}

best_models = {}
for model_name, (model, param_grid) in models_to_tune.items():
    search = RandomizedSearchCV(
        estimator=create_pipeline(model),
        param_distributions=param_grid,
        n_iter=200,  # ëª¨ë¸ë‹¹ 200íšŒ
        scoring='average_precision',
        cv=5,
        n_jobs=-1,
        random_state=42
    )
    search.fit(X_train, y_train)
    best_models[model_name] = search.best_estimator_
    print(f"{model_name} ìµœì  CV PR-AUC: {search.best_score_:.4f}")

# ë°©ë²• 2: Optuna ì‚¬ìš© (ì„ íƒì )
import optuna
# ... Optuna ê¸°ë°˜ Bayesian Optimization ...
```

---

## ğŸ“‹ ë…¸íŠ¸ë¶ êµ¬ì¡° (í•„ìˆ˜ ì„¹ì…˜)

```markdown
# ğŸ“— ë°œí‘œìš© Part 3: ëª¨ë¸ë§ ë° ìµœì í™” v2

## ğŸ¯ Part 3 ëª©í‘œ ë° ì´ì „ Part ìš”ì•½
[Part 1-2 ìš”ì•½, ê°œì„  ì‚¬í•­ ëª…ì‹œ]

## 0. í™˜ê²½ ì„¤ì •
[ë¼ì´ë¸ŒëŸ¬ë¦¬, í•œê¸€ í°íŠ¸]

## 1. ë°ì´í„° ë¡œë”© ë° **3-Way Split** â­
- Train (60%) / Validation (20%) / Test (20%)
- Stratified splitìœ¼ë¡œ ë¶€ë„ìœ¨ ìœ ì§€
- ê° setì˜ í†µê³„ ì¶œë ¥

## 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
- ìˆœì„œ ê°œì„ : Imputer â†’ Log â†’ Winsorizer(ì„ íƒì ) â†’ Scaler
- Winsorizer ì‹¤í—˜ ê²°ê³¼ í¬í•¨

## 3. ë¦¬ìƒ˜í”Œë§ ì „ëµ ëŒ€ì¡° ì‹¤í—˜
- **Strategy A**: SMOTE ê³„ì—´ (Class Weight ì œì™¸)
- **Strategy B**: Class Weight Only (ë¦¬ìƒ˜í”Œë§ ì œì™¸)
- Validation Set ë¹„êµ â†’ ìš°ìˆ˜í•œ ì „ëµ ì„ íƒ

## 4. AutoML: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- RandomizedSearchCV (ëª¨ë¸ë³„ 200íšŒ or í†µí•© 500íšŒ)
- **Train Set + 5-Fold CV**ë¡œë§Œ í•™ìŠµ
- ìƒìœ„ 10ê°œ ëª¨ë¸ ì¶œë ¥

## 5. ëª¨ë¸ ì„ íƒ: **Validation Set ê¸°ë°˜** â­
- Single Best vs Voting Ensemble **Validation í‰ê°€**
- Statistical Significance Test (Wilcoxon)
- ìµœì¢… ëª¨ë¸ ì„ íƒ ë¡œì§ + ê·¼ê±° ëª…ì‹œ

## 6. ì„ê³„ê°’ ìµœì í™”: **Validation Set ê¸°ë°˜** â­
- F2-Score (Recall ìš°ì„ ) ìµœì í™”
- Recall 80% ëª©í‘œ ì„ê³„ê°’
- CV ê¸°ë°˜ ê²€ì¦ (ì„ íƒì )

## 7. Traffic Light ì‹œìŠ¤í…œ: **ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’** â­
- Red: Recall 80% ë³´ì¥
- Yellow: Recall 95% ë³´ì¥
- Validation Set ì„±ëŠ¥ ì¶œë ¥
- ë…¼ë¦¬ì  ê·¼ê±° ëª…ì‹œ

## 8. **Test Set ìµœì¢… í‰ê°€** (ë‹¨ í•œ ë²ˆ!) â­
- ì„ê³„ê°’ ì ìš© í›„ Test Set í‰ê°€
- Confusion Matrix (ìµœì  ì„ê³„ê°’)
- PR-AUC Curve
- Traffic Light ì„±ëŠ¥ (Test Set)
- âš ï¸ "Test Setì€ ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•Šì•˜ìŒ" ëª…ì‹œ

## 9. Feature Importance ë¶„ì„
[Top 15, Plotly ì‹œê°í™”]

## 10. ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„
- Cumulative Gains Curve (Test Set)
- íš¨ìœ¨ì„± ë¶„ì„

## 11. ëª¨ë¸ ì €ì¥ ë° ë‹¤ìŒ ë‹¨ê³„
- Part 4 SHAP ë¶„ì„ìš© íŒŒì¼ ì €ì¥
- ì „ì²˜ë¦¬ëœ ë°ì´í„° (X_train, X_val, X_test processed)
- ì„ê³„ê°’ ì €ì¥
```

---

## ğŸ“Š í•„ìˆ˜ ì¶œë ¥ ë‚´ìš©

### ë°ì´í„° ë¶„í•  ì •ë³´

```
âœ… 3-Way Split ì™„ë£Œ
==================================================
Train Set:      30,063 (60%, ë¶€ë„ìœ¨: 1.51%)
Validation Set: 10,021 (20%, ë¶€ë„ìœ¨: 1.51%)
Test Set:       10,021 (20%, ë¶€ë„ìœ¨: 1.51%)
==================================================
âš ï¸ Test Setì€ ìµœì¢… í‰ê°€ ì „ê¹Œì§€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!
```

### ë¦¬ìƒ˜í”Œë§ ì „ëµ ë¹„êµ

```
ğŸ“Š ë¦¬ìƒ˜í”Œë§ ì „ëµ Validation í‰ê°€
==================================================
Strategy A (SMOTE):         PR-AUC = 0.XXXX
Strategy B (Class Weight):  PR-AUC = 0.XXXX
==================================================
âœ… ì„ íƒ: Strategy B (Class Weight) - X.XX% ë” ìš°ìˆ˜
```

### ëª¨ë¸ ì„ íƒ ê²°ê³¼

```
ğŸ“Š ëª¨ë¸ ì„ íƒ (Validation Set ê¸°ë°˜)
==================================================
Single Best Model:     PR-AUC = 0.XXXX
Voting Ensemble:       PR-AUC = 0.XXXX
Wilcoxon p-value:      0.XXX
==================================================
âœ… ìµœì¢… ì„ íƒ: [ëª¨ë¸ëª…]
   ì´ìœ : [Validation ì„±ëŠ¥ + Statistical Test + ë³µì¡ë„ ê³ ë ¤]
```

### ì„ê³„ê°’ ìµœì í™” ê²°ê³¼

```
ğŸ“Š ì„ê³„ê°’ ìµœì í™” (Validation Set ê¸°ë°˜)
==================================================
F2-Score ìµœì  ì„ê³„ê°’:     0.XXXX
  - F2-Score:             0.XXXX
  - Precision:            XX.XX%
  - Recall:               XX.XX%

Recall 80% ë³´ì¥ ì„ê³„ê°’:   0.XXXX
  - Precision:            XX.XX%
  - Recall:               XX.XX%
==================================================
âœ… ì„ íƒ: Recall 80% ì„ê³„ê°’ (ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ìš°ì„ )
```

### Test Set ìµœì¢… í‰ê°€ (ë‹¨ í•œ ë²ˆ!)

```
ğŸ¯ Test Set ìµœì¢… í‰ê°€ (ì„ê³„ê°’ ì ìš©)
==================================================
âš ï¸ ì´ ê²°ê³¼ëŠ” ìµœì¢… ë³´ê³ ìš©ì´ë©°, ì´ì „ ë‹¨ê³„ì—ì„œ Test Setì„
   ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì„ ë³´ì¥í•©ë‹ˆë‹¤.

ì„ê³„ê°’: 0.XXXX (Validationì—ì„œ ê²°ì •)
PR-AUC:              0.XXXX
ROC-AUC:             0.XXXX
F2-Score:            0.XXXX
Precision:           XX.XX%
Recall:              XX.XX%
Type II Error:       XX.XX%

Confusion Matrix:
  TN: X,XXX  |  FP: XXX
  FN: XXX    |  TP: XXX
==================================================
```

### Traffic Light ì„±ëŠ¥ (Test Set)

```
ğŸš¦ Traffic Light ì‹œìŠ¤í…œ (Test Set ìµœì¢… í‰ê°€)
==================================================
ë“±ê¸‰      ê¸°ì—… ìˆ˜    ë¹„ìœ¨    ì‹¤ì œ ë¶€ë„    ì •ë°€ë„    í¬ì°©ë¥ 
ğŸ”´ Red    XXX       X.X%    XXX         XX.X%    XX.X%
ğŸŸ¡ Yellow XXX       X.X%    XXX         XX.X%    XX.X%
ğŸŸ¢ Green  XXX       XX.X%   XXX         X.X%     X.X%
----------------------------------------------------------
í•©ê³„      10,021    100%    XXX         -        XX.X%
==================================================
âœ… ë¦¬ìŠ¤í¬ ë°©ì–´ìœ¨: XX.X% (Red+Yellowì—ì„œ ë¶€ë„ í¬ì°©)
```

---

## âš ï¸ ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­ (Checklist)

- [ ] âŒ Test setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
- [ ] âŒ Test setìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™”
- [ ] âŒ Test setìœ¼ë¡œ Traffic Light ê¸°ì¤€ ê²°ì •
- [ ] âŒ Test setì„ ë³´ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬íŠœë‹
- [ ] âŒ Test setì„ ë³´ê³  ì „ì²˜ë¦¬ ë°©ë²• ë³€ê²½
- [ ] âŒ Test set ì„±ëŠ¥ì´ ë‚˜ì˜ë‹¤ê³  ë‹¤ì‹œ ì‹¤í—˜

âœ… **Test setì€ ìµœì¢… ë³´ê³  ì§ì „ ë‹¨ í•œ ë²ˆë§Œ í‰ê°€!**

---

## âœ… í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­ (Checklist)

### ë°ì´í„° ë¶„í• 
- [ ] âœ… Train/Val/Test 3-way split (60/20/20)
- [ ] âœ… Stratified splitìœ¼ë¡œ ë¶€ë„ìœ¨ ìœ ì§€
- [ ] âœ… ê° setì˜ í†µê³„ ì¶œë ¥

### ë¦¬ìƒ˜í”Œë§ ì „ëµ
- [ ] âœ… SMOTE vs Class Weight ëŒ€ì¡° ì‹¤í—˜
- [ ] âœ… Validation set ë¹„êµ
- [ ] âœ… ìš°ìˆ˜í•œ ì „ëµ ì„ íƒ ê·¼ê±° ëª…ì‹œ

### ëª¨ë¸ ì„ íƒ
- [ ] âœ… Validation set ê¸°ë°˜ í‰ê°€
- [ ] âœ… Statistical significance test (ì„ íƒì )
- [ ] âœ… ìµœì¢… ëª¨ë¸ ì„ íƒ ê·¼ê±° ëª…ì‹œ

### ì„ê³„ê°’ ìµœì í™”
- [ ] âœ… Validation set ë˜ëŠ” CV ê¸°ë°˜
- [ ] âœ… F2-Score (Recall ìš°ì„ ) ì‚¬ìš©
- [ ] âœ… Test setì—ëŠ” ì ìš©ë§Œ

### Traffic Light
- [ ] âœ… ë°ì´í„° ê¸°ë°˜ ì„ê³„ê°’ (Recall ë˜ëŠ” percentile)
- [ ] âœ… Validation set ê²€ì¦
- [ ] âœ… ë…¼ë¦¬ì  ê·¼ê±° ëª…ì‹œ

### Test Set í‰ê°€
- [ ] âœ… ìµœì¢… ë³´ê³  ì§ì „ ë‹¨ í•œ ë²ˆë§Œ
- [ ] âœ… "Test set ë¯¸ì‚¬ìš©" ëª…ì‹œ
- [ ] âœ… ëª¨ë“  ì„ê³„ê°’ Validationì—ì„œ ê²°ì •ë¨ í™•ì¸

### ì½”ë“œ í’ˆì§ˆ
- [ ] âœ… í•˜ë“œì½”ë”© ê¸ˆì§€ (ê²½ë¡œ, ì„ê³„ê°’ ë³€ìˆ˜í™”)
- [ ] âœ… í•œê¸€ í°íŠ¸ ì„¤ì •
- [ ] âœ… UTF-8 ì¸ì½”ë”©
- [ ] âœ… Top-to-bottom ì‹¤í–‰ ê°€ëŠ¥

### ë¬¸ì„œí™”
- [ ] âœ… ê° ì„¹ì…˜ë§ˆë‹¤ ëª…í™•í•œ ì„¤ëª…
- [ ] âœ… ì˜ì‚¬ê²°ì • ê·¼ê±° ëª…ì‹œ
- [ ] âœ… Validation vs Test êµ¬ë¶„ ëª…í™•
- [ ] âœ… Data Leakage ë°©ì§€ ëª…ì‹œ

---

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼ (ì„±ëŠ¥ ëª©í‘œ)

### Validation Set (ì˜ì‚¬ê²°ì • ê¸°ì¤€)
- PR-AUC: 0.15~0.20 (ë¶ˆê· í˜• ë°ì´í„° ê³ ë ¤)
- F2-Score: 0.35~0.50
- Recall: 60~80%
- Type II Error: 20~40%

### Test Set (ìµœì¢… ë³´ê³ )
- PR-AUC: Validation Â± 0.01 (ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸)
- Recall: Validation Â± 5%p
- Type II Error: Validation Â± 5%p

**ì¤‘ìš”**: Test ì„±ëŠ¥ì´ Validationê³¼ í¬ê²Œ ë‹¤ë¥´ë©´ ê³¼ì í•© ë˜ëŠ” ë°ì´í„° ë¶„í¬ ì´ìŠˆ

---

## ğŸ“ ì¶œë ¥ íŒŒì¼ ëª©ë¡

```
data/processed/
â”œâ”€â”€ ë°œí‘œ_Part3_v2_ìµœì¢…ëª¨ë¸.pkl
â”œâ”€â”€ ë°œí‘œ_Part3_v2_ë¶„ë¥˜ê¸°.pkl (SHAPìš©)
â”œâ”€â”€ ë°œí‘œ_Part3_v2_X_train_processed.csv
â”œâ”€â”€ ë°œí‘œ_Part3_v2_X_val_processed.csv
â”œâ”€â”€ ë°œí‘œ_Part3_v2_X_test_processed.csv
â”œâ”€â”€ ë°œí‘œ_Part3_v2_y_train.csv
â”œâ”€â”€ ë°œí‘œ_Part3_v2_y_val.csv
â”œâ”€â”€ ë°œí‘œ_Part3_v2_y_test.csv
â”œâ”€â”€ ë°œí‘œ_Part3_v2_ì„ê³„ê°’.pkl
â””â”€â”€ ë°œí‘œ_Part3_v2_ê²°ê³¼ìš”ì•½.pkl
```

---

## ğŸ’¡ ì¶”ê°€ ê¶Œì¥ ì‚¬í•­

### 1. Nested CV (ì„ íƒì , ë” robust)

```python
# Outer loop: ëª¨ë¸ í‰ê°€
# Inner loop: Hyperparameter tuning

from sklearn.model_selection import cross_validate

def nested_cv_evaluation(pipeline, X, y, param_grid):
    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    nested_scores = []

    for train_idx, test_idx in outer_cv.split(X, y):
        X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]
        y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]

        # Inner CV: Hyperparameter tuning
        search = RandomizedSearchCV(
            pipeline, param_grid,
            cv=inner_cv, scoring='average_precision', n_iter=50
        )
        search.fit(X_train_outer, y_train_outer)

        # Outer CV: ìµœì  ëª¨ë¸ í‰ê°€
        score = average_precision_score(
            y_test_outer,
            search.best_estimator_.predict_proba(X_test_outer)[:, 1]
        )
        nested_scores.append(score)

    return np.mean(nested_scores), np.std(nested_scores)

# Nested CVë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ì¶”ì •
mean_score, std_score = nested_cv_evaluation(pipeline, X_train, y_train, param_grid)
print(f"Nested CV PR-AUC: {mean_score:.4f} Â± {std_score:.4f}")
```

### 2. Calibration Check (í™•ë¥  ë³´ì •)

```python
from sklearn.calibration import calibration_curve

# Validation Setì—ì„œ Calibration í™•ì¸
prob_true, prob_pred = calibration_curve(
    y_val, y_prob_val, n_bins=10, strategy='quantile'
)

# Calibration Plot
fig = go.Figure()
fig.add_trace(go.Scatter(x=prob_pred, y=prob_true, mode='markers+lines', name='Model'))
fig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Perfect', line=dict(dash='dash')))
fig.update_layout(title='Calibration Curve (Validation Set)',
                  xaxis_title='Predicted Probability',
                  yaxis_title='True Probability')
fig.show()

# Calibrationì´ ë‚˜ì˜ë©´ CalibratedClassifierCV ì‚¬ìš©
from sklearn.calibration import CalibratedClassifierCV

calibrated_model = CalibratedClassifierCV(final_model, cv=5, method='isotonic')
calibrated_model.fit(X_train, y_train)
```

### 3. Learning Curve (ê³¼ì í•© ì§„ë‹¨)

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    final_model, X_train, y_train,
    cv=5, scoring='average_precision',
    train_sizes=np.linspace(0.1, 1.0, 10),
    n_jobs=-1
)

# Plot
fig = go.Figure()
fig.add_trace(go.Scatter(
    x=train_sizes,
    y=train_scores.mean(axis=1),
    mode='lines+markers',
    name='Train Score',
    error_y=dict(array=train_scores.std(axis=1))
))
fig.add_trace(go.Scatter(
    x=train_sizes,
    y=val_scores.mean(axis=1),
    mode='lines+markers',
    name='CV Score',
    error_y=dict(array=val_scores.std(axis=1))
))
fig.update_layout(title='Learning Curve', xaxis_title='Training Size', yaxis_title='PR-AUC')
fig.show()
```

---

## ğŸ” í’ˆì§ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹¤í–‰ ì „ (ì½”ë“œ ë¦¬ë·°)
- [ ] Test set ì‚¬ìš© ìœ„ì¹˜ ì „ìˆ˜ ì¡°ì‚¬ (ì ˆëŒ€ ì˜ì‚¬ê²°ì • ì—†ìŒ)
- [ ] ëª¨ë“  ì„ê³„ê°’ì´ Validationì—ì„œ ê²°ì •ë˜ëŠ”ì§€ í™•ì¸
- [ ] Random state ì„¤ì • í™•ì¸ (ì¬í˜„ì„±)
- [ ] ê²½ë¡œ í•˜ë“œì½”ë”© ì œê±°

### ì‹¤í–‰ ì¤‘ (ì¶œë ¥ í™•ì¸)
- [ ] ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì •í™•í•œì§€ í™•ì¸
- [ ] ë¶€ë„ìœ¨ì´ ê° setì—ì„œ ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸
- [ ] Validation ì„±ëŠ¥ > Test ì„±ëŠ¥ (ì •ìƒ)
- [ ] Test ì„±ëŠ¥ì´ Validation Â± 10% ë²”ìœ„ ë‚´

### ì‹¤í–‰ í›„ (ê²°ê³¼ ê²€ì¦)
- [ ] Test set í‰ê°€ê°€ ë‹¨ í•œ ë²ˆë§Œ ë‚˜íƒ€ë‚¨
- [ ] ëª¨ë“  ì˜ì‚¬ê²°ì • ê·¼ê±°ê°€ Validation ê¸°ë°˜ì„ ëª…ì‹œ
- [ ] Confusion Matrix ìˆ˜ì¹˜ í™•ì¸ (TP+FN = ì „ì²´ ë¶€ë„ ìˆ˜)
- [ ] Traffic Light ë“±ê¸‰ë³„ ë¶€ë„ìœ¨ ì°¨ì´ ëª…í™•

---

## ğŸ“ ìµœì¢… í™•ì¸ ì‚¬í•­

### í•™ìˆ ì  ì—„ë°€ì„±
- âœ… Data Leakage ì™„ì „ ì œê±°
- âœ… Train/Val/Test ëª…í™•í•œ ë¶„ë¦¬
- âœ… ì¬í˜„ ê°€ëŠ¥ì„± (random_state ì„¤ì •)
- âœ… Statistical significance test

### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
- âœ… F2-Score (Recall ìš°ì„ ) ì‚¬ìš©
- âœ… Traffic Light ì‹œìŠ¤í…œ (ì˜ì‚¬ê²°ì • ì§€ì›)
- âœ… Cumulative Gains (íš¨ìœ¨ì„± ì…ì¦)
- âœ… Type II Error ìµœì†Œí™”

### ì½”ë“œ í’ˆì§ˆ
- âœ… ëª¨ë“ˆí™” (í•¨ìˆ˜/í´ë˜ìŠ¤)
- âœ… í•œê¸€ í°íŠ¸ ì„¤ì •
- âœ… ì£¼ì„ ë° ë¬¸ì„œí™”
- âœ… Top-to-bottom ì‹¤í–‰

### Part 4 ì—°ê³„
- âœ… SHAP ë¶„ì„ìš© íŒŒì¼ ì €ì¥
- âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥
- âœ… ë‹¨ì¼ ëª¨ë¸ ìš°ì„  (í•´ì„ë ¥)

---

## ğŸ‰ ì™„ì„± ê¸°ì¤€

ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë”°ë¼ ìƒì„±ëœ ë…¸íŠ¸ë¶ì€:

1. âœ… **í•™ìˆ  ë…¼ë¬¸ ìˆ˜ì¤€ì˜ ë°©ë²•ë¡ ì  ì—„ë°€ì„±**
2. âœ… **ì‹¤ë¬´ ë°°í¬ ê°€ëŠ¥í•œ ì¼ë°˜í™” ì„±ëŠ¥**
3. âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì • ì§€ì› ê°€ëŠ¥**
4. âœ… **ì¬í˜„ ê°€ëŠ¥í•˜ê³  íˆ¬ëª…í•œ í”„ë¡œì„¸ìŠ¤**
5. âœ… **Part 4 SHAP ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ**

**íŒŒì¼ëª…**: `notebooks/ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_v2.ipynb`

**ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„**: 30~60ë¶„ (AutoML + ëŒ€ì¡° ì‹¤í—˜)

**ìµœì¢… ì‹ ë¢°ë„**: **9.5/10** â­â­â­â­â­

---

**í”„ë¡¬í”„íŠ¸ ì‘ì„±ì¼**: 2025ë…„
**ì‘ì„±ì**: Senior Data Scientist Review Team
**ë²„ì „**: 2.0 (Data Leakage Free, Production Ready)

---

ì´ì œ ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒˆ Claude Code ì±„íŒ…ì°½ì— ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•˜ì„¸ìš”! ğŸš€
