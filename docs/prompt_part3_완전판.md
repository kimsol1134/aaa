# ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_ì™„ì „íŒ.ipynb ìƒì„± í”„ë¡¬í”„íŠ¸

## ğŸ¯ ROLE & EXPERTISE

ë‹¹ì‹ ì€ **15ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´**ì´ì **ê¸ˆìœµ ë¦¬ìŠ¤í¬ ëª¨ë¸ë§ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤. ë‹¤ìŒ ì „ë¬¸ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤:

- âœ… **í•™ìˆ ì  ì—„ë°€ì„±**: Data Leakage, Selection Bias ë“± ML í•¨ì • ì™„ë²½ íšŒí”¼
- âœ… **ì‹¤ë¬´ ë°°í¬ ê²½í—˜**: ê¸ˆìœµê¶Œ ì‹ ìš©í‰ê°€ ëª¨ë¸ ìƒìš©í™” ê²½í—˜ ë‹¤ìˆ˜
- âœ… **ë¶ˆê· í˜• ë°ì´í„° ì „ë¬¸ê°€**: SMOTE, Class Weight, Threshold Optimization ë§ˆìŠ¤í„°
- âœ… **ëª¨ë¸ í•´ì„ë ¥**: SHAP, Feature Importance, Business Impact ë¶„ì„ ëŠ¥ë ¥
- âœ… **ì½”ë“œ í’ˆì§ˆ**: Production-ready, Reproducible, Well-documented

---

## ğŸ“‹ CONTEXT & BACKGROUND

### í”„ë¡œì íŠ¸ ì •ë³´
- **ì£¼ì œ**: í•œêµ­ ê¸°ì—… ë¶€ë„ ì˜ˆì¸¡ ëª¨ë¸ (50,000+ ê¸°ì—…, 170+ ë³€ìˆ˜)
- **ë°ì´í„° íŠ¹ì„±**:
  - ê·¹ì‹¬í•œ ë¶ˆê· í˜• (ë¶€ë„ìœ¨ ~1.5%, ë¹„ìœ¨ 1:20)
  - ì‹œê³„ì—´ì´ ì•„ë‹Œ íš¡ë‹¨ë©´ ë°ì´í„° (2021ë…„ 8ì›” ìŠ¤ëƒ…ìƒ·)
  - ë„ë©”ì¸ ê¸°ë°˜ Feature Engineering ì™„ë£Œ (65ê°œ íŠ¹ì„±)
- **ê¸°ì¡´ ì‘ì—…**:
  - Part 1: ë¬¸ì œ ì •ì˜ ë° EDA ì™„ë£Œ
  - Part 2: ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± ê³µí•™ ì™„ë£Œ (7ê°œ ì¹´í…Œê³ ë¦¬ 65ê°œ íŠ¹ì„±)
  - Part 3 ì´ˆì•ˆ: ëª¨ë¸ë§ ì™„ë£Œí–ˆìœ¼ë‚˜ **ì¹˜ëª…ì  Data Leakage ë°œê²¬**

### í˜„ì¬ Part 3 ì´ˆì•ˆì˜ ì¹˜ëª…ì  ë¬¸ì œì 

#### ğŸš¨ Critical Issue 1: Test Set Leakage (ê°€ì¥ ì‹¬ê°)
```python
# âŒ ì˜ëª»ëœ ë¡œì§ (í˜„ì¬ ì½”ë“œ)
# 1. Test setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
single_best_metrics = evaluate_model(best_model, X_test, y_test)
voting_metrics = evaluate_model(voting_clf, X_test, y_test)
if pr_auc_diff > ENSEMBLE_THRESHOLD:
    final_model = voting_clf  # Test set ì„±ëŠ¥ìœ¼ë¡œ ì„ íƒ!

# 2. Test setìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™”
precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob_test)
optimal_threshold = thresholds[np.argmax(f2_scores)]  # Test setì—ì„œ ìµœì í™”!
```

**Impact**:
- í˜„ì¬ ë³´ê³ ëœ ì„±ëŠ¥ì€ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ
- ì‹¤ì œ ë°°í¬ ì‹œ ì„±ëŠ¥ 5-10% í•˜ë½ ì˜ˆìƒ
- í•™ìˆ ì ìœ¼ë¡œ ë¬´íš¨í•œ í‰ê°€

#### ğŸš¨ Critical Issue 2: Validation Set ë¶€ì¬
```
í˜„ì¬ êµ¬ì¡°:  Data â†’ Train (80%) â†’ Test (20%)
                    â†“ (5-Fold CV)
                  ì—¬ê¸°ì„œë§Œ ê²€ì¦

ì˜¬ë°”ë¥¸ êµ¬ì¡°: Data â†’ Train (60%) â†’ Validation (20%) â†’ Test (20%)
                     â†“ (CV)        â†“ (ì˜ì‚¬ê²°ì •)      â†“ (ìµœì¢…í‰ê°€ë§Œ)
                   í•™ìŠµ/íŠœë‹      ëª¨ë¸ì„ íƒ,ì„ê³„ê°’      ë³´ê³ ìš©
```

#### âš ï¸ Major Issue 3: Top 3 ì¬í•™ìŠµ ì‹œ ë¦¬ìƒ˜í”Œë§ ì¬ì ìš© ë¬¸ì œ
- CVì—ì„œ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ Train set ì¬í•™ìŠµ
- í•˜ì§€ë§Œ SMOTE ë“± ë¦¬ìƒ˜í”Œë§ë„ ë‹¤ì‹œ ëœë¤í•˜ê²Œ ì ìš©ë¨
- CV ì„±ëŠ¥ â‰  ì¬í•™ìŠµ í›„ ì„±ëŠ¥ (ì¬í˜„ì„± ë¬¸ì œ)

#### âš ï¸ Major Issue 4: Weighted Votingì˜ ë¬´ì˜ë¯¸í•œ ê°€ì¤‘ì¹˜
```python
# ë¬¸ì œ: CV ì ìˆ˜ ì°¨ì´ê°€ ë¯¸ë¯¸
weights = [0.1601, 0.1598, 0.1595]  # 0.0003 ì°¨ì´
# ì •ê·œí™” í›„: [0.333, 0.333, 0.333] â† ê±°ì˜ ê· ë“± ê°€ì¤‘ì¹˜
```

#### âš ï¸ Design Flaw 5: RandomizedSearchCV 100íšŒ ë¶€ì¡±
- íƒìƒ‰ ê³µê°„: 5ê°œ ëª¨ë¸ Ã— 5ê°œ ë¦¬ìƒ˜í”Œë§ Ã— ìˆ˜ì‹­ ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° = ìˆ˜ë§Œ ì¡°í•©
- 100íšŒ ìƒ˜í”Œë§ = ì•½ 0.9% ì»¤ë²„ë¦¬ì§€ (í„±ì—†ì´ ë¶€ì¡±)

---

## ğŸ¯ MISSION & OBJECTIVES

### Primary Mission
**í•™ìˆ ì ìœ¼ë¡œ ì™„ë²½í•˜ê³ , ì‹¤ë¬´ì—ì„œ ì¦‰ì‹œ ë°°í¬ ê°€ëŠ¥í•œ `ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_ì™„ì „íŒ.ipynb` ìƒì„±**

### Success Criteria
1. âœ… **Zero Data Leakage**: Train/Validation/Test ì™„ë²½ ë¶„ë¦¬
2. âœ… **Reproducible**: random_state í†µì œ, ì‹¤í–‰ ì‹œë§ˆë‹¤ ë™ì¼ ê²°ê³¼
3. âœ… **Statistically Rigorous**: ëª¨ë“  ì„ íƒì— í†µê³„ì  ê·¼ê±°
4. âœ… **Business-Ready**: Traffic Light ì‹œìŠ¤í…œ, ROI ë¶„ì„, ë°°í¬ ê°€ì´ë“œ
5. âœ… **Explainable**: ë‹¨ì¼ ìµœì  ëª¨ë¸ ì„ íƒ (SHAP ë¶„ì„ ëŒ€ë¹„)

---

## ğŸ“ DETAILED TASK SPECIFICATION

### ë…¸íŠ¸ë¶ êµ¬ì¡° ë° í•„ìˆ˜ êµ¬í˜„ ì‚¬í•­

#### Section 0: ì„¤ì • ë° ì„í¬íŠ¸
```python
# í•„ìˆ˜ ìƒìˆ˜
RANDOM_STATE = 42
TEST_SIZE = 0.2      # ìµœì¢… Test set
VAL_SIZE = 0.25      # Trainì˜ 25% â†’ ì „ì²´ì˜ 20%
CV_FOLDS = 5
N_ITER_PER_MODEL = 200  # ëª¨ë¸ë‹¹ 200íšŒ (ì´ 1000íšŒ)

# í‰ê°€ ì§€í‘œ ìš°ì„ ìˆœìœ„
PRIMARY_METRIC = 'PR-AUC'  # ë¶ˆê· í˜• ë°ì´í„° í•µì‹¬ ì§€í‘œ
SECONDARY_METRIC = 'F2-Score'  # Recall ì¤‘ì‹œ
BUSINESS_CONSTRAINT = 'Type II Error < 20%'  # ë¶€ë„ ë¯¸íƒì§€ìœ¨
```

#### Section 1: ë°ì´í„° ë¡œë”© ë° 3-Way Split
```python
# âœ… ì˜¬ë°”ë¥¸ ë°ì´í„° ë¶„í• 
# 1ë‹¨ê³„: Test set ë¶„ë¦¬ (20%) - ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE
)

# 2ë‹¨ê³„: Train/Validation ë¶„ë¦¬ (60%/20%)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=VAL_SIZE, stratify=y_temp, random_state=RANDOM_STATE
)

# ìµœì¢… ë¹„ìœ¨: Train 60% / Validation 20% / Test 20%
print(f"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)")
print(f"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
print(f"ë¶€ë„ìœ¨ - Train: {y_train.mean():.3%}, Val: {y_val.mean():.3%}, Test: {y_test.mean():.3%}")
```

#### Section 2: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ê°œì„ )
```python
# âœ… ìˆœì„œ ìµœì í™” ë° Winsorizer ì¡°ì •
def create_preprocessing_pipeline():
    return Pipeline([
        ('imputer', IterativeImputer(max_iter=20, random_state=RANDOM_STATE, verbose=0)),  # ë¨¼ì € ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        ('winsorizer', Winsorizer(lower=0.005, upper=0.995)),  # 0.5%~99.5%ë¡œ ì™„í™” (ê·¹ë‹¨ê°’ ë³´ì¡´)
        ('log_transformer', LogTransformer()),
        ('scaler', StandardScaler())
    ])
```

**ì£¼ìš” ë³€ê²½ì **:
- `IterativeImputer` max_iter 10â†’20 (ìˆ˜ë ´ ì•ˆì •ì„±)
- `Winsorizer` 1%~99% â†’ 0.5%~99.5% (ë¶€ë„ ì˜ˆì¸¡ì—ì„œ ê·¹ë‹¨ê°’ì€ ì¤‘ìš” ì‹œê·¸ë„)
- ìˆœì„œ: Imputer â†’ Winsorizer â†’ Log â†’ Scaler

#### Section 3: ë¦¬ìƒ˜í”Œë§ ì „ëµ ë¹„êµ ì‹¤í—˜
```python
# âœ… ë¦¬ìƒ˜í”Œë§ vs Class Weight ëª…í™•í•œ ëŒ€ì¡°êµ° ì„¤ì •
resampling_strategies = {
    'baseline': None,  # ë¦¬ìƒ˜í”Œë§ ì—†ìŒ (Class Weightë§Œ)
    'smote': SMOTE(random_state=RANDOM_STATE),
    'borderline_smote': BorderlineSMOTE(random_state=RANDOM_STATE),
    'smote_tomek': SMOTETomek(random_state=RANDOM_STATE),
    'adasyn': ADASYN(random_state=RANDOM_STATE)
}

# ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ (Train set + CV)
# â†’ Validation setì—ì„œ ìµœì¢… ì„ íƒ
```

#### Section 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê°œì„ )
```python
# âœ… ëª¨ë¸ë³„ ê°œë³„ íŠœë‹ (ê° 200íšŒ)
models_to_tune = {
    'LightGBM': (LGBMClassifier, lgbm_param_dist),
    'XGBoost': (XGBClassifier, xgb_param_dist),
    'CatBoost': (CatBoostClassifier, cat_param_dist),
    'RandomForest': (RandomForestClassifier, rf_param_dist),  # ë‹¤ì–‘ì„± í™•ë³´
    'LogisticRegression': (LogisticRegression, lr_param_dist)  # ì´ì¢… ëª¨ë¸
}

all_results = []
for model_name, (model_class, param_dist) in models_to_tune.items():
    print(f"\n{'='*60}")
    print(f"ğŸ”§ {model_name} íŠœë‹ ì‹œì‘ (200íšŒ íƒìƒ‰)")
    print(f"{'='*60}")

    search = RandomizedSearchCV(
        create_pipeline(model_class),
        param_distributions=param_dist,
        n_iter=N_ITER_PER_MODEL,  # 200íšŒ
        cv=StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE),
        scoring='average_precision',  # PR-AUC
        n_jobs=-1,
        random_state=RANDOM_STATE,
        verbose=1
    )

    search.fit(X_train, y_train)
    all_results.append({
        'model': model_name,
        'best_params': search.best_params_,
        'cv_score': search.best_score_,
        'best_estimator': search.best_estimator_
    })
```

**ì£¼ìš” ë³€ê²½ì **:
- 100íšŒ â†’ ëª¨ë¸ë‹¹ 200íšŒ (ì´ 1,000íšŒ)
- RandomForest, LogisticRegression ì¶”ê°€ (ì•™ìƒë¸” ë‹¤ì–‘ì„± í™•ë³´)
- CV ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì²´ê³„ì  ì €ì¥

#### Section 5: ëª¨ë¸ ì„ íƒ (Validation Set í™œìš©) â­ í•µì‹¬!
```python
# âœ… Validation setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ (Test set ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€!)
print("\n" + "="*80)
print("ğŸ“Š Validation Set ê¸°ë°˜ ëª¨ë¸ í‰ê°€ ë° ì„ íƒ")
print("="*80)

# Top 5 ëª¨ë¸ì„ Validation setì—ì„œ í‰ê°€
top5_models = sorted(all_results, key=lambda x: x['cv_score'], reverse=True)[:5]

val_scores = []
for result in top5_models:
    model = result['best_estimator']
    y_prob_val = model.predict_proba(X_val)[:, 1]

    val_metrics = {
        'model': result['model'],
        'cv_score': result['cv_score'],
        'val_pr_auc': average_precision_score(y_val, y_prob_val),
        'val_roc_auc': roc_auc_score(y_val, y_prob_val)
    }
    val_scores.append(val_metrics)

val_df = pd.DataFrame(val_scores).sort_values('val_pr_auc', ascending=False)
print(val_df)

# ìµœê³  ì„±ëŠ¥ ë‹¨ì¼ ëª¨ë¸ ì„ íƒ
best_single_model = top5_models[0]['best_estimator']
best_single_val_score = val_df.iloc[0]['val_pr_auc']
```

#### Section 6: ì•™ìƒë¸” êµ¬ì„± ë° í†µê³„ì  ê²€ì¦
```python
# âœ… Top 3 ì´ì¢… ëª¨ë¸ë¡œ ì•™ìƒë¸” (ë‹¤ì–‘ì„± í™•ë³´)
# GBM ê³„ì—´ë§Œ ì„ íƒí•˜ì§€ ì•Šê³ , ìƒê´€ê³„ìˆ˜ ë‚®ì€ ëª¨ë¸ ì¡°í•©
top3_diverse = select_diverse_models(top5_models, n_models=3)

# Exponential Weighting (ì ìˆ˜ ì°¨ì´ ê°•ì¡°)
cv_scores = np.array([m['cv_score'] for m in top3_diverse])
cv_scores_norm = (cv_scores - cv_scores.min()) / (cv_scores.max() - cv_scores.min() + 1e-10)
weights = np.exp(cv_scores_norm * 5)
weights = weights / weights.sum()

voting_clf = VotingClassifier(
    estimators=[(f"model_{i}", m['best_estimator']) for i, m in enumerate(top3_diverse)],
    voting='soft',
    weights=weights
)
voting_clf.fit(X_train, y_train)

# Validation setì—ì„œ ì•™ìƒë¸” í‰ê°€
y_prob_val_voting = voting_clf.predict_proba(X_val)[:, 1]
voting_val_score = average_precision_score(y_val, y_prob_val_voting)

# âœ… í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (McNemar's test ë˜ëŠ” Paired t-test)
from scipy.stats import wilcoxon

# CV foldë³„ ì ìˆ˜ë¡œ paired test (ê°„ì ‘ ë¹„êµ)
# ë˜ëŠ” Validation setì—ì„œ bootstrap CI ê³„ì‚°
print(f"\në‹¨ì¼ ëª¨ë¸ Val PR-AUC: {best_single_val_score:.4f}")
print(f"ì•™ìƒë¸” Val PR-AUC: {voting_val_score:.4f}")
print(f"ì°¨ì´: {voting_val_score - best_single_val_score:.4f}")

# ìµœì¢… ì„ íƒ (ì‹¤ë¬´ì  íŒë‹¨: ë³µì¡ë„ vs ì„±ëŠ¥)
if voting_val_score > best_single_val_score + 0.005:  # 0.5% ì´ìƒ ê°œì„ 
    print("\nâœ… ì•™ìƒë¸” ì„ íƒ (ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ)")
    final_model = voting_clf
else:
    print("\nâœ… ë‹¨ì¼ ëª¨ë¸ ì„ íƒ (ë³µì¡ë„ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ ë¯¸ë¯¸, SHAP ë¶„ì„ ìš©ì´)")
    final_model = best_single_model
```

#### Section 7: ì„ê³„ê°’ ìµœì í™” (Validation Set) â­ í•µì‹¬!
```python
# âœ… Validation setì—ì„œ F2-optimal threshold ì°¾ê¸°
print("\n" + "="*80)
print("ğŸ¯ Validation Set ê¸°ë°˜ ì„ê³„ê°’ ìµœì í™” (F2-Score)")
print("="*80)

y_prob_val_final = final_model.predict_proba(X_val)[:, 1]
precisions, recalls, thresholds = precision_recall_curve(y_val, y_prob_val_final)

# F2-Score ê³„ì‚° (Recallì— 2ë°° ê°€ì¤‘ì¹˜)
f2_scores = (5 * precisions * recalls) / (4 * precisions + recalls + 1e-10)
f2_optimal_idx = np.argmax(f2_scores)
optimal_threshold_f2 = thresholds[f2_optimal_idx]

print(f"F2-optimal Threshold: {optimal_threshold_f2:.4f}")
print(f"í•´ë‹¹ ì§€ì  - Precision: {precisions[f2_optimal_idx]:.3f}, Recall: {recalls[f2_optimal_idx]:.3f}")

# Type II Error ì œì•½ í™•ì¸ (ë¶€ë„ ë¯¸íƒì§€ < 20%)
type2_error = 1 - recalls[f2_optimal_idx]
print(f"Type II Error (ë¶€ë„ ë¯¸íƒì§€ìœ¨): {type2_error:.1%}")

if type2_error > 0.20:
    print("âš ï¸ Type II Errorê°€ 20%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. Recall 80% ì§€ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
    recall_80_idx = np.argmin(np.abs(recalls - 0.80))
    optimal_threshold = thresholds[recall_80_idx]
else:
    optimal_threshold = optimal_threshold_f2

print(f"\nâœ… ìµœì¢… ì„ íƒëœ Threshold: {optimal_threshold:.4f}")
```

#### Section 8: Traffic Light ì‹œìŠ¤í…œ (ê°œì„ )
```python
# âœ… ë°ì´í„° ê¸°ë°˜ Yellow êµ¬ê°„ ì„¤ì •
# Red: F2-optimal (Type II < 20%)
# Yellow: Recall 95% ì»¤ë²„ ì§€ì  (ë¶€ë„ ê¸°ì—…ì˜ 95%ë¥¼ ìµœì†Œí•œ ê²½ê³ )

red_threshold = optimal_threshold
recall_95_idx = np.argmin(np.abs(recalls - 0.95))
yellow_threshold = thresholds[recall_95_idx]

print(f"ğŸš¦ Traffic Light ì‹œìŠ¤í…œ:")
print(f"  ğŸ”´ Red (ê³ ìœ„í—˜):   {red_threshold:.4f} ì´ìƒ")
print(f"  ğŸŸ¡ Yellow (ê²½ê³„): {yellow_threshold:.4f} ~ {red_threshold:.4f}")
print(f"  ğŸŸ¢ Green (ì•ˆì „):  {yellow_threshold:.4f} ë¯¸ë§Œ")
print(f"\ní•´ì„: Yellow ë“±ê¸‰ì€ ë¶€ë„ ê¸°ì—…ì˜ 95%ë¥¼ ì»¤ë²„í•˜ëŠ” ë°©ì–´ì„ ì…ë‹ˆë‹¤.")
```

#### Section 9: Test Set ìµœì¢… í‰ê°€ (ë‹¨ í•œ ë²ˆ!) â­ í•µì‹¬!
```python
# âœ… Test setì€ ìµœì¢… ë³´ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (ì˜ì‚¬ê²°ì • ê¸ˆì§€!)
print("\n" + "="*80)
print("ğŸ“Š TEST SET ìµœì¢… í‰ê°€ (Hold-out Performance)")
print("="*80)
print("âš ï¸ ì£¼ì˜: ì´ ê²°ê³¼ëŠ” unseen data ì„±ëŠ¥ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤.")
print("         ëª¨ë¸ ìˆ˜ì •ì´ë‚˜ ì¬íŠœë‹ì— ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!\n")

y_prob_test = final_model.predict_proba(X_test)[:, 1]
y_pred_test = (y_prob_test >= optimal_threshold).astype(int)

# ì¢…í•© í‰ê°€
test_metrics = {
    'PR-AUC': average_precision_score(y_test, y_prob_test),
    'ROC-AUC': roc_auc_score(y_test, y_prob_test),
    'F2-Score': fbeta_score(y_test, y_pred_test, beta=2),
    'Recall': recall_score(y_test, y_pred_test),
    'Precision': precision_score(y_test, y_pred_test),
    'Type II Error': 1 - recall_score(y_test, y_pred_test)
}

print("ìµœì¢… Test Set ì„±ëŠ¥:")
for metric, value in test_metrics.items():
    print(f"  {metric:20s}: {value:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_test)
print(f"\nConfusion Matrix:")
print(cm)
print(f"  TN: {cm[0,0]:,}  FP: {cm[0,1]:,}")
print(f"  FN: {cm[1,0]:,}  TP: {cm[1,1]:,}")
```

#### Section 10: ëª¨ë¸ ì €ì¥ ë° ì¬í˜„ì„± ë³´ì¥
```python
# âœ… ëª¨ë¸, ì „ì²˜ë¦¬, ì„ê³„ê°’ ëª¨ë‘ ì €ì¥
import joblib

model_artifacts = {
    'model': final_model,
    'optimal_threshold': optimal_threshold,
    'red_threshold': red_threshold,
    'yellow_threshold': yellow_threshold,
    'feature_names': X_train.columns.tolist(),
    'test_metrics': test_metrics,
    'random_state': RANDOM_STATE
}

joblib.dump(model_artifacts, '../data/processed/part3_final_model_artifacts.pkl')
print("âœ… ëª¨ë¸ ë° ì„¤ì • ì €ì¥ ì™„ë£Œ: part3_final_model_artifacts.pkl")
```

#### Section 11: ì‹œê°í™” (í•œê¸€ í°íŠ¸ ë³´ì¥)
```python
# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (CLAUDE.md ê·œì¹™ ì¤€ìˆ˜)
import platform
import matplotlib.pyplot as plt

if platform.system() == 'Darwin':
    plt.rc('font', family='AppleGothic')
elif platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
else:
    plt.rc('font', family='NanumGothic')
plt.rc('axes', unicode_minus=False)

# Precision-Recall Curve (Validation + Test ë¹„êµ)
# Confusion Matrix Heatmap
# Traffic Light ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
# ... (ê¸°ì¡´ ì‹œê°í™” ì½”ë“œ í™œìš©)
```

---

## ğŸ”’ CONSTRAINTS & REQUIREMENTS

### í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­ (ì ˆëŒ€ì )
1. âœ… **Zero Data Leakage**: Test setì€ Section 9ì—ì„œ ë‹¨ í•œ ë²ˆë§Œ ì‚¬ìš©
2. âœ… **Validation Set í•„ìˆ˜**: ëª¨ë“  ì˜ì‚¬ê²°ì •ì€ Validation set ê¸°ë°˜
3. âœ… **Reproducibility**: ëª¨ë“  random_state = 42 í†µì¼
4. âœ… **í•œê¸€ í°íŠ¸**: ëª¨ë“  ì‹œê°í™”ì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€ (CLAUDE.md ê·œì¹™)
5. âœ… **íŒŒì¼ ê²½ë¡œ**: `../data/` í˜•ì‹ ì‚¬ìš© (notebooks/ ê¸°ì¤€)

### ì½”ë”© ê·œì¹™ (CLAUDE.md ì¤€ìˆ˜)
```python
# âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
if 'ìœ„í—˜ê²½ë³´ë“±ê¸‰' in X.columns:
    X['ìœ„í—˜ê²½ë³´ë“±ê¸‰'] = X['ìœ„í—˜ê²½ë³´ë“±ê¸‰'].cat.codes

# âœ… ê²°ì¸¡ì¹˜ ë° ë¬´í•œëŒ€ ì²˜ë¦¬
X_filled = X.fillna(X.median())
X_filled = X_filled.replace([np.inf, -np.inf], 0)

# âœ… ë°ì´í„° ì¸ì½”ë”©
df = pd.read_csv('../data/ê¸°ì—…ì‹ ìš©í‰ê°€ì •ë³´_210801.csv', encoding='utf-8')
df.to_csv('../data/features/xxx.csv', encoding='utf-8-sig')
```

### í‰ê°€ ì§€í‘œ ìš°ì„ ìˆœìœ„ (ëª…ì‹œ)
```python
# ëª¨ë“  í‰ê°€ ê²°ê³¼ëŠ” ë‹¤ìŒ ìˆœì„œë¡œ ë³´ê³ 
metrics_priority = [
    'PR-AUC',        # Primary (ë¶ˆê· í˜• ë°ì´í„° í•µì‹¬)
    'F2-Score',      # Secondary (Recall ì¤‘ì‹œ)
    'Type II Error', # Business Constraint (< 20%)
    'Recall',        # ë¶€ë„ íƒì§€ìœ¨
    'Precision',     # ì˜¤íƒë¥ 
    'ROC-AUC'        # ì°¸ê³ ìš©
]
```

---

## ğŸ“Š OUTPUT FORMAT & DELIVERABLES

### ë…¸íŠ¸ë¶ êµ¬ì¡° (ìµœì¢…)
```
ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_ì™„ì „íŒ.ipynb

â”œâ”€â”€ Section 0: í™˜ê²½ ì„¤ì • ë° ì„í¬íŠ¸
â”œâ”€â”€ Section 1: ë°ì´í„° ë¡œë”© ë° 3-Way Split â­
â”œâ”€â”€ Section 2: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜ (ê°œì„ )
â”œâ”€â”€ Section 3: ë¦¬ìƒ˜í”Œë§ ì „ëµ ë¹„êµ ì‹¤í—˜
â”œâ”€â”€ Section 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ëª¨ë¸ë³„ 200íšŒ)
â”œâ”€â”€ Section 5: ëª¨ë¸ ì„ íƒ (Validation Set) â­
â”œâ”€â”€ Section 6: ì•™ìƒë¸” êµ¬ì„± ë° í†µê³„ì  ê²€ì¦
â”œâ”€â”€ Section 7: ì„ê³„ê°’ ìµœì í™” (Validation Set) â­
â”œâ”€â”€ Section 8: Traffic Light ì‹œìŠ¤í…œ (ë°ì´í„° ê¸°ë°˜)
â”œâ”€â”€ Section 9: Test Set ìµœì¢… í‰ê°€ (ë‹¨ í•œ ë²ˆ!) â­
â”œâ”€â”€ Section 10: ëª¨ë¸ ì €ì¥ ë° ì¬í˜„ì„± ë³´ì¥
â”œâ”€â”€ Section 11: ì¢…í•© ì‹œê°í™” (í•œê¸€ í°íŠ¸ ë³´ì¥)
â””â”€â”€ Section 12: ê²°ë¡  ë° ë°°í¬ ê°€ì´ë“œ
```

### í•„ìˆ˜ ì‚°ì¶œë¬¼
1. **ë…¸íŠ¸ë¶ íŒŒì¼**: `ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_ì™„ì „íŒ.ipynb`
2. **ëª¨ë¸ íŒŒì¼**: `../data/processed/part3_final_model_artifacts.pkl`
3. **ì‹¤í–‰ ê²°ê³¼**: Test set ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”)
4. **ì‹œê°í™”**: PR Curve, CM, Traffic Light ë¶„í¬ (í•œê¸€ ê¹¨ì§ ì—†ìŒ)

### ë¬¸ì„œí™” ìš”êµ¬ì‚¬í•­
- ê° Section ì‹œì‘ ì‹œ **ë§ˆí¬ë‹¤ìš´ ì„¤ëª…** (ë¬´ì—‡ì„, ì™œ, ì–´ë–»ê²Œ)
- ì½”ë“œ ì£¼ì„: `# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•`, `# âŒ í”¼í•´ì•¼ í•  ë°©ë²•` ëª…ì‹œ
- ì£¼ìš” ì˜ì‚¬ê²°ì • ì§€ì ì— **ê·¼ê±°** ëª…ì‹œ (ì˜ˆ: "Validation PR-AUC ê¸°ì¤€ ì„ íƒ")

---

## ğŸ’¡ EXAMPLES & ANTI-PATTERNS

### âœ… ì˜¬ë°”ë¥¸ íŒ¨í„´
```python
# 1. 3-way split
X_train, X_val, X_test = ...  # 60/20/20

# 2. Train setìœ¼ë¡œ CV íŠœë‹
search.fit(X_train, y_train)

# 3. Validation setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
val_score = model.score(X_val, y_val)

# 4. Validation setìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™”
optimal_threshold = find_threshold(X_val, y_val)

# 5. Test setì€ ìµœì¢… ë³´ê³ ë§Œ
test_score = model.score(X_test, y_test)  # ë!
```

### âŒ ì ˆëŒ€ ê¸ˆì§€ íŒ¨í„´
```python
# 1. Test setìœ¼ë¡œ ëª¨ë¸ ì„ íƒ (ê¸ˆì§€!)
if test_score_A > test_score_B:
    final_model = model_A  # â† Data Leakage!

# 2. Test setìœ¼ë¡œ ì„ê³„ê°’ ìµœì í™” (ê¸ˆì§€!)
threshold = optimize_f2(y_test, y_prob_test)  # â† Leakage!

# 3. Test set ë³´ê³  ëª¨ë¸ ì¬íŠœë‹ (ê¸ˆì§€!)
# "Testì—ì„œ ì„±ëŠ¥ ë‚®ë„¤? íŒŒë¼ë¯¸í„° ìˆ˜ì •í•˜ì" â† ì ˆëŒ€ ì•ˆ ë¨!
```

---

## ğŸš€ EXECUTION INSTRUCTIONS

### Step-by-Step ì‹¤í–‰ ê°€ì´ë“œ

1. **ê¸°ì¡´ Part 3 ë…¸íŠ¸ë¶ ì½ê¸°**
   - `notebooks/ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”.ipynb` ì „ì²´ ì½”ë“œ íŒŒì•…
   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜/í´ë˜ìŠ¤ ì‹ë³„ (Winsorizer, LogTransformer ë“±)

2. **ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±**
   - íŒŒì¼ëª…: `notebooks/ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_ì™„ì „íŒ.ipynb`
   - ì²« ì…€ì— ëª…ì‹œ: "âš ï¸ ì´ ë…¸íŠ¸ë¶ì€ Data Leakageë¥¼ ì™„ì „íˆ ì œê±°í•œ ê°œì„ íŒì…ë‹ˆë‹¤"

3. **Sectionë³„ êµ¬í˜„**
   - ìœ„ TASK SPECIFICATIONì˜ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¦„
   - ê° Sectionë§ˆë‹¤ ë§ˆí¬ë‹¤ìš´ ì„¤ëª… â†’ ì½”ë“œ â†’ ê²°ê³¼ ê²€ì¦

4. **ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**
   ```python
   # ë…¸íŠ¸ë¶ ì™„ì„± í›„ ë‹¤ìŒ ì§ˆë¬¸ì— ëª¨ë‘ "Yes"ì¸ì§€ í™•ì¸
   checklist = {
       "Test setì€ Section 9ì—ì„œë§Œ ì‚¬ìš©í–ˆëŠ”ê°€?": "Yes/No",
       "ëª¨ë“  ì˜ì‚¬ê²°ì •ì€ Validation set ê¸°ë°˜ì¸ê°€?": "Yes/No",
       "random_state=42ê°€ ëª¨ë“  ê³³ì— ì„¤ì •ë˜ì—ˆëŠ”ê°€?": "Yes/No",
       "í•œê¸€ í°íŠ¸ê°€ ëª¨ë“  ì‹œê°í™”ì— ì ìš©ë˜ì—ˆëŠ”ê°€?": "Yes/No",
       "Type II Error < 20% ì œì•½ì„ í™•ì¸í–ˆëŠ”ê°€?": "Yes/No"
   }
   ```

5. **ì‹¤í–‰ ë° ì €ì¥**
   - ì „ì²´ ì…€ ìˆœì°¨ ì‹¤í–‰ (Kernel Restart â†’ Run All)
   - ì—ëŸ¬ ì—†ì´ ì™„ë£Œ í™•ì¸
   - `_executed.ipynb` ë²„ì „ë„ ì €ì¥ (CLAUDE.md ê·œì¹™)

---

## ğŸ“ QUALITY ASSURANCE

### í•™ìˆ ì  ì—„ë°€ì„± ì²´í¬
- [ ] Data Leakage ì™„ì „ ì œê±°
- [ ] Statistical significance test ìˆ˜í–‰
- [ ] Baseline ëŒ€ì¡°êµ° ì„¤ì • (ë¦¬ìƒ˜í”Œë§ ì—†ëŠ” ë²„ì „)
- [ ] Cross-validation ì˜¬ë°”ë¥¸ ì‚¬ìš©

### ì‹¤ë¬´ ë°°í¬ ì¤€ë¹„ë„ ì²´í¬
- [ ] Reproducible (random_state í†µì œ)
- [ ] ëª¨ë¸ artifacts ì €ì¥ (.pkl)
- [ ] Traffic Light ì‹œìŠ¤í…œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
- [ ] í•œê¸€ í°íŠ¸ ê¹¨ì§ ë°©ì§€

### ì½”ë“œ í’ˆì§ˆ ì²´í¬
- [ ] CLAUDE.md ê·œì¹™ 100% ì¤€ìˆ˜
- [ ] ì£¼ì„ ë° ë¬¸ì„œí™” ì¶©ë¶„
- [ ] ë§¤ì§ ë„˜ë²„ ì—†ìŒ (ëª¨ë‘ ìƒìˆ˜í™”)
- [ ] í•˜ë“œì½”ë”© ì—†ìŒ

---

## ğŸ”¥ FINAL REMINDER

ì´ ë…¸íŠ¸ë¶ì€ **í¬íŠ¸í´ë¦¬ì˜¤ ë° í•™ìˆ ì  í‰ê°€**ì— ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
1. **"Test setì„ ì–´ë””ì— ì‚¬ìš©í–ˆë‚˜ìš”?"** â†’ "Section 9ì—ì„œ ìµœì¢… ë³´ê³ ë§Œ í–ˆìŠµë‹ˆë‹¤."
2. **"ëª¨ë¸ê³¼ ì„ê³„ê°’ì€ ì–´ë–»ê²Œ ì„ íƒí–ˆë‚˜ìš”?"** â†’ "Validation set ê¸°ë°˜ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤."
3. **"ì¬í˜„ ê°€ëŠ¥í•œê°€ìš”?"** â†’ "ë„¤, random_state=42ë¡œ í†µì œí–ˆìŠµë‹ˆë‹¤."
4. **"ì‹¤ë¬´ ë°°í¬ ê°€ëŠ¥í•œê°€ìš”?"** â†’ "ë„¤, Traffic Lightì™€ artifacts ì €ì¥í–ˆìŠµë‹ˆë‹¤."

---

**ì´ì œ ì‹œì‘í•˜ì„¸ìš”. ë‹¹ì‹ ì€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸš€**
