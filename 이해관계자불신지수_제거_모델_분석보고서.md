# ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì œê±° ëª¨ë¸ - ë¶„ì„ ë° ê°œì„  ë°©ì•ˆ

**ì‘ì„±ì¼**: 2025-11-23
**ë¶„ì„ ëŒ€ìƒ**: `notebooks/ë°œí‘œ_Part3_ëª¨ë¸ë§_ë°_ìµœì í™”_v3_ì™„ì „íŒ copy.ipynb`

---

## ğŸ“Š Executive Summary

### í•µì‹¬ ë°œê²¬

**âœ… ì¢‹ì€ ì†Œì‹: Test ì„±ëŠ¥ í–¥ìƒ**
- **ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì œê±° í›„ Test PR-AUC 0.1542 â†’ 0.1602 (3.9% í–¥ìƒ)**
- ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„ : ê³¼ì í•© ê°ì†Œ

**âš ï¸ ìš°ë ¤ ì‚¬í•­: Val-Test ê´´ë¦¬ ì‹¬í™”**
- Val PR-AUC: 0.1245
- Test PR-AUC: 0.1602
- **ê²©ì°¨: 28.7%** (ì´ì „ 2.0% â†’ 14ë°° ì¦ê°€)

### ê¶Œì¥ ì¡°ì¹˜

1. **ìš°ì„ ìˆœìœ„ 1**: Val-Test ê´´ë¦¬ ì›ì¸ ë¶„ì„ ë° êµì°¨ ê²€ì¦ ì „ëµ ê°œì„ 
2. **ìš°ì„ ìˆœìœ„ 2**: Feature Importance ë¶„ì„ í›„ ì‹ ìš©ë“±ê¸‰ ê´€ë ¨ íŠ¹ì„± ì •ë°€ ì¡°ì‚¬
3. **ìš°ì„ ìˆœìœ„ 3**: ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ êµ¬ì„± ìš”ì†Œ ë¶„í•´ í›„ ì¬ì‹¤í—˜

---

## 1. ì„±ëŠ¥ ë¹„êµ ë¶„ì„

### 1.1 ì´ì „ ëª¨ë¸ vs í˜„ì¬ ëª¨ë¸

| ëª¨ë¸ | ë³€ìˆ˜ | Val PR-AUC | Test PR-AUC | Val-Test Gap | Test Recall | Test F2 |
|-----|------|-----------|------------|-------------|-------------|---------|
| **Baseline (ì´ì „)** | ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ **í¬í•¨** (27ê°œ) | 0.1572 | 0.1542 | -2.0% | 80.26% | 0.2044 |
| **Current (í˜„ì¬)** | ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ **ì œê±°** (26ê°œ) | 0.1245 | 0.1602 | **+28.7%** | 86.84% | 0.2046 |

### 1.2 í•µì‹¬ ì¸ì‚¬ì´íŠ¸

#### âœ… ê¸ì •ì  ì¸¡ë©´

1. **Test ì„±ëŠ¥ í–¥ìƒ**
   - PR-AUC: 0.1542 â†’ 0.1602 (+3.9%)
   - Recall: 80.26% â†’ 86.84% (+6.58%p)
   - F2-Score: 0.2044 â†’ 0.2046 (+0.1%)

2. **ê³¼ì í•© ê°ì†Œ ì§•í›„**
   - ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ê°€ Valì—ì„œëŠ” ê°•ë ¥í•˜ì§€ë§Œ Testì—ì„œëŠ” ì˜¤íˆë ¤ ë°©í•´
   - ë‹¨ì¼ íŠ¹ì„± ì˜ì¡´ ì œê±° â†’ ëª¨ë¸ ì•ˆì •ì„± í–¥ìƒ

3. **ì‹¤ë¬´ ì„íŒ©íŠ¸ í–¥ìƒ**
   - Recall 86.84%: ë¶€ë„ ê¸°ì—…ì˜ 86.84% í¬ì°© (ì´ì „ 80.26%)
   - False Negative ê°ì†Œ: ë¶€ë„ ë¯¸íƒì§€ ìœ„í—˜ ê°ì†Œ

#### âš ï¸ ìš°ë ¤ ì‚¬í•­

1. **ì‹¬ê°í•œ Val-Test ê´´ë¦¬**
   ```
   ì´ì „ ëª¨ë¸: Val 0.1572 vs Test 0.1542 (2.0% ì°¨ì´)
   í˜„ì¬ ëª¨ë¸: Val 0.1245 vs Test 0.1602 (28.7% ì°¨ì´) â† 14ë°° ì¦ê°€
   ```

   **ì˜ë¯¸**: Validation Setì´ Test Setì„ ëŒ€í‘œí•˜ì§€ ëª»í•¨
   - ë°ì´í„° ë¶„í•  ë¬¸ì œ ê°€ëŠ¥ì„±
   - íŠ¹ì • íŠ¹ì„±ì˜ ë¶„í¬ ì°¨ì´
   - ì„ê³„ê°’ ì„ íƒ ì „ëµ ì¬ê²€í†  í•„ìš”

2. **Val ì„±ëŠ¥ í•˜ë½**
   - Val PR-AUC: 0.1572 â†’ 0.1245 (20.8% í•˜ë½)
   - ì˜ˆì¸¡ëŒ€ë¡œ ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì œê±° ì‹œ ì„±ëŠ¥ í•˜ë½ í™•ì¸

3. **ëª¨ë¸ ì„ íƒ ì‹ ë¢°ë„ ë¬¸ì œ**
   - Val ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ì„ ì„ íƒí–ˆìœ¼ë‚˜ Testì—ì„œëŠ” ë‹¤ë¥¸ ì–‘ìƒ
   - AutoML ê²°ê³¼ ì‹ ë¢°ì„± ì˜ë¬¸

---

## 2. AutoML ê²°ê³¼ ìƒì„¸ ë¶„ì„

### 2.1 ëª¨ë¸ë³„ ì„±ëŠ¥

| ëª¨ë¸ | CV PR-AUC | Val PR-AUC | CV-Val Gap | ë¹„ê³  |
|-----|----------|-----------|-----------|------|
| **CatBoost** | **0.1607** | **0.1245** | **-22.5%** | ìµœì¢… ì„ íƒ |
| RandomForest | 0.1559 | 0.1005 | -35.5% | ì‹¬ê°í•œ ê³¼ì í•© |
| XGBoost | 0.1516 | 0.1118 | -26.3% | ì¼ê´€ëœ ê³¼ì í•© |
| ExtraTrees | 0.1250 | 0.1224 | -2.1% | ê°€ì¥ ì•ˆì •ì  |
| LightGBM | 0.1238 | 0.1101 | -11.1% | ì¤‘ê°„ ìˆ˜ì¤€ |
| LogisticRegression | 0.0470 | 0.0268 | -43.0% | ì„±ëŠ¥ ë§¤ìš° ë‚®ìŒ |

### 2.2 ë¬¸ì œì  ì§„ë‹¨

#### ğŸš¨ Problem 1: ì „ë°˜ì ì¸ ê³¼ì í•©

**ëª¨ë“  ëª¨ë¸ì—ì„œ CV > Val ì„±ëŠ¥ í•˜ë½ í™•ì¸**
- CatBoost: CV 0.1607 â†’ Val 0.1245 (22.5% í•˜ë½)
- RandomForest: CV 0.1559 â†’ Val 0.1005 (35.5% í•˜ë½)

**ì›ì¸ ë¶„ì„**:
1. **SMOTE ì ìš© ë°©ì‹ ë¬¸ì œ**
   - SMOTEë¥¼ Train Setì—ë§Œ ì ìš© â†’ CVì—ì„œëŠ” í•©ì„± ìƒ˜í”Œë¡œ í‰ê°€
   - Val Setì€ ì›ë³¸ ë°ì´í„° â†’ ì„±ëŠ¥ í•˜ë½

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³¼ìµœì í™”**
   - n_iter=50 (ì½”ë“œì—ì„œ í™•ì¸)ìœ¼ë¡œ ì¶©ë¶„í•œ íƒìƒ‰
   - ê·¸ëŸ¬ë‚˜ CV ì„±ëŠ¥ì—ë§Œ ê³¼ë„í•˜ê²Œ ìµœì í™”
   - Val, Testì—ëŠ” ì¼ë°˜í™”ë˜ì§€ ì•ŠìŒ

3. **ë°ì´í„° ë¶„í•  ì „ëµ ë¯¸í¡**
   - ë‹¨ìˆœ Stratified Split (60/20/20)
   - ì‹œê°„ ê¸°ë°˜ ê²€ì¦ ì—†ìŒ (2021ë…„ 8ì›” ë‹¨ì¼ ìŠ¤ëƒ…ìƒ·)
   - ì—…ì¢…/ê·œëª¨ë³„ stratification ë¯¸ì ìš©

#### ğŸš¨ Problem 2: Voting Ensemble íš¨ê³¼ ë¯¸ë¯¸

```
CatBoost (Single): Val PR-AUC 0.1245
Voting Ensemble:   Val PR-AUC 0.1263 (+0.0018, 1.4% í–¥ìƒ)
Wilcoxon p-value:  0.0625 (ìœ ì˜í•˜ì§€ ì•ŠìŒ)
```

**ì›ì¸**:
- ìƒìœ„ 3ê°œ ëª¨ë¸ (CatBoost, RandomForest, XGBoost) ëª¨ë‘ Tree ê¸°ë°˜
- ëª¨ë¸ ë‹¤ì–‘ì„± ë¶€ì¡± â†’ Ensemble íš¨ê³¼ ì œí•œì 
- LogisticRegression ì„±ëŠ¥ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ì œì™¸ë¨

**ê°œì„  ë°©í–¥**:
- Stacking Ensemble (Meta-learner ì‚¬ìš©)
- Diverse base models (Tree + Linear + Neural Net)

#### ğŸš¨ Problem 3: íŠ¹ì„± ì„ íƒ ì „ëµ ì—†ìŒ

**í˜„ì¬ ìƒíƒœ**:
- ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ë§Œ ì œê±°
- ë‚˜ë¨¸ì§€ 26ê°œ íŠ¹ì„± ëª¨ë‘ ì‚¬ìš©
- VIF ë¶„ì„ ê²°ê³¼ ë¬´ì‹œ:
  - ì‹ ìš©ë“±ê¸‰ì ìˆ˜: VIF 23.24 (ì—¬ì „íˆ ë†’ìŒ)
  - ìœ ë™ì„±ì••ë°•ì§€ìˆ˜ â†” ìš´ì „ìë³¸_ëŒ€_ìì‚°: r = -0.892

**ì˜ˆìƒ ë¬¸ì œ**:
- ì‹ ìš©ë“±ê¸‰ì ìˆ˜ê°€ ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì—­í•  ëŒ€ì²´
- ë‹¤ì¤‘ê³µì„ ì„± ì—¬ì „íˆ ì¡´ì¬
- Feature Importance í™•ì¸ í•„ìš”

---

## 3. Feature Importance ë¶„ì„ (CatBoost)

### 3.1 ë…¸íŠ¸ë¶ ê²°ê³¼ í™•ì¸

**ë…¸íŠ¸ë¶ Cell 44ì—ì„œ Feature Importance ì‹œê°í™” ìƒì„±ë¨**
- CatBoostëŠ” `feature_importances_` ì†ì„± ë³´ìœ 
- Top 15 íŠ¹ì„± í‘œì‹œ

### 3.2 ì˜ˆìƒ ì¤‘ìš” íŠ¹ì„± (ì¶”ì •)

**ë…¸íŠ¸ë¶ì— ì‹¤í–‰ ê²°ê³¼ê°€ ì—†ì–´ì„œ ì¶”ì •í•˜ë©´**:

ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì œê±° í›„ ì˜ˆìƒ ìƒìœ„ íŠ¹ì„±:
1. **ì‹ ìš©ë“±ê¸‰ì ìˆ˜** (r=0.891ë¡œ ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ì™€ ê±°ì˜ ë™ì¼ ì •ë³´)
2. ì—°ì²´ì‹¬ê°ë„
3. ê³µê³µì •ë³´ë¦¬ìŠ¤í¬
4. OCF_ëŒ€_ìœ ë™ë¶€ì±„
5. í˜„ê¸ˆì°½ì¶œëŠ¥ë ¥

**âš ï¸ ê²€ì¦ í•„ìš”**:
- ì‹¤ì œ Feature Importance í™•ì¸
- ì‹ ìš©ë“±ê¸‰ì ìˆ˜ê°€ 1ìœ„ì¸ì§€ í™•ì¸
- 1ìœ„ íŠ¹ì„±ì˜ importance ë¹„ìœ¨ (ë…ì  ì—¬ë¶€)

---

## 4. Val-Test ê´´ë¦¬ ì›ì¸ ë¶„ì„

### 4.1 ê°€ëŠ¥í•œ ì›ì¸

#### ì›ì¸ 1: ë°ì´í„° ë¶„í•  ìš´ (Lucky Split)

**ì¦ìƒ**:
- Valì—ì„œëŠ” CatBoostê°€ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ìƒ˜í”Œ ë§ìŒ
- Testì—ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ìƒ˜í”Œ ë§ìŒ

**ê²€ì¦ ë°©ë²•**:
```python
# K-Fold Cross Validationìœ¼ë¡œ ì¬ê²€ì¦
from sklearn.model_selection import StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
cv_scores = []

for train_idx, val_idx in cv.split(X, y):
    X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
    y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    model.fit(X_train_cv, y_train_cv)
    pr_auc = average_precision_score(y_val_cv, model.predict_proba(X_val_cv)[:, 1])
    cv_scores.append(pr_auc)

print(f'CV í‰ê· : {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}')
```

#### ì›ì¸ 2: íŠ¹ì„± ë¶„í¬ ì°¨ì´

**ê°€ì„¤**:
- Val Setê³¼ Test Setì—ì„œ ì‹ ìš©ë“±ê¸‰ì ìˆ˜ ë¶„í¬ê°€ ë‹¤ë¦„
- ì—…ì¢…/ê·œëª¨ ë¶„í¬ ì°¨ì´
- ë¶€ë„ ì‹œì  ì°¨ì´ (ë°ì´í„° ìˆ˜ì§‘ ì‹œì  ë¬¸ì œ)

**ê²€ì¦ ë°©ë²•**:
```python
# Val vs Test ë¶„í¬ ë¹„êµ
import scipy.stats as stats

for col in X_train.columns:
    ks_stat, p_value = stats.ks_2samp(X_val[col], X_test[col])
    if p_value < 0.05:
        print(f'{col}: p={p_value:.4f} (ë¶„í¬ ì°¨ì´ ìœ ì˜)')
```

#### ì›ì¸ 3: SMOTE ë¶€ì‘ìš©

**ê°€ì„¤**:
- SMOTEë¡œ ìƒì„±ëœ í•©ì„± ìƒ˜í”Œì´ CV í‰ê°€ì— ì˜í–¥
- Val Setì€ ì›ë³¸ë§Œ í¬í•¨ â†’ ì„±ëŠ¥ í•˜ë½
- Test Setë„ ì›ë³¸ë§Œ í¬í•¨ â†’ Valê³¼ ë™ì¼í•´ì•¼ í•˜ëŠ”ë° ì˜¤íˆë ¤ ë†’ìŒ

**ê²€ì¦ ë°©ë²•**:
```python
# SMOTE ì—†ì´ ì¬í•™ìŠµ
pipe_no_smote = create_pipeline(CatBoostClassifier(...), resamp=None)
pipe_no_smote.fit(X_train, y_train)

val_pr = average_precision_score(y_val, pipe_no_smote.predict_proba(X_val)[:, 1])
test_pr = average_precision_score(y_test, pipe_no_smote.predict_proba(X_test)[:, 1])
print(f'Val: {val_pr:.4f}, Test: {test_pr:.4f}, Gap: {(test_pr-val_pr)/val_pr*100:.1f}%')
```

#### ì›ì¸ 4: ì„ê³„ê°’ ì„ íƒ ì „ëµ

**í˜„ì¬ ì „ëµ**:
- Val Recall 80% ì„ê³„ê°’: 0.0468
- CV Recall 80% ì„ê³„ê°’: 0.0525
- **ìµœì¢… ì„ê³„ê°’: 0.0497 (í‰ê· )**

**ë¬¸ì œ**:
- ì„ê³„ê°’ì´ Val+CV ê¸°ì¤€ìœ¼ë¡œ ì„ íƒë¨
- Testì— ìµœì í™”ë˜ì§€ ì•ŠìŒ
- í•˜ì§€ë§Œ PR-AUCëŠ” ì„ê³„ê°’ ë…ë¦½ì  ì§€í‘œ â†’ ì´ ë¬¸ì œëŠ” ì•„ë‹˜

---

## 5. ê°œì„  ë°©ì•ˆ (Priorityë³„)

### ğŸ”´ Priority 1: Val-Test ê´´ë¦¬ í•´ì†Œ (Critical)

#### ë°©ì•ˆ 1-1: Nested Cross-Validation

**ëª©ì **: ëª¨ë¸ ì„ íƒ í¸í–¥ ì œê±°

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Outer CV: ëª¨ë¸ í‰ê°€
outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Inner CV: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

nested_scores = []

for train_idx, test_idx in outer_cv.split(X, y):
    X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]
    y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]

    # Inner loop: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    search = RandomizedSearchCV(
        pipe, param_grid, cv=inner_cv,
        scoring='average_precision', n_iter=50
    )
    search.fit(X_train_outer, y_train_outer)

    # Outer loop: í‰ê°€
    pr_auc = average_precision_score(
        y_test_outer,
        search.best_estimator_.predict_proba(X_test_outer)[:, 1]
    )
    nested_scores.append(pr_auc)

print(f'Nested CV PR-AUC: {np.mean(nested_scores):.4f} Â± {np.std(nested_scores):.4f}')
```

#### ë°©ì•ˆ 1-2: Stratified Split ê°œì„ 

**í˜„ì¬ ë¬¸ì œ**:
- ë‹¨ìˆœ ë¶€ë„ ì—¬ë¶€ë§Œ stratify
- ì—…ì¢…, ê¸°ì—… ê·œëª¨, ì‹ ìš©ë“±ê¸‰ ë¶„í¬ ë¬´ì‹œ

**ê°œì„ ì•ˆ**:
```python
from sklearn.model_selection import StratifiedShuffleSplit

# ë³µí•© Stratification
# 1. ë¶€ë„ ì—¬ë¶€ (0/1)
# 2. ì‹ ìš©ë“±ê¸‰ ê·¸ë£¹ (1~10)
# 3. ì—…ì¢… ëŒ€ë¶„ë¥˜ (ì œì¡°/ë¹„ì œì¡°)

df['strat_key'] = (
    df['ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'].astype(str) + '_' +
    df['ì‹ ìš©ë“±ê¸‰_ê·¸ë£¹'].astype(str) + '_' +
    df['ì œì¡°ì—…ì—¬ë¶€'].astype(str)
)

sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in sss.split(X, df['strat_key']):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
```

#### ë°©ì•ˆ 1-3: SMOTE ì „ëµ ì¬ê²€í† 

**ì‹¤í—˜ ê³„íš**:
1. **Baseline**: SMOTE sampling_strategy=0.2 (í˜„ì¬)
2. **Variant 1**: SMOTE sampling_strategy=0.5
3. **Variant 2**: SMOTE + ENN (Edited Nearest Neighbors)
4. **Variant 3**: ADASYN (Adaptive Synthetic Sampling)
5. **Variant 4**: Class Weightë§Œ ì‚¬ìš© (SMOTE ì œê±°)

---

### ğŸŸ  Priority 2: Feature Engineering ê°œì„ 

#### ë°©ì•ˆ 2-1: ì‹ ìš©ë“±ê¸‰ì ìˆ˜ íŠ¹ì„± ë¶„í•´

**í˜„ì¬ ë¬¸ì œ**:
- ì‹ ìš©ë“±ê¸‰ì ìˆ˜ê°€ ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì—­í•  ëŒ€ì²´ ê°€ëŠ¥ì„±
- VIF 23.24ë¡œ ì—¬ì „íˆ ë†’ìŒ

**ê°œì„ ì•ˆ**:
```python
# ì‹ ìš©ë“±ê¸‰ì„ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜
df['ì‹ ìš©ë“±ê¸‰_High'] = (df['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] >= 8).astype(int)  # AAA, AA
df['ì‹ ìš©ë“±ê¸‰_Mid'] = ((df['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] >= 5) & (df['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] < 8)).astype(int)  # A, BBB
df['ì‹ ìš©ë“±ê¸‰_Low'] = (df['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] < 5).astype(int)  # BB ì´í•˜

# ë˜ëŠ” One-Hot Encoding
ì‹ ìš©ë“±ê¸‰_dummies = pd.get_dummies(df['ì‹ ìš©ë“±ê¸‰_ê·¸ë£¹'], prefix='ì‹ ìš©ë“±ê¸‰')

# ì›ë³¸ ì‹ ìš©ë“±ê¸‰ì ìˆ˜ ì œê±°
X = X.drop(columns=['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'])
```

#### ë°©ì•ˆ 2-2: ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ êµ¬ì„± ìš”ì†Œ í™œìš©

**ì „ëµ**:
- ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì „ì²´ëŠ” ì œê±°í•˜ë˜
- êµ¬ì„± ìš”ì†Œ 4ê°œëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©

```python
# ì´ë¯¸ ë°ì´í„°ì— í¬í•¨ëœ êµ¬ì„± ìš”ì†Œ í™•ì¸
êµ¬ì„±ìš”ì†Œ = [
    'ì—°ì²´ì—¬ë¶€',        # ë˜ëŠ” 'ì—°ì²´ì‹¬ê°ë„'
    'ì„¸ê¸ˆì²´ë‚©ë¦¬ìŠ¤í¬',  # ë˜ëŠ” 'ê³µê³µì •ë³´ë¦¬ìŠ¤í¬'ì˜ ì¼ë¶€
    'ë²•ì ë¦¬ìŠ¤í¬',      # ì••ë¥˜/ì†Œì†¡ ê´€ë ¨
    'ì‹ ìš©ë“±ê¸‰ì ìˆ˜'     # ì´ë¯¸ ë³„ë„ íŠ¹ì„±ìœ¼ë¡œ ì¡´ì¬
]

# ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ì™€ êµ¬ì„± ìš”ì†Œ ëª¨ë‘ ì œê±°
X = X.drop(columns=['ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜'])

# êµ¬ì„± ìš”ì†ŒëŠ” ìœ ì§€ (ì¤‘ë³µ ì—†ì´)
# ì´ë¯¸ 26ê°œ íŠ¹ì„±ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”
```

#### ë°©ì•ˆ 2-3: VIF ê¸°ë°˜ íŠ¹ì„± ì„ íƒ

**ë‹¨ê³„**:
1. VIF > 10 íŠ¹ì„± ì œê±°
   - ì‹ ìš©ë“±ê¸‰ì ìˆ˜ (VIF 23.24) â†’ ì œê±° ë˜ëŠ” ë³€í™˜

2. ìƒê´€ê³„ìˆ˜ |r| > 0.8 ìŒ ì²˜ë¦¬
   - ìœ ë™ì„±ì••ë°•ì§€ìˆ˜ â†” ìš´ì „ìë³¸_ëŒ€_ìì‚° (r=-0.892)
   - ë‘˜ ì¤‘ í•˜ë‚˜ ì œê±° ë˜ëŠ” PCA

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

# VIF ê³„ì‚°
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# VIF > 10 íŠ¹ì„± ì œê±°
high_vif_features = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()
X_reduced = X.drop(columns=high_vif_features)

print(f'ì œê±°ëœ íŠ¹ì„± ({len(high_vif_features)}ê°œ): {high_vif_features}')
```

#### ë°©ì•ˆ 2-4: Feature Selection (RFE, LASSO)

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# 1. RFE (Recursive Feature Elimination)
lr = LogisticRegression(penalty='l2', C=1, random_state=42, max_iter=1000)
rfe = RFE(estimator=lr, n_features_to_select=15, step=1)
rfe.fit(X_train, y_train)

selected_features = X_train.columns[rfe.support_].tolist()
print(f'RFE ì„ íƒ íŠ¹ì„± ({len(selected_features)}ê°œ): {selected_features}')

# 2. LASSO (L1 ì •ê·œí™”)
from sklearn.linear_model import LassoCV

lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_train, y_train)

lasso_coefs = pd.Series(lasso.coef_, index=X_train.columns)
selected_lasso = lasso_coefs[lasso_coefs != 0].index.tolist()
print(f'LASSO ì„ íƒ íŠ¹ì„± ({len(selected_lasso)}ê°œ): {selected_lasso}')
```

---

### ğŸŸ¡ Priority 3: ëª¨ë¸ ê°œì„ 

#### ë°©ì•ˆ 3-1: Stacking Ensemble

**í˜„ì¬ Votingì˜ í•œê³„**:
- ë‹¨ìˆœ í‰ê·  (soft voting)
- ëª¨ë¸ ë‹¤ì–‘ì„± ë¶€ì¡±

**Stacking ì¥ì **:
- Meta-learnerê°€ ê° ëª¨ë¸ì˜ ì¥ë‹¨ì  í•™ìŠµ
- ë‹¤ì–‘í•œ ëª¨ë¸ ì¡°í•© ê°€ëŠ¥

```python
from sklearn.ensemble import StackingClassifier

# Level 0 (Base Models)
base_models = [
    ('catboost', CatBoostClassifier(...)),
    ('xgboost', XGBClassifier(...)),
    ('rf', RandomForestClassifier(...)),
    ('lr', LogisticRegression(...))  # ë‹¤ì–‘ì„± í™•ë³´
]

# Level 1 (Meta-learner)
meta_model = LogisticRegression(penalty='l2', C=1, random_state=42)

# Stacking
stacking = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5,
    passthrough=False  # Base model ì˜ˆì¸¡ë§Œ ì‚¬ìš©
)

# íŒŒì´í”„ë¼ì¸ì— í†µí•©
stacking_pipe = create_pipeline(stacking, wins=False, resamp='smote')
stacking_pipe.fit(X_train, y_train)

# í‰ê°€
val_pr = average_precision_score(y_val, stacking_pipe.predict_proba(X_val)[:, 1])
test_pr = average_precision_score(y_test, stacking_pipe.predict_proba(X_test)[:, 1])
print(f'Stacking - Val: {val_pr:.4f}, Test: {test_pr:.4f}')
```

#### ë°©ì•ˆ 3-2: CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬íŠœë‹

**í˜„ì¬ ë¬¸ì œ**:
- CV-Val ì„±ëŠ¥ ì°¨ì´ 22.5% (ê³¼ì í•©)

**ì¬íŠœë‹ ì „ëµ**:
```python
# ê³¼ì í•© ë°©ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°•í™”
catboost_grid_conservative = {
    'clf__iterations': [100, 150],           # ì¤„ì„ (ê¸°ì¡´ 100~300)
    'clf__depth': [4, 5],                    # ì¤„ì„ (ê¸°ì¡´ 4~8)
    'clf__learning_rate': [0.01, 0.03],      # ì¤„ì„ (ê¸°ì¡´ 0.01~0.1)
    'clf__l2_leaf_reg': [3, 5, 10],          # ê°•í™” (ê¸°ì¡´ 1~5)
    'clf__bagging_temperature': [0, 0.5, 1], # ì¶”ê°€ (ëœë¤ì„± ì¦ê°€)
    'clf__random_strength': [0.5, 1, 2],     # ì¶”ê°€ (ê³¼ì í•© ë°©ì§€)
    'clf__scale_pos_weight': [1]             # SMOTE ì‚¬ìš© ì‹œ
}

# Early Stopping ì¶”ê°€
pipe_catboost = create_pipeline(
    CatBoostClassifier(
        random_state=42,
        verbose=0,
        early_stopping_rounds=20,  # Early stopping
        eval_metric='Precision'    # PR-AUC ìµœì í™”
    ),
    wins=False,
    resamp='smote'
)
```

#### ë°©ì•ˆ 3-3: LightGBM DART ëª¨ë“œ

**DART**: Dropouts meet Multiple Additive Regression Trees
- Tree dropoutìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
- Ensemble ë‹¤ì–‘ì„± ì¦ê°€

```python
lgbm_dart = lgb.LGBMClassifier(
    boosting_type='dart',      # DART ëª¨ë“œ
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05,
    drop_rate=0.1,             # Tree dropout ë¹„ìœ¨
    skip_drop=0.5,
    random_state=42
)
```

---

## 6. ì‹¤í—˜ ìš°ì„ ìˆœìœ„ ë¡œë“œë§µ

### Week 1: ê¸´ê¸‰ ì§„ë‹¨ (Val-Test ê´´ë¦¬ ì›ì¸ íŒŒì•…)

**ì‹¤í—˜ 1-1: K-Fold CV ì¬ê²€ì¦**
```python
# 5-Fold CVë¡œ ì„±ëŠ¥ ë¶„ì‚° í™•ì¸
cv_scores = cross_val_score(
    final_model, X, y,
    cv=StratifiedKFold(5, shuffle=True, random_state=42),
    scoring='average_precision'
)
print(f'CV: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}')
print(f'Individual folds: {cv_scores}')
```

**ê¸°ëŒ€ ê²°ê³¼**:
- CV í‰ê· ì´ Val(0.1245)ê³¼ Test(0.1602) ì‚¬ì´ì— ìˆìœ¼ë©´ â†’ ë°ì´í„° ë¶„í•  ë¬¸ì œ
- CV ë¶„ì‚°ì´ í¬ë©´ (Â±0.03 ì´ìƒ) â†’ ëª¨ë¸ ë¶ˆì•ˆì •

**ì‹¤í—˜ 1-2: Val vs Test ë¶„í¬ ë¹„êµ**
```python
# KS-Testë¡œ ë¶„í¬ ì°¨ì´ ê²€ì •
significant_diffs = []
for col in X.columns:
    ks_stat, p_val = stats.ks_2samp(X_val[col], X_test[col])
    if p_val < 0.05:
        significant_diffs.append((col, p_val))

print(f'ë¶„í¬ ì°¨ì´ ìœ ì˜í•œ íŠ¹ì„±: {len(significant_diffs)}ê°œ')
for col, p in sorted(significant_diffs, key=lambda x: x[1])[:5]:
    print(f'  {col}: p={p:.4f}')
```

**ê¸°ëŒ€ ê²°ê³¼**:
- ìœ ì˜í•œ ì°¨ì´ê°€ 5ê°œ ì´ìƒì´ë©´ â†’ Stratified Split ê°œì„  í•„ìš”
- ì‹ ìš©ë“±ê¸‰ì ìˆ˜ì—ì„œ ì°¨ì´ ë°œê²¬ ì‹œ â†’ íŠ¹ì„± ì¬ì„¤ê³„ í•„ìš”

**ì‹¤í—˜ 1-3: SMOTE ì œê±° ì‹¤í—˜**
```python
# SMOTE ì—†ì´ Class Weightë§Œ ì‚¬ìš©
pipe_no_smote = create_pipeline(
    CatBoostClassifier(scale_pos_weight=66.5, ...),
    resamp=None
)
pipe_no_smote.fit(X_train, y_train)

print(f'Val:  {val_pr:.4f}')
print(f'Test: {test_pr:.4f}')
print(f'Gap:  {abs(test_pr - val_pr)/val_pr*100:.1f}%')
```

**ê¸°ëŒ€ ê²°ê³¼**:
- Gap < 10%ì´ë©´ â†’ SMOTE ë¶€ì‘ìš© í™•ì¸, Class Weight ì „í™˜
- Gap > 20%ì´ë©´ â†’ SMOTE ë¬¸ì œ ì•„ë‹˜, ë‹¤ë¥¸ ì›ì¸ íƒìƒ‰

---

### Week 2: Feature Engineering

**ì‹¤í—˜ 2-1: ì‹ ìš©ë“±ê¸‰ì ìˆ˜ ë³€í™˜**
```python
# One-Hot Encoding
X['ì‹ ìš©ë“±ê¸‰_1~3'] = (X['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] <= 3).astype(int)
X['ì‹ ìš©ë“±ê¸‰_4~6'] = ((X['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] > 3) & (X['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] <= 6)).astype(int)
X['ì‹ ìš©ë“±ê¸‰_7~10'] = (X['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'] > 6).astype(int)
X = X.drop(columns=['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'])

# ì¬í•™ìŠµ ë° í‰ê°€
model.fit(X_train, y_train)
print(f'Val: {val_pr:.4f}, Test: {test_pr:.4f}')
```

**ì‹¤í—˜ 2-2: VIF ê¸°ë°˜ íŠ¹ì„± ì œê±°**
```python
# VIF > 10 íŠ¹ì„± ì œê±°
X_reduced = X.drop(columns=['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'])  # VIF 23.24

# ìœ ë™ì„± ì¤‘ë³µ íŠ¹ì„± ì œê±°
X_reduced = X_reduced.drop(columns=['ìœ ë™ì„±ì••ë°•ì§€ìˆ˜'])  # r=-0.892

# ì¬í•™ìŠµ
model.fit(X_train_reduced, y_train)
print(f'íŠ¹ì„± ìˆ˜: {X_reduced.shape[1]}')
print(f'Val: {val_pr:.4f}, Test: {test_pr:.4f}')
```

**ì‹¤í—˜ 2-3: RFEë¡œ ìµœì  íŠ¹ì„± ìˆ˜ íƒìƒ‰**
```python
# 10~25ê°œ ë²”ìœ„ì—ì„œ ìµœì  íŠ¹ì„± ìˆ˜ ì°¾ê¸°
for n in [10, 15, 20, 25]:
    rfe = RFE(LogisticRegression(...), n_features_to_select=n)
    rfe.fit(X_train, y_train)
    X_selected = X_train.iloc[:, rfe.support_]

    # CatBoost ì¬í•™ìŠµ
    model.fit(X_selected, y_train)
    val_pr = ...
    print(f'n={n}: Val PR-AUC={val_pr:.4f}')
```

---

### Week 3: ëª¨ë¸ ê°œì„ 

**ì‹¤í—˜ 3-1: Stacking Ensemble**
```python
stacking = StackingClassifier(
    estimators=[
        ('catboost', best_catboost),
        ('xgboost', best_xgboost),
        ('lightgbm', best_lightgbm),
        ('lr', LogisticRegression(penalty='l2', C=0.1))
    ],
    final_estimator=LogisticRegression(penalty='l2', C=1),
    cv=5
)
```

**ì‹¤í—˜ 3-2: CatBoost Early Stopping**
```python
catboost = CatBoostClassifier(
    iterations=500,
    early_stopping_rounds=50,
    eval_set=(X_val, y_val),
    verbose=50
)
catboost.fit(X_train, y_train)
print(f'Best iteration: {catboost.best_iteration_}')
```

**ì‹¤í—˜ 3-3: LightGBM DART**
```python
lgbm_dart = lgb.LGBMClassifier(
    boosting_type='dart',
    drop_rate=0.1,
    max_drop=50,
    ...
)
```

---

### Week 4: ì¢…í•© í‰ê°€ ë° ìµœì¢… ì„ íƒ

**ë¹„êµ ê¸°ì¤€**:
| ëª¨ë¸ | Val PR-AUC | Test PR-AUC | Val-Test Gap | Recall | F2-Score |
|-----|-----------|------------|-------------|--------|----------|
| Baseline (ì´í•´ê´€ê³„ì í¬í•¨) | 0.1572 | 0.1542 | 2.0% | 80.3% | 0.2044 |
| Current (ì œê±°) | 0.1245 | 0.1602 | 28.7% | 86.8% | 0.2046 |
| Improved 1 (VIF ì œê±°) | ? | ? | ? | ? | ? |
| Improved 2 (Stacking) | ? | ? | ? | ? | ? |
| Improved 3 (RFE) | ? | ? | ? | ? | ? |

**ìµœì¢… ì„ íƒ ê¸°ì¤€**:
1. **Val-Test Gap < 10%** (í•„ìˆ˜)
2. **Test PR-AUC â‰¥ 0.15** (ëª©í‘œ)
3. **Test Recall â‰¥ 80%** (ì‹¤ë¬´ ìš”êµ¬ì‚¬í•­)

---

## 7. ì½”ë“œ ê°œì„  ì œì•ˆ

### 7.1 ë°ì´í„° ë¶„í•  ê°œì„ 

**í˜„ì¬ ì½”ë“œ** (Cell 5):
```python
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE)
```

**ê°œì„  ì½”ë“œ**:
```python
from sklearn.model_selection import StratifiedShuffleSplit

# ë³µí•© Stratification Key ìƒì„±
df['strat_key'] = (
    df['ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'].astype(str) + '_' +
    pd.qcut(df['ì‹ ìš©ë“±ê¸‰ì ìˆ˜'], q=3, labels=['Low', 'Mid', 'High'], duplicates='drop').astype(str) + '_' +
    df['ì œì¡°ì—…ì—¬ë¶€'].astype(str)
)

# Test Set ë¶„í•  (20%)
sss_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for temp_idx, test_idx in sss_test.split(X, df['strat_key']):
    X_temp, X_test = X.iloc[temp_idx], X.iloc[test_idx]
    y_temp, y_test = y.iloc[temp_idx], y.iloc[test_idx]
    strat_temp = df['strat_key'].iloc[temp_idx]

# Val Set ë¶„í•  (25% of temp = 20% of total)
sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)
for train_idx, val_idx in sss_val.split(X_temp, strat_temp):
    X_train, X_val = X_temp.iloc[train_idx], X_temp.iloc[val_idx]
    y_train, y_val = y_temp.iloc[train_idx], y_temp.iloc[val_idx]

# ë¶„í¬ ê²€ì¦
print('='*70)
print('ë°ì´í„° ë¶„í•  ê²€ì¦')
print('='*70)
for dataset_name, y_data in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
    print(f'{dataset_name}: ë¶€ë„ìœ¨ {y_data.mean():.2%}, í¬ê¸° {len(y_data):,}')
```

### 7.2 SMOTE ì „ëµ ê°œì„ 

**í˜„ì¬ ì½”ë“œ** (Cell 10):
```python
# SMOTE sampling_strategy=0.2 ê³ ì •
s.append(('resamp', SMOTE(sampling_strategy=0.2, random_state=RANDOM_STATE) if resamp else 'passthrough'))
```

**ê°œì„  ì½”ë“œ**:
```python
def create_pipeline_v2(clf, wins=False, resamp_strategy='smote', resamp_ratio=0.2):
    s = [
        ('inf', InfiniteHandler()),
        ('imp', SimpleImputer(strategy='median').set_output(transform='pandas')),
        ('log', LogTransformer())
    ]
    if wins:
        s.append(('wins', Winsorizer()))
    s.append(('scaler', RobustScaler()))

    # Resampling ì „ëµ ì„ íƒ
    if resamp_strategy == 'smote':
        resampler = SMOTE(sampling_strategy=resamp_ratio, random_state=42)
    elif resamp_strategy == 'borderline':
        resampler = BorderlineSMOTE(sampling_strategy=resamp_ratio, random_state=42)
    elif resamp_strategy == 'adasyn':
        from imblearn.over_sampling import ADASYN
        resampler = ADASYN(sampling_strategy=resamp_ratio, random_state=42)
    elif resamp_strategy == 'smote_enn':
        from imblearn.combine import SMOTEENN
        resampler = SMOTEENN(random_state=42)
    elif resamp_strategy is None:
        resampler = 'passthrough'
    else:
        resampler = 'passthrough'

    s.append(('resamp', resampler))
    s.append(('clf', clf))
    return ImbPipeline(s)

# ì‹¤í—˜
for strategy in ['smote', 'borderline', 'adasyn', 'smote_enn', None]:
    pipe = create_pipeline_v2(CatBoostClassifier(...), resamp_strategy=strategy)
    pipe.fit(X_train, y_train)
    val_pr = average_precision_score(y_val, pipe.predict_proba(X_val)[:, 1])
    print(f'{strategy}: {val_pr:.4f}')
```

### 7.3 AutoML n_iter ì¦ê°€

**í˜„ì¬ ì½”ë“œ** (Cell 12):
```python
search = RandomizedSearchCV(pipe, pipe_grid, n_iter=50, ...)
```

**ê°œì„  ì½”ë“œ**:
```python
# n_iterë¥¼ ëª¨ë¸ë³„ë¡œ ì°¨ë“± ì ìš©
model_specific_n_iter = {
    'LightGBM': 100,      # ë¹ ë¥´ë¯€ë¡œ ë§ì´
    'XGBoost': 80,
    'CatBoost': 50,       # ëŠë¦¬ë¯€ë¡œ ì ê²Œ
    'RandomForest': 60,
    'ExtraTrees': 60,
    'LogisticRegression': 30  # íŒŒë¼ë¯¸í„° ì ìŒ
}

for name, (model, grid) in models.items():
    n_iter = model_specific_n_iter.get(name, 50)
    search = RandomizedSearchCV(
        pipe, pipe_grid,
        n_iter=n_iter,
        scoring=scorer,
        cv=cv,
        n_jobs=-1,
        random_state=42,
        verbose=1  # ì§„í–‰ ìƒí™© í‘œì‹œ
    )
    search.fit(X_train, y_train)
```

### 7.4 Feature Importance ì €ì¥

**í˜„ì¬ ì½”ë“œ** (Cell 44):
```python
# Feature Importance ì‹œê°í™”ë§Œ í•˜ê³  ì €ì¥ ì•ˆ í•¨
```

**ê°œì„  ì½”ë“œ**:
```python
import joblib

if hasattr(clf, 'feature_importances_'):
    imp = clf.feature_importances_
    feat_imp = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': imp
    }).sort_values('Importance', ascending=False)

    # CSV ì €ì¥
    feat_imp.to_csv(
        f'{PROCESSED_DIR}/feature_importance_{final_name}.csv',
        index=False
    )

    # ì‹œê°í™”
    fig = go.Figure(...)
    fig.show()

    # ìƒìœ„ 15ê°œ íŠ¹ì„± ì¶œë ¥
    print('='*70)
    print(f'Top 15 Features ({final_name})')
    print('='*70)
    for idx, row in feat_imp.head(15).iterrows():
        print(f'{row["Feature"]:30s}: {row["Importance"]:.4f}')
```

---

## 8. ìµœì¢… ê¶Œì¥ì‚¬í•­

### 8.1 ì¦‰ì‹œ ì‹¤í–‰ (This Week)

1. **âœ… K-Fold CV ì¬ê²€ì¦** (30ë¶„)
   - 5-Fold CVë¡œ ì„±ëŠ¥ ë¶„ì‚° í™•ì¸
   - Val-Test ê´´ë¦¬ê°€ ë°ì´í„° ë¶„í•  ë¬¸ì œì¸ì§€ í™•ì¸

2. **âœ… Feature Importance í™•ì¸** (10ë¶„)
   - Cell 44 ì‹¤í–‰ ê²°ê³¼ í™•ì¸
   - ì‹ ìš©ë“±ê¸‰ì ìˆ˜ê°€ 1ìœ„ì¸ì§€ ê²€ì¦
   - ìƒìœ„ 5ê°œ íŠ¹ì„±ì˜ importance í•©ê³„ í™•ì¸

3. **âœ… SMOTE ì œê±° ì‹¤í—˜** (1ì‹œê°„)
   - Class Weightë§Œ ì‚¬ìš©í•´ì„œ ì¬í•™ìŠµ
   - Val-Test Gap ë³€í™” í™•ì¸

### 8.2 ë‹¨ê¸° ì‹¤í–‰ (Next 2 Weeks)

4. **ğŸŸ  Stratified Split ê°œì„ ** (2ì‹œê°„)
   - ë³µí•© Stratification (ë¶€ë„+ì‹ ìš©ë“±ê¸‰+ì—…ì¢…)
   - Val vs Test ë¶„í¬ ë¹„êµ (KS-Test)

5. **ğŸŸ  ì‹ ìš©ë“±ê¸‰ì ìˆ˜ ì¬ì„¤ê³„** (3ì‹œê°„)
   - One-Hot Encoding ì‹¤í—˜
   - VIF ì¬ì¸¡ì •
   - ì„±ëŠ¥ ë¹„êµ

6. **ğŸŸ  VIF ê¸°ë°˜ íŠ¹ì„± ì œê±°** (2ì‹œê°„)
   - VIF > 10 íŠ¹ì„± ì œê±°
   - ìœ ë™ì„± ì¤‘ë³µ íŠ¹ì„± ì •ë¦¬
   - ì„±ëŠ¥ ë¹„êµ

### 8.3 ì¤‘ê¸° ì‹¤í–‰ (Month 1)

7. **ğŸŸ¡ Stacking Ensemble** (1ì¼)
   - CatBoost + XGBoost + LightGBM + LR
   - Meta-learner: LogisticRegression
   - 5-Fold CVë¡œ í•™ìŠµ

8. **ğŸŸ¡ RFE Feature Selection** (1ì¼)
   - 10~25ê°œ ë²”ìœ„ íƒìƒ‰
   - ìµœì  íŠ¹ì„± ìˆ˜ ê²°ì •
   - ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ

9. **ğŸŸ¡ Nested Cross-Validation** (1ì¼)
   - Outer 5-Fold + Inner 3-Fold
   - ëª¨ë¸ ì„ íƒ í¸í–¥ ì œê±°
   - ì‹ ë¢° êµ¬ê°„ ì¶”ì •

### 8.4 ì„±ê³µ ê¸°ì¤€

**ìµœì†Œ ìš”êµ¬ì‚¬í•­** (Must Have):
- âœ… Val-Test Gap < 10%
- âœ… Test PR-AUC â‰¥ 0.15
- âœ… Test Recall â‰¥ 80%

**ëª©í‘œ** (Should Have):
- ğŸ¯ Val-Test Gap < 5%
- ğŸ¯ Test PR-AUC â‰¥ 0.16 (í˜„ì¬ ìˆ˜ì¤€ ìœ ì§€)
- ğŸ¯ Test Recall â‰¥ 85%
- ğŸ¯ Feature ìˆ˜ < 20ê°œ

**ì´ìƒì ** (Nice to Have):
- ğŸŒŸ Val-Test Gap < 3%
- ğŸŒŸ Test PR-AUC â‰¥ 0.17
- ğŸŒŸ Test Recall â‰¥ 90%
- ğŸŒŸ Feature Importance ê· í˜• (Top 1 < 20%)

---

## 9. ê²°ë¡ 

### 9.1 í•µì‹¬ ë©”ì‹œì§€

**"ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì œê±°ëŠ” ì˜³ì€ ë°©í–¥ì´ë‚˜, Val-Test ê´´ë¦¬ í•´ì†Œê°€ ìµœìš°ì„  ê³¼ì œ"**

### 9.2 ê¸ì •ì  ì¸¡ë©´

1. **Test ì„±ëŠ¥ í–¥ìƒ** (0.1542 â†’ 0.1602, +3.9%)
   - ì¼ë°˜í™” ëŠ¥ë ¥ ê°œì„ 
   - ê³¼ì í•© ê°ì†Œ

2. **Recall í–¥ìƒ** (80.3% â†’ 86.8%, +6.58%p)
   - ë¶€ë„ ë¯¸íƒì§€ ìœ„í—˜ ê°ì†Œ
   - ì‹¤ë¬´ ê°€ì¹˜ í–¥ìƒ

3. **ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ë³´**
   - ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ í•´ê²°
   - ëª¨ë¸ í•´ì„ë ¥ ê°œì„ 

### 9.3 ìš°ë ¤ ì‚¬í•­

1. **ì‹¬ê°í•œ Val-Test ê´´ë¦¬** (28.7%)
   - ëª¨ë¸ ì„ íƒ ì‹ ë¢°ë„ ì €í•˜
   - ë°°í¬ ì‹œ ì˜ˆì¸¡ ì„±ëŠ¥ ë¶ˆí™•ì‹¤

2. **ê³¼ì í•© ì§•í›„** (ëª¨ë“  ëª¨ë¸ì—ì„œ CV > Val)
   - SMOTE ë¶€ì‘ìš© ê°€ëŠ¥ì„±
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³¼ìµœì í™”

3. **íŠ¹ì„± ì„ íƒ ë¯¸í¡**
   - ì‹ ìš©ë“±ê¸‰ì ìˆ˜ VIF 23.24 (ì—¬ì „íˆ ë†’ìŒ)
   - Feature Importance ë¯¸í™•ì¸

### 9.4 ë‹¤ìŒ ë‹¨ê³„

**Week 1**: ì§„ë‹¨ (K-Fold CV, SMOTE ì‹¤í—˜, Feature Importance)
**Week 2**: Feature Engineering (ì‹ ìš©ë“±ê¸‰ ë³€í™˜, VIF ì œê±°)
**Week 3**: ëª¨ë¸ ê°œì„  (Stacking, RFE)
**Week 4**: ì¢…í•© í‰ê°€ ë° ìµœì¢… ì„ íƒ

### 9.5 ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥

**ë³´ìˆ˜ì  ì¶”ì •** (Val-Test Gap < 10% ë‹¬ì„± ì‹œ):
- Val PR-AUC: 0.145 ~ 0.155
- Test PR-AUC: 0.155 ~ 0.165
- Test Recall: 82% ~ 88%

**ì´ìƒì  ì¶”ì •** (ëª¨ë“  ê°œì„  ì„±ê³µ ì‹œ):
- Val PR-AUC: 0.155 ~ 0.165
- Test PR-AUC: 0.165 ~ 0.175
- Test Recall: 85% ~ 90%

---

**ì‘ì„±ì**: Claude Code
**ë¶„ì„ ì¼ì**: 2025-11-23
**ë‹¤ìŒ ë¦¬ë·° ì¼ì •**: Week 1 ì‹¤í—˜ ì™„ë£Œ í›„ (2025-11-30)
