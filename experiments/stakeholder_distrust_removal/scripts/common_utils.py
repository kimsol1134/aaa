"""
ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ìŒ
ì‹¤í—˜ ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê³µí†µ í•¨ìˆ˜ë“¤
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import average_precision_score, recall_score, f1_score, fbeta_score, confusion_matrix
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN
from imblearn.combine import SMOTEENN
from imblearn.pipeline import Pipeline as ImbPipeline
import scipy.stats as stats
from scipy.stats import ks_2samp

import joblib
import json
from pathlib import Path
from datetime import datetime

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path('/home/user/aaa')
DATA_DIR = BASE_DIR / 'data'
FEATURES_DIR = DATA_DIR / 'features'
PROCESSED_DIR = DATA_DIR / 'processed'
EXPERIMENT_DIR = BASE_DIR / 'experiments' / 'stakeholder_distrust_removal'
RESULTS_DIR = EXPERIMENT_DIR / 'results'

RANDOM_STATE = 42


def load_data():
    """
    ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± ë°ì´í„° ë¡œë”©

    Returns:
        X: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„
        y: íƒ€ê²Ÿ ë³€ìˆ˜ (ë¶€ë„ ì—¬ë¶€)
    """
    # ì™„ì „íŒ ë°ì´í„° ë¡œë”©
    data_path = FEATURES_DIR / 'domain_based_features_ì™„ì „íŒ.csv'
    df = pd.read_csv(data_path, encoding='utf-8')

    target_col = 'ëª¨í˜•ê°œë°œìš©Performance(í–¥í›„1ë…„ë‚´ë¶€ë„ì—¬ë¶€)'

    y = df[target_col]
    X = df.drop(columns=[target_col])

    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {X.shape[0]:,} ê¸°ì—…, {X.shape[1]:,} íŠ¹ì„±")
    print(f"   ë¶€ë„ìœ¨: {y.mean()*100:.2f}% ({y.sum():,} / {len(y):,})")

    return X, y


def split_data(X, y, test_size=0.2, val_size=0.25, random_state=RANDOM_STATE):
    """
    ë°ì´í„°ë¥¼ Train/Val/Testë¡œ ë¶„í• 

    Args:
        X: íŠ¹ì„± ë°ì´í„°
        y: íƒ€ê²Ÿ ë³€ìˆ˜
        test_size: Test set ë¹„ìœ¨ (ì „ì²´ì˜ 20%)
        val_size: Validation set ë¹„ìœ¨ (tempì˜ 25% = ì „ì²´ì˜ 20%)
        random_state: ëœë¤ ì‹œë“œ

    Returns:
        X_train, X_val, X_test, y_train, y_val, y_test
    """
    # Test set ë¶„í• 
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, stratify=y, random_state=random_state
    )

    # Validation set ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=random_state
    )

    print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"   Train: {len(y_train):,} ({y_train.mean()*100:.2f}% ë¶€ë„)")
    print(f"   Val:   {len(y_val):,} ({y_val.mean()*100:.2f}% ë¶€ë„)")
    print(f"   Test:  {len(y_test):,} ({y_test.mean()*100:.2f}% ë¶€ë„)")

    return X_train, X_val, X_test, y_train, y_val, y_test


class InfiniteHandler:
    """ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬"""
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_copy = X.copy()
        X_copy = X_copy.replace([np.inf, -np.inf], np.nan)
        return X_copy

    def fit_transform(self, X, y=None):
        return self.transform(X)


class LogTransformer:
    """ë¡œê·¸ ë³€í™˜ (ìŒìˆ˜ ì²˜ë¦¬ í¬í•¨)"""
    def __init__(self, epsilon=1e-8):
        self.epsilon = epsilon

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_copy = X.copy()
        for col in X_copy.columns:
            if X_copy[col].min() < 0:
                # ìŒìˆ˜ê°€ ìˆìœ¼ë©´ ë³€í™˜ ìŠ¤í‚µ
                continue
            else:
                # ì–‘ìˆ˜ë§Œ ìˆìœ¼ë©´ ë¡œê·¸ ë³€í™˜
                X_copy[col] = np.log1p(X_copy[col] + self.epsilon)
        return X_copy

    def fit_transform(self, X, y=None):
        return self.transform(X)


def create_pipeline(clf, wins=False, resamp='smote', resamp_ratio=0.2):
    """
    ì „ì²˜ë¦¬ + ë¦¬ìƒ˜í”Œë§ + ë¶„ë¥˜ê¸° íŒŒì´í”„ë¼ì¸ ìƒì„±

    Args:
        clf: ë¶„ë¥˜ê¸°
        wins: Winsorization ì ìš© ì—¬ë¶€
        resamp: ë¦¬ìƒ˜í”Œë§ ë°©ë²• ('smote', 'borderline', 'adasyn', 'smote_enn', None)
        resamp_ratio: ë¦¬ìƒ˜í”Œë§ ë¹„ìœ¨

    Returns:
        ImbPipeline ê°ì²´
    """
    steps = [
        ('inf', InfiniteHandler()),
        ('imp', SimpleImputer(strategy='median')),
        ('scaler', RobustScaler())
    ]

    # Resampling
    if resamp == 'smote':
        resampler = SMOTE(sampling_strategy=resamp_ratio, random_state=RANDOM_STATE)
    elif resamp == 'borderline':
        resampler = BorderlineSMOTE(sampling_strategy=resamp_ratio, random_state=RANDOM_STATE)
    elif resamp == 'adasyn':
        resampler = ADASYN(sampling_strategy=resamp_ratio, random_state=RANDOM_STATE)
    elif resamp == 'smote_enn':
        resampler = SMOTEENN(random_state=RANDOM_STATE)
    elif resamp is None:
        resampler = 'passthrough'
    else:
        resampler = 'passthrough'

    steps.append(('resamp', resampler))
    steps.append(('clf', clf))

    return ImbPipeline(steps)


def evaluate_model(model, X_val, y_val, X_test, y_test, model_name='Model'):
    """
    ëª¨ë¸ í‰ê°€ ë° ë©”íŠ¸ë¦­ ê³„ì‚°

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        X_val: Validation íŠ¹ì„±
        y_val: Validation íƒ€ê²Ÿ
        X_test: Test íŠ¹ì„±
        y_test: Test íƒ€ê²Ÿ
        model_name: ëª¨ë¸ ì´ë¦„

    Returns:
        ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # Validation ì„±ëŠ¥
    y_val_pred_proba = model.predict_proba(X_val)[:, 1]
    val_pr_auc = average_precision_score(y_val, y_val_pred_proba)

    # Test ì„±ëŠ¥
    y_test_pred_proba = model.predict_proba(X_test)[:, 1]
    test_pr_auc = average_precision_score(y_test, y_test_pred_proba)

    # Recall@80% threshold (validation ê¸°ì¤€)
    threshold = np.percentile(y_val_pred_proba, 80)
    y_test_pred = (y_test_pred_proba >= threshold).astype(int)

    test_recall = recall_score(y_test, y_test_pred)
    test_f2 = fbeta_score(y_test, y_test_pred, beta=2)

    # Confusion Matrix
    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()

    # Val-Test Gap
    gap = abs(test_pr_auc - val_pr_auc) / val_pr_auc * 100

    results = {
        'model_name': model_name,
        'val_pr_auc': val_pr_auc,
        'test_pr_auc': test_pr_auc,
        'val_test_gap': gap,
        'test_recall': test_recall,
        'test_f2': test_f2,
        'threshold': threshold,
        'tp': int(tp),
        'fp': int(fp),
        'tn': int(tn),
        'fn': int(fn)
    }

    print(f"\nğŸ“Š {model_name} í‰ê°€ ê²°ê³¼:")
    print(f"   Val PR-AUC:    {val_pr_auc:.4f}")
    print(f"   Test PR-AUC:   {test_pr_auc:.4f}")
    print(f"   Val-Test Gap:  {gap:.1f}%")
    print(f"   Test Recall:   {test_recall:.2%}")
    print(f"   Test F2-Score: {test_f2:.4f}")

    return results


def compare_distributions(X_val, X_test, features):
    """
    Validationê³¼ Test ì„¸íŠ¸ì˜ íŠ¹ì„± ë¶„í¬ ë¹„êµ (KS-Test)

    Args:
        X_val: Validation íŠ¹ì„±
        X_test: Test íŠ¹ì„±
        features: ë¹„êµí•  íŠ¹ì„± ë¦¬ìŠ¤íŠ¸

    Returns:
        ë¶„í¬ ì°¨ì´ ìœ ì˜í•œ íŠ¹ì„± ë¦¬ìŠ¤íŠ¸
    """
    significant_diffs = []

    for feature in features:
        ks_stat, p_value = ks_2samp(X_val[feature], X_test[feature])

        if p_value < 0.05:
            significant_diffs.append({
                'feature': feature,
                'ks_stat': ks_stat,
                'p_value': p_value
            })

    return pd.DataFrame(significant_diffs).sort_values('p_value')


def save_results(results, experiment_name, suffix=''):
    """
    ì‹¤í—˜ ê²°ê³¼ ì €ì¥

    Args:
        results: ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ë°ì´í„°í”„ë ˆì„
        experiment_name: ì‹¤í—˜ ì´ë¦„ (week1, week2, week3)
        suffix: íŒŒì¼ëª… ì ‘ë¯¸ì‚¬
    """
    output_dir = RESULTS_DIR / experiment_name
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{experiment_name}_{suffix}_{timestamp}.json" if suffix else f"{experiment_name}_{timestamp}.json"

    output_path = output_dir / filename

    # ë°ì´í„°í”„ë ˆì„ì¸ ê²½ìš° CSVë¡œ ì €ì¥
    if isinstance(results, pd.DataFrame):
        csv_path = output_path.with_suffix('.csv')
        results.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ê²°ê³¼ ì €ì¥: {csv_path}")
    else:
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° JSONìœ¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")

    return output_path


def remove_feature_from_data(X, feature_name):
    """
    íŠ¹ì„± ì œê±° (ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ë“±)

    Args:
        X: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„
        feature_name: ì œê±°í•  íŠ¹ì„±ëª…

    Returns:
        íŠ¹ì„± ì œê±°ëœ ë°ì´í„°í”„ë ˆì„
    """
    if feature_name in X.columns:
        X_removed = X.drop(columns=[feature_name])
        print(f"âœ… '{feature_name}' ì œê±°: {X.shape[1]} â†’ {X_removed.shape[1]} íŠ¹ì„±")
        return X_removed
    else:
        print(f"âš ï¸ '{feature_name}' íŠ¹ì„±ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        return X.copy()


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸
    print("=" * 70)
    print("ê³µí†µ ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
    X, y = load_data()

    # ë°ì´í„° ë¶„í•  í…ŒìŠ¤íŠ¸
    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)

    # ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜ ì œê±° í…ŒìŠ¤íŠ¸
    X_removed = remove_feature_from_data(X, 'ì´í•´ê´€ê³„ì_ë¶ˆì‹ ì§€ìˆ˜')

    print("\nâœ… ê³µí†µ ìœ í‹¸ë¦¬í‹° ì •ìƒ ì‘ë™ í™•ì¸")
